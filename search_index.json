[
["index.html", "Let’s program in R (“Introduction to programming language R” course book) Foreword", " Let’s program in R Damir Pintar 2020-01-20 (“Introduction to programming language R” course book) ** NOTE: During the current academic year, the “Introduction to Programming Language R” is revised chapter by chapter. After the subject is New chapters will be dynamically added to this HTML document. If you need to have access to all the materials, contact the author of the tutorial at damir.pintar@fer.hr** Foreword This tutorial is based on interactive lessons used in the “Introduction to Programming Language R” at the Faculty of Electrical Engineering and Computing at the University of Zagreb. But the topics discussed here are not only useful to the students of the mentioned faculty - knowledge of the language R will be good both in academia and in the business world. Although R is known as a “programming language made up of statisticians, for statisticians” and is most often associated with the field of data science within which it is used for complex statistical analysis and data mining, it can be very useful for tasks related to the management of smaller or larger data at tasks that are not necessarily strictly oriented to advanced analytics. Namely, popular graphical tools with their interactive tabular presentation are very intuitive and excellent for simpler tasks, but as the need for more complex tasks appears, they quickly lose their efficiency and simplicity; on the other hand, the interactive program approach offered by R is initially somewhat more demanding but long-term highly cost-effective because complex tasks can be dealt with in an efficient, consistent and insightful way. For this reason, in the business world there is a clear shifting trend from classic GUI office tools to platforms with better support for more complex calculations and the creation of attractive visualizations. This is evidenced by a strong increase in the popularity of R language and other platforms with similar approach to data analysis. The aforementioned popularity of R language results in an increased need for learning resources, which are not currently very much present in Croatia. This coursebook will try to make learning R as easy and interesting as possible through its “learning through examples” approach. Emphasis will be placed primarily on mastering R as a programming language. For this reason, the initial chapters will focus on “programmatical aspects”, followed by a review of available tools presumed to be useful for the widest set of readers - tools for data gathering, extracting useful information, and visualizations. Since R is a domain-oriented language, R’s role in its support for statistical analysis will be reviewed followed byexamining selected machine learning methods and their applications. Although there will be enough information to put all the presented methods into context, the idea of this textbook is not to teach readers statistics nor deeply enter the field of machine learning - the intention of the author is to intrigue readers to continue exploring this interesting area, adequately armed with platform knowledge that will enable all new knowledge is immediately practically applied in further research. "],
["introduction.html", "1 Introduction 1.1 What is R? 1.2 Installing Software Support 1.3 Overview of the development interface RStudio 1.4 How to use this coursebook?", " 1 Introduction 1.1 What is R? 1.1.1 General facts about R Programming language R came from the programming language S, developed for the needs of Bell Telephone Laboratory owned by AT &amp; T corporation. It was designed as an internal statistical analysis tool. The basic philosophy of the S language (inherited by programming language R) was domain orientation - ie facilitating work with data analysts without the need to adapt conventions to traditional programming languages. Language S gained significant popularity in business analysts and statisticians within the 80s and 90s, but is now only available through a commercial variant called S-PLUS. The programming language R was created at the University of Auckland (NZ), modeled on S, and is released under the GNU Open Code Code. The standard distribution of the R programming language consists of: “core” R, with basic functions and so called “core” base package that provides basic functionality a collection of additional packages (“base” - base and “recommended” - recommended) for data management, visualization and statistical analysis We must not ignore the excellent integration of R with a rich repository called CRAN (Coprehensive R Archive Network) that enables fast and easy installation of any packet from that repository, after which it becomes part of the local R installation. Since the R community is extremely strong in the development of new packages, often after the introduction of new experimental methods and access to data analysis, CRAN can quickly offer packages that implement them. Also, strong and continuous enthusiasm of the R community for the enhancment of existing R elements alleviates or eliminates a large number of detected language deficiencies. R is therefore often able to compare it with a “do it yourself” project where, after getting acquainted with the supplied “factory” components (in this case basic functions and packages), the user begins to adapt his development environment by choosing a package that exactly matches his needs and preferences. Creativity and flexibility in using R is considered to be of great advantage, although it results in a certain informality and liberal approach to programming is occasionally not favored by users familiar with strict and formal programming frameworks with a clear set of guidelines and rules to be followed Despite the exceptionally high acceptance of the R language for data analysis and the variety of options offered to the user, it is necessary to be aware of its limitations: R intensely uses RAM which has been considered a serious restriction for a long time; By increasing the capacity of modern hardware systems, this limitation is much more acceptable today, and there are also numerous packages that rationalize the use of memory. Still, the fact remains that R quickly eats the RAM of our computer, though it is often the result of the neglect or ignorance of a developer who has not adopted the “R” mode of programming well enough. R is quite unconventional so the learning curve is initially steeper, especially for programmers accustomed to standard conventions of other programming languages. On the other hand, if viewed long-term, programming in R is quite simple since most of the complex tasks are abstracted into high-level functions that transparently perform low-level operative tasks. It is often said that R is more oriented towards the goal we want to achieve and less detail about the way we reach it. R is not a “fast” language; although it is a language that is expected to work over large data sets, R is not optimized for performance speed or even for parallelism; although there is a great deal of effort to implement virtually all key routines in C which prevents significant slowdowns, and there are a number of packages that allow multiple execution of the R program, the fact remains that R is not designed to be executed as quickly as possible ; If speed is a priority, it is often necessary to look for alternative solutions - which is why it is often said that R is primarily a research language, not a production language. R is primarily intended for interactive work, i.e. performing a series of machine instructions that are dynamically enrolled and executed with the help of a program console. This is tailored to the standard process of data analysis where the analyst can download data, clean it, transform, develop models, test, etc. with constant feedback from a computer an overview of intercepts, adapt the analysis process to current findings etc. This does not mean that programming language can not be programmed in a classical “procedural” way by developing algorithms encapsulated in functions that automatically perform their tasks after call, but the fact is that the efficiency of R is exactly reflected in interactive work. This principle is also transmitted to the teaching of R; programming language R is much easier to learn with interactive approaches by performing specific tasks, experimenting with data sets, accessible methods, and so on, rather than using a “classic” approach by designing scripts that implement some low-level jobs. 1.1.2 R alternatives Programming Language R is a popular but not the only solution for interactive data analysis and statistical programming. Below we will give a brief overview of some of the more popular technologies and solutions used today for this purpose, with a brief comparison and a review of the advantages and disadvantages compared to language R. SAS and SPSS - SAS (Statistical Analysis System, developed by SAS Institute) and SPSS (Software Package for Statistical Analysis, developed by IBM) are two different software packages that we put under the same paragraph primarily because they are commercial tools, ie tools that require full payment for their full functionality. Similarly, SAS and SPSS are relatively easy to learn and their functionality is largely based on carefully designed user interfaces. These tools emphasize efficiency and are an excellent option for large companies looking for a consistent, robust solution for their analytics, not bothered by the commercial nature of such solutions. Weka and Orange - Weka (Waikato Environment for Knowledge Analysis, developed by Waikato University in New Zealand) and Orange (deep data analysis software developed at the University of Ljubljana) are free software for exploratory data analysis and data mining that base their functionality on relatively simple graphing interfaces and visual programming approach. These solutions are very good for users who are not too demanding in terms of the flexibility and complexity of their analysis because they allow the implementation of defined steps in the analysis process in a very accessible and clear way. This does not mean that these tools can not do more complex analysis, but they are still more suited to analyzes through the predefined functionality of the provided graphical interface. Python (Numpy / Pandas / Scikit) - in the last couple of years, Python is the most serious competitor of language R, primarily because Python is a very popular programming language with a very similar process approach to data analysis compared to one used by language R. The discussion of which language to choose is very common in the field of data science, usually without a clear final conclusion. It’s easy to make sure that the differences are in fact miniscule - while R is strongly domain-oriented and emphasis is placed on ease and ease of use with a wide range of available overlapping packages to enable the user to choose the one that best suits him, Python emphasizes the rigid formal structure and principle “for one job one way of doing”. Therefore, it could be said that R is somewhat more suitable for “data research” while Python’s advantage is easier to develop and integrate analytical modules in a production environment, especially if the above environment is already implemented in Python. But with the strong development of both languages and the mutual overlappiong of functionality, this distinction becomes less relevant - it is no longer a problem to integrate R scripts into existing systems regardless of the platform they are running on, and the Python community is developing its versions of popular packages from R that faithfully map their functionality. Ultimately, it can be said that the initial choice between these two alternatives is not so important - the approach they use is so similar and the functionality sharing is so pronounced that learning a single language introduce all of the major concepts from the other so data scientists often decide to master both languages, to more easily adapt to a large number of environments in which they must conduct their analyses. 1.2 Installing Software Support Installing the R language support software is pretty simple, especially if the recommended development interface RStudio is used as a platform. This is not the only option - one of the popular alternatives is the multilingual platform Jupyter Notebook which offers its own R support. Readers are ecnouraged to explore all available options and choose the final selection of the interface that personally matches their needs best, but this coursebook will focus on RStudio mainly because of a clear, easy-to-view interface, easy installation and a very rich support for a variety of functionalities - from installing new packages, easy retrieval of documentation, creating visualizations and publishing reports. Notions that will be discussed below will mostly assume that you have chosen an interface RStudio. To successfully set up a development platform, two things need to be installed language distribution R development interface RStudio It is recommended to use the latest available versions. At the time of writing this document, they are R 3.6.1 and RStudio 1.2.5. If these versions differ from those on your computer, there will probably be no problem if the version numbers are higher than the above; otherwise, their upgrade is recommended. The procedure for installing this software on the operating system Microsoft Windows will be described below. If you are working on some other operating system, such as some Linux distribution or Mac OS, the procedure is somewhat different, but still not too complex - it’s enough to follow the instructions on the websites mentioned below that are a platform-oriented use. To find the software mentioned in the search engine, type the following terms: download R download RStudio In both cases, you will get links to the executable files that you have to run to install the software on your computer. In the case of R, this can be a file R-3.6.1-win.exe (exact numbers may differ). In the interface RStudio you can see more options - choose a free “desktop” version. Commercial versions have some additional functionalities that are mostly oriented to use in professional multi-user environments and are not essential to normal work. You can run the executable files and let the wizard install all the necessary components on your computer. It is recommended to accept all of the nominal options except the installation folder - instead of the subfolder Program Files “it is better to install R directly in the root folder (eg.”C:\\R\\R-3.6.1“). This way, it will be easier to find the currently installed version and eventually update it later. For the same reason, it is recommended that RStudio be installed in the folder”C:\\R\\RStudio\". If you do not have the option or you do not want to choose these folders, you can define some other or retain the default options - this choice should not ultimately affect the further work. After installing the interface RStudio it is enough to simply run it with the help of the created shortcut on the workbook (or alternatively, with the help of the executable file RStudio.exe in the selected installation folder). After launch, the application should look similar to the following image: Figure 1.1: RStudio interface layout If there are any problems, make sure that you have completed all of the installation steps listed above. Below we will deal with the details of the interface shown. 1.3 Overview of the development interface RStudio Let’s look at the interface RStudio. We see it being divided into three windows - the left part is the “work area” and the program code is entered into it. On the right, there are auxiliary windows that show different things, depending on the card selected; In the upper right hand corner, we can see, among other things, what is currently in our working environment (which is empty at the beginning) and the history of the commands that we have executed. The bottom part serves to display documentation, files, installed packages, visualizationa, etc. 1.3.1 Interactive console Let’s go back to the left part of the interface, especially the so-called. “interactive console”. Namely, by its nature, R is an “interpreter language” in the sense that commands are immediately interpreted and executed. Though it is possible to create larger scripts that are then executed “all at once”, working with the R language often comes down to the command-by-command principle. This is precisely why we are talking about “interactive program data analysis” - the analyst is “programming” by entering commands and can at any time study the interminent results obtained and decide on further steps. Let’s see how the interactive console works. With the help of a keyboard, you can type a simple math expression - eg 3 + 2 and press the ENTER key. We will see that R will immediately deliver the result - we can use it as a calculator! For mathematical expressions that are not simply “typed” we need to use functions. Thus, for example, a square root can be calculated using the sqrt () function. Let’s try typing sqrt(10) in the console and press ENTER. R again shows the result immediately. At this time, the screen should look like the next picture. Figure 1.2: R kao kalkulator One of the problems of using this kind of R is the mixing of commands and results, and the history of a string of commands becomes more and more cluttered as we use the console to move everything “lower and lower”. Likewise, if for some reason the command that we execute results in a mistake we are trying to correct, the console becomes “dirty” as mistakes are mixed with error reports, so any slightly more complicated procedure we want to run becomes “torn” and hard to interpret. This is why analysts often use so-called R scripts\" that allow you to visually distinguish the commands we want to execute from the console itself, but with the ability to easily transfer them in the console, where we then look at the result. 1.3.2 Writing R scripts In the toolbar, select File -&gt; New File -&gt; R Script (or press the CTRL + SHIFT + N key combination). We see that the “working area” on the left is divided into two parts. The upper part represents the space for our “script” - actually the series of commands we want to execute - while the interactive console now occupies the lower part of the work surface. If we want, we can tweak the size of these (and other windows) by moving the boundaries, but for now it’s important to have a look at the scripts and consoles. Write two commands in the script script - the first one should be print(' hello!') And underneath it a simple mathematical expression 3 + 4. Return the cursor to the first line and press the CTRL + ENTER key combination. If we have correctly followed these steps, the command at the cursor site will automatically be copied to the interactive console and executed. The cursor will now be the next command that you can do with CTRL + ENTER. The screen should now look similar to the next image. Figure 1.3: R script This is actually the usual way of working in R-language - entering the script space into commands that we then execute by automatically copying them into the console. If something is wrong with the command, we can easily modify it and perform it again. If you want to execute a block of commands, select them by clicking and dragging and then press the CTRL + ENTER key. Scripts can be expanded with comments (starting with the # character that R interprets as ’ ignore this line’). We can save our scripts under the chosen name on the hard disk. But we can go one step further. Though the R scripts are quite adequate for comfortable work in the R language, there is an additional technology that gives us even more flexibility in working with the R - R Markdown. 1.3.3 R Markdown Writing R scripts is very similar to the classic concept of “programming” - we write program commands that are usually executed sequentially, and optionally we add comments for the purpose of the documentation. But since a final step of data analysis usually involves formulating reports that will adequately display the results obtained, the RStudio interface supports technology that provides an effective combination of programming and structured documentation on the principle of “interactive notebooks”. The analyst can write “pure” text, optionally with formulas, images, and changes in the size and nature of the text font, and then can see both the executable code together and its results together, in a format resembling a notebook (in fact, RStudio recently introduced the R Notebook which works very similar to usual “interactive notebook” technologies). This technology is easiest to demonstrate through the examples - in the toolbar, select File -&gt; New File -&gt; R Markdown ... and in the next window choose an arbitrary title (eg \"Testing\"), optionally add the author’s name, and choose one of the options for final form of report (HTML is recommended due to the lowest dependency on additional packages). Unlike the R script, which will initially appear empty, with RMD R will create a “filled” document. This is done in this way for the simple reason that the user gets an easily modifiable template with all the basic elements included as a reminder. We will delete most of this template for our needs - everything after the initial header, ie below the second sign of ---. Then we can write any text below. With the #, ##, ### etc tags we can set the title of a certain section (they are now not commentaries because this is not R code!), While *and **signs in front and back we select the italic or bold font which weall appear in the final report. This is so-called. pure “markdown”, ie plain text that can be converted into formatted text with the help of additional tools, if desired. When we want to incorporate the program code into our “report”, we have to create so-called \" code chunk\". This can be done by selecting Insert -&gt; R on the toolbar or using the CTRL + ALT + I keys. Notice that the chunk begins and ends with a specially selected string of characters - three “backticks”. Likewise, the beginning of the chunk has description of certain parameteres in the opening brackets, the most important of which is the programming language we will use. In this textbook, we will almost exclusively use the language R, although other languages may be used if they are installed on the platform running RStudio. The code chunk behaves the same as the standard R script - we can enter commands and execute them. The difference is just that - if you want - the results can be seen immediately in the R Markdown document itself. If this option is not what we want, we can turn it off (click on the gear in the toolbar and select Chunk output in console) but in general we prefer to have the results right below the code that created them, notebook-style. If we follow the instructions, the screen may look similar to the following image: Figure 1.4: R Markdown document We can now try to create a “report” from the current document. First, we must save it under a particular name (eg Testing.rmd), and then we can click on the Knit button to convert the document from pure text to an HTML file. R Markdown documents are much more powerful than it may seem judging by the elements that have been presented so far. Chunks can have plenty of different parameters which influence their behavior. The output format can be PDF, DOCX as well as other forms such as slides of various technologies, books intended for mobile devices, interactive web applications etc. The coursebook you are reading is actually nothing more than a series of RMD files converted into the appropriate form you are currently using. As we will explain in the next chapter, RMD files are also the main way for you to use this coursebook effectively and try out the examples and tasks that follow. The universality and flexibility of technology R Markdown is exceptionally great, and is very well accepted by the R community. 1.4 How to use this coursebook? The basic idea of this coursebook is “learning through application”. Therefore, the lessons below will not use too many examples, but rather the reader will be encouraged to try out each new concept by solving a series of easy or intermediate tasks. Each chapter that follows has an accompanying “workbook”. Simply put, it is an RMD file that contains all the examples from the lecture, accompanied by a concise text for easier reference to the concepts being dealt with. The basic idea is that the reader reads the coursebook and solves the workbook in parallel, looking at the task solution only after it is solved within the programming tool. Some tasks will require simply removing the # sign (meaning “comment”) from the start of the command and executing it. In spite of the trivial approach, in this way, however, the reader is encouraged to independently test the command rather than just look at its result. Other tasks will require a bit more engagement. Finally, after each lesson there is a series of “Homework exercises” that will not have the solution and which will represent a kind of evaluation milestone of all the concepts of the lesson. Readers are strongly suggested solving all of the examples and tasks before moving on to the next lesson, as the lessons that follow presume the well-accepted knowledge of all the topics that are discussed previously. Of course, the textbook can be read without the above-mentioned “interactive” approach. Task solutions reveal the correct method of accessing the problem, and most of the commands are accompanied by a print that the user would get on the screen by executing them. Nevertheless, the coursebook author’s attitude is that programming languages can not be taught by reading, and that the extra effort to try all, even the simplest, concepts is ultimately very rewarding. Let’s get to know the concept of workbooks more closely. First you need to find and open a workbook that matches the lesson you are reading. It is easy to recognize by the appropriate number of lessons - the notebook for this lesson is named 01_Introduction_RB.Rmd. It is recommended that all workbooks that you plan to use are copied somewhere to the local computer together with all the accompanying files. As stated previously, the workbook will typically contain all the program code in the lesson to which it refers, but only enough pieces of text sufficient for easier understanding. If you read this text directly from the workbook and not as part of the tutorial, you can see that the entire previous lesson is missing; this is because the introductory steps described in it relate to the concepts that need to be adopted before using the workbook. If you have not passed them, it is a good idea to go back and pass them, and then continue with the following examples and tasks. Workbooks differ in Examples and Assignments. Examples are usually only required to be executed. Assignments on the other hand are expecting some changes or entering a new program code. As said, the tutorial will set far more emphasis on tasks. An example might look like this: Example - a few simple commands R of programming language 3 + 2 # adding log(10) # natural logarithm! log10(10) # this is a base-10 logarithm! By the way, we comment with the &quot;#&quot; sin(0.5 * pi) # pi is one of the built-in constants ## [1] 5 ## [1] 2.302585 ## [1] 1 ## [1] 1 You can execute the commands from the examples individually, or the entire chunk at once with the CTRL + SHIFT + ENTER key combination. No modification of the code is necessary (although it is often not bad to experiment with the given commands!). Tasks on the other hand always require a certain - even minimal - intervention. Assignment 1.1 - commands for checking and editing workbooks # Make the following commands by removing the comment character #getwd() # folder in which we are currently working #setwd(&quot;.&quot;) # Here we can specify a new working folder if desired getwd() # directory in which we are currently working setwd(&quot;.&quot;) # Here we can specify a new work directory if desired The assignment will often refer to the just introduced concept. Eg. it is convenient to note that, although language R supports an operator = for assigning a value to a variable, it is recommended to use the &lt;- operator for that purpose, which is somewhat more “R-like”. Also, note that R supports the so-called. autoprint , ie always prints the result of the last command on the screen. This means that if we create a new variable x in the snippet and want to print it on the screen, we do not have to put print(x)as the last command, but just x. Let’s try this in the following exercise. Assignment 1.2 - Assignment operator # put `5` in the variable` x` # then print the variable `x` on the screen x &lt;- 5 x ## [1] 5 Now that we are well acquainted with the work platform, we can begin by learning the basic elements of the R programming language. Programirajmo u R-u by Damir Pintar is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License.Based on a work at https://ratnip.github.io/FER_OPJR/ "],
["tipovi.html", "2 Basic data types and operators 2.1 Basic data types 2.2 Operators 2.3 Missing, unknown, and non-existent values Exercises", " 2 Basic data types and operators “Basic” or “primitive” types of data are the underlying building blocks of programming languages. They are basically embedded mechanisms that allow storing basic information - most commonly of logical, numeric, or character nature. Most programming languages use the same or very similar methods of storing such information, which means that they implement similar basic data types - the difference is often in details such as the actual type name, the nominal number of bytes, etc. In any case, the most common first step in learning a new programming language is to know the basic types of data that it supports. The next thing that may interest us is the language syntax, that is, the way we write commands that language interpreters can understand and execute. The R language in its syntax follows similar conventions seen in languages such as Python, Ruby, or Java, of course with certain specifics. Some basic syntax rules are: each command must, as a rule, go to its own line, but the indentation of commands is not as important as placing a point-to-end commands; defining blocks with bracketed brackets; we do not have to define the types of variables in advance since their type will be inferred by the assigned value; comments start with #; etc. Syntax will be best learned through examples - by learning the elements of the language syntax rules often become intuitively clear. It is best to start with simple functions and operators, as we will do in this lesson. We will end the lecture with a debate on the so-called. “missing” or “non-existent” values. Since R has its own way of defining this type of value, we will immediately clarify the way in which those in R are implemented so that in the following lessons the readers are prepared to easily manage such values (which often occur in work with actual data sets). 2.1 Basic data types R offers six basic data types: type examples logical TRUE, FALSE ili T, F integer 2L, 5L, 123456789L double 4, 6, 3.14, 2e5 complex 5 + 2i, 7 + 1i character \"A\", \"B\", \"Pero\", \"ABCDEFGHijklmnoPQRSTUVwyz\" raw as.raw(2), charToRaw(\"1\") Some remarks: integer and real types are often treated together as a numeric type (although this is not entirely consistent!) complex type must have a declared imaginary constant even if it is 1 (2 + i is not a good record!) The type of “raw” bytes is relatively rarely used Checking whether a variable is of a certain type can work with the is.&lt;type_name&gt; function. We will try this in the next exercise. Before we begin with solving, we introduce one new feature: in exercises where we print more things on the screen, it is useful to visually separate the different print segments so that we can easily understand which part of the code is referenced. For this purpose, we will use the cat(\"-----------\\n\") command that simply prints a dash of the line and goes to the new line. We could also use the print() function, but it always starts printing with the element index while the cat command is a “raw” print, which in this case is more appropriate. Assignment 2.1 - checking data types #try the following commands: #is.logical(FALSE) #is.integer(2L) #is.double(1.11) # perform the following checks: # is 5L numeric? # is 3.14 numeric? # is &quot;ABC&quot; character? # is 4 + 2i complex? # is 5 integer? is.logical(FALSE) is.integer(2L) is.double(1.11) cat(&quot;-----------\\n&quot;) is.numeric(5L) is.numeric(3.14) is.character(&quot;ABC&quot;) is.complex(4 + 2i) is.integer(5) ## [1] TRUE ## [1] TRUE ## [1] TRUE ## ----------- ## [1] TRUE ## [1] TRUE ## [1] TRUE ## [1] TRUE ## [1] FALSE Did you notice anything unusual in these checks? Try to explain the obtained results. Type some variables or constants we can retrieve with the typeof orclass function. The difference between them is the following: typeof - fetches\" primitive\" or “basic” type ( integer, double) class - ‘object type’, actually the value of class attribute Assignment 2.2 - data type retrieval # print the types of the following constants: TRUE, 2L, F, 3.14, &quot;ABC&quot; # print the classes of the same constants. Are there any differences? typeof(TRUE) typeof(2L) typeof(F) typeof(3.14) typeof(&quot;ABC&quot;) cat(&quot;-----------\\n&quot;) class(TRUE) class(2L) class(F) class(3.14) class(&quot;ABC&quot;) ## [1] &quot;logical&quot; ## [1] &quot;integer&quot; ## [1] &quot;logical&quot; ## [1] &quot;double&quot; ## [1] &quot;character&quot; ## ----------- ## [1] &quot;logical&quot; ## [1] &quot;integer&quot; ## [1] &quot;logical&quot; ## [1] &quot;numeric&quot; ## [1] &quot;character&quot; Data can be explicitly converted from one type to another using the function as.&lt;type_name&gt;: Assignment 2.3 - conversion of data types # perform the following conversions and print the result # 2.35 to integer # TRUE to ntomeric # 100L to character # 2.35 to character # 2e2 to character # 0 to logical # 2.75 to logical as.integer(2.35) as.numeric(TRUE) as.character(100L) as.character(2.35) as.character(2e2) as.logical(0) as.logical(2.75) ## [1] 2 ## [1] 1 ## [1] &quot;100&quot; ## [1] &quot;2.35&quot; ## [1] &quot;200&quot; ## [1] FALSE ## [1] TRUE R will implement implicit conversion if possible: Assignment 2.4 - implicit conversion # Write the following phrases and print the results: # arithmetic operator between logical and numeric variables # arithmetic operator between integer and numeric variables # logical operator negation (!) applied to numeric variable # arithmetic operator between logical and numeric variables TRUE + 5 # arithmetic operator between integer and numeric variables 5L + 3.14 # logical operator negation (!) applied to numeric variable !25 ## [1] 6 ## [1] 8.14 ## [1] FALSE Implicit conversion will only be performed if it is meaningful - eg an arithmetic operator between the character and numeric variables will result in a mistake 2.2 Operators As in other programming languages, R permits the use of operators in terms. Some of the more frequently used operators are: arithmetic +, -, *, /, **, %% (modulo), %/% comparison &lt;, &lt;=, &gt;, &gt; =, ==, != logical ! (negation), &amp;&amp; (scalar AND), || (scalar OR), &amp; (vector AND), | (vector OR) assignment &lt;- or= Assignment 2.5 - operators # try the `5 / 2` and` 5%/% 2` expressions # check how much &quot;square of 17&quot; and &quot;the remainder of 101 divided by 12&quot; are # check what is the result of the following expressions: `17 &gt; 13`,`!TRUE`, `5 &amp;&amp; 0`,`0. || 2` # try the `5 / 2` and` 5%/% 2` expressions 5 / 2 5 %/% 2 cat(&quot;-----------\\n&quot;) # check how much &quot;square of 17&quot; and &quot;the remainder of 101 divided by 12&quot; are 17 ^ 2 101 %% 12 cat(&quot;-----------\\n&quot;) # check what is the result of the following expressions: `17 &gt; 13`,`!TRUE`, `5 &amp;&amp; 0`,`0. || 2` 17&gt; 13 ! TRUE 5 &amp;&amp; 0 0. || 2 ## [1] 2.5 ## [1] 2 ## ----------- ## [1] 289 ## [1] 5 ## ----------- ## [1] TRUE ## [1] FALSE ## [1] FALSE ## [1] TRUE Logical values and comparison operators will most commonly be used with so-called “Conditional Execution of Commands”, known from other programming languages as the “IF ELSE” commands. In R, its syntax looks like this: if (expression) {block} else {block} Let’s try this command on the following task: `r zadHead(“conditional execution”) ** # Write a command that performs the following: # &quot;if 100 is an even number print &#39;Success!&#39;&quot; if (100 %% 2 == 0) print(&quot;Success!&quot;) ## [1] &quot;Success!&quot; We have noted above that we have two types of logical operators for “and” and “or”. We will explain the difference later, for now it is enough to say that we are almost exclusively using &amp;&amp; i || (“C++” operators!) with the conditional execution of commands or program loops. Likewise, we have already mentioned that R offers two assignment operators, &lt;- and =. There are some minor differences, but they do not have any impact on the normal use of this operator in practice. In the literature, both versions can be seen for assigning values, but we will primarily and consistently use &lt;-, mostly because the code is visually more distinctive than the other programming languages. NOTE: an easy way to type in the &lt;- operator is to press the keys ALT and - When assigning, we keep in mind that on the left there is so-called “left value” (lvalue). This is interpreted in the programmer’s sense as “something in which the calculated value can be stored”. x + 1 &lt;- 2 # error !!!] As a rule, in R as lvalue we can most commonly see a variable, though sometimes there may be a function call. This perhaps initially confusing feature will be clarified later. Naming the variables mostly follows the rules from other programming languages - letters, numbers, subfolders, and points are allowed. The first symbol must be a letter or a dot. .myVarijable &lt;- 5 #OK my.Variable &lt;- 5 #OK _myVariable &lt;- 5 # not OK 123Variable &lt;- 5 # not OK In practice for complex name variables we need to select one of the following conventions: myVariable &lt;- 1 # camelcase my_Variable &lt;- 2 # underscore seperation or my.variable &lt;- 3 # point separation It is important that we do not mix conventions in a program code, ie, after the selection, be consistent. If we insist on strange names using special characters, then we have to put them under the so called “left single apostrophes” (* backticks *): Assignment 2.6 - variable name with special characters # Enter an arbitrary name with special characters inside the left apostrophes # and print the value of the variable # `` &lt;- 2 `!% ^$*@ __ =` &lt;- 2 `!% ^$*@ __ =` ## [1] 2 Such a way of naming variables is not too useful in practice, but has its purpose - since the operators in R are actually functions (whose names are literally +, ** etc.) using left backticks we can directly reference them in their the original form, which can be very practical in the so-called functional programming (which we will talk about in one of the future lessons). Assigning values to new variable names we actually create new variables in the working environment (called the “global environment”“). All variables we have created so far can be seen with the ls() function. If we want to delete some variables, just give their names in the call function rm() (eg rm (x, y, z)). To delete * all * variables from the working environment, we use the call rm(list=ls ()), but we have to be careful (no”undo\"!). Assignment 2.7 - printing and deleting global environment variables # print all of the global environment variables that have been created so far # delete some of the above-written variables - eg rm(x, y, z) # list all remaining variables # delete ALL variables from the global environment # (cautiously with this call in practice!) # Make sure the global environment is empty # print all of the global environment variables that have been created so far ls() # delete some of the above-written variables - eg rm(x, y, z) # list all remaining variables rm(x, y) ls() # delete ALL variables from the global environment # (cautiously with this call in practice!) # Make sure the global environment is empty rm(list=ls()) ls() Finally, whenever we need help with some function, we have the following options available: write only &lt;function_name&gt; (without parenthesis with parameters) and press - if the function is written in R (and not just proxy to a C implementation) the screen will get a printout of the original code function Write help (&lt;function_name&gt;) or ? &lt;name_functions&gt; to get a help function page with the list of parameters, examples, and so on. Write example (&lt;function_name&gt;) where we get a list of examples of using the function and the results obtained The following code snippet shows how to use the above methods (due to space savings, we do not show their result). #program code for `ls` function ls # help for `ls` function ?ls # or help(ls) # examples of using the `ls` function example(ls) 2.3 Missing, unknown, and non-existent values In R there are three ways of modeling “non-existent” values: NA - (* not available *) Missing or unknown value of a particular type NaN - (* not a number *) “impossible”number, eg 0 / 0 NULL - non-existent value, literally “nothing” Assignment 2.8 - working with NA, NaN and NULL # how much is &quot;5 + unknown number&quot; # how much is &quot;5 + non-existent number&quot; # check classes of the following: # NA # arithmetic operation between numeric and NA # NaN # NULL # how much is &quot;5 + unknown number&quot; 5 + NaN # how much is &quot;5 + non-existent number&quot; 5 + NA cat(&quot;-----------\\n&quot;) # check classes of the following: # NA # arithmetic operation between numeric and NA # NaN # NULL class(NA) # logical type is &quot;weakest&quot;! class(5 + NA) class(NaN) class(NULL) ## [1] NaN ## [1] NA ## ----------- ## [1] &quot;logical&quot; ## [1] &quot;numeric&quot; ## [1] &quot;numeric&quot; ## [1] &quot;NULL&quot; Checking missing valuesis similar to checking data types - we use the is.na,is.nan and is.null functions. We have to keep in mind that NaN is a subclass of NA and that NULL is a completely separate class. This is especially important for SQL users - what is NULL in SQL isNA in R and that is what we normally use in practice, whereas NULL has very specific applications and is not so commonly used the program code. Assignment 2.9 - check NA, NaN and NULL # which of the following is NA? NA, NaN, NULL, &quot;&quot;, 0 # which of the following is NaN? NA, NaN, NULL # which of the following is NULL? NA, NaN, NULL # which of the following is NA? NA, NaN, NULL, &quot;&quot;, 0 is.na(NA) is.na(NaN) is.na(NULL) is.na(&quot;&quot;) is.na(0) cat(&quot;-----------\\n&quot;) # which of the following is NaN? NA, NaN, NULL is.nan(NA) is.nan(NaN) is.nan(NULL) cat(&quot;-----------\\n&quot;) # which of the following is NULL? NA, NaN, NULL is.null(NA) is.null(NaN) is.null(NULL) ## [1] TRUE ## [1] TRUE ## logical(0) ## [1] FALSE ## [1] FALSE ## ----------- ## [1] FALSE ## [1] TRUE ## logical(0) ## ----------- ## [1] FALSE ## [1] FALSE ## [1] TRUE To end, we dedicate some room to a discussion of the NA value, since we will often encounter it in practice. Simply put, if the NA values appear, we can expect the following side effects: Results of arithmetic expressions result in NA values the results of some function call result with NA (unless we specify compensation actions, such as the parameter na.rm = T which actually means ‘ignore NA’) logical expression results may not necessarily result in a NA value depending on whether the term depends onNA or not (eg TRUE || NA has the result of TRUE, but FALSE || NA has result NA) With this last one, we must be especially careful as the NA in the conditional term results in a mistake: if (NA &lt;2) print (&quot;Success!&quot;) # error !! In this lesson we have come to know the basic elements of the language R. In working with R, we usually work with complex types of data that we will learn in the following - namely, vectors, matrices, data frames and lists. Exercises What is the result of the following commands? Consider a possible result before executing. as.complex(2) as.integer(-3.25) as.logical(&quot;0&quot;) as.numeric(TRUE) as.character(11.5 + 2i) as.numeric(&quot;ABC&quot;) How do the following expressions look like in R: “three times ten on the power of nine” “logarithm of 5” “integer division of 10 by 3” “the remainder of tnteger division of 10 by 3” “tangent of 75 degrees” | Using the if expression, check whether the result of dividing whole number with NULL is considered to beNA, NaN orNULL. Enter x in the variable5. Print all environment variables. Then put NULL in the x variable. Is this variable still there? Program in Ru &lt;/ span&gt; by Damir Pintar is licensed under Creative Commons Attribution-NonCommercial-NoDerivative 4.0 International License Based on a work at https://ratnip.github.io/FER_OPJR/ "],
["vektori.html", "3 Vectors, matrices and lists 3.1 Vector 3.2 Index vectors 3.3 Matrices and arrays 3.4 Lists Exercises", " 3 Vectors, matrices and lists 3.1 Vector The vector is one of the main “complex” types of data in the language R, in the sense that it contains more values of the same type. It is similar to the term “array” in the C language. But there is one important difference here, which is necessarily adopted since it is one of the most important characteristics of the language R - in R (almost) each variable type is actually a vector. Even the variables and constants we used in the previous lesson were actually single-element vectors. This has far-reaching consequences to be discussed in detail below, and to begin with, we will first get acquainted with the syntax of creating and managing vectors. 3.1.1 Create a vector The new vector(having more than one element) is created using the c (from * combine *) function. # numeric vector m &lt;- c(1, 2, 3, 4, 5) # logic vector v &lt;- c(T, F, T) # character vector names &lt;- c(&quot;Ivo&quot;, &quot;Pero&quot;, &quot;Ana&quot;) So, simply said, the vector is arranged with a set of elements of the same type. If we create a new vector with elements of different types of data, R will automatically convert all elements into the “strongest” type, which will eventually become the type of vector itself (the term “stronger” type in this context means the type option to store all the information “stored in weaker” type, and in general the conversion goes in the direction of logic -&gt; numeric -&gt; character type). Assignment 3.1 - creating vectors # create a new vector `x` with four arbitrary elements of the following types: # logical, real, character and integer # print the vector content and its class screen # create a new vector `x` with four arbitrary elements of the following types: # logical, realistic, character and integer x &lt;- c(T, 1.25, &quot;Ivo&quot;, 10L) # print the vector content and its class on screen x class(x) ## [1] &quot;TRUE&quot; &quot;1.25&quot; &quot;Ivo&quot; &quot;10&quot; ## [1] &quot;character&quot; The vector can be explicitly converted to another type with the help of already familiar functions as.&lt;type_name&gt;. If the conversion is impossible to implement, the element will be converted to NA with a suitable warning. Assignment 3.2 - explicit vector type conversion x &lt;- c(1, T, 2L) y &lt;- c(1L, 2L, 3L) z &lt;- c(1.25, TRUE, &quot;Ana&quot;) # Consider a possible result and then try to make the following conversions # vector `x` in numeric type # vector `y` in character type # vector `z` in an integer type # Consider a possible result and then try to make the following conversions # vector `x` in numeric type # vector `y` in character type # vector `z` in an integer type as.numeric(x) as.character(y) as.integer(z) ## Warning: NAs introduced by coercion ## [1] 1 1 2 ## [1] &quot;1&quot; &quot;2&quot; &quot;3&quot; ## [1] 1 NA NA Can you answer the question - why in the last example did the value TRUE not become 1L but NA instead? Try to print the z vector and notice the results of the implicit conversion you might have neglected (which converts the TRUE logical value to a string of \"TRUE\" that can no longer be ‘returned’ to the numeric value 1L). With the c function we can also connect multiple vectors to one: a &lt;- c(1, 2, 3) b &lt;- c(4, 5) c &lt;- c(6, 7, 8) # variable can be called &quot;c&quot; in spite of the function c() d &lt;- c(a, b, c) # d is now c(1, 2, 3, 4, 5, 6, 7, 8) In addition to the c function, R also offers additional convenient ways of creating new vectors: : - “range” operator, giving the range from upper to lower bound, both included seq - sequence function, similar to the range operator but with additional options rep - replicate function, repeats the provided elements provided number of times Assignment 3.3 - vector creation helper functions # print the results of the following commands # 1: 5 # rep(c(1, 2, 3), times = 3) # rep(c(1, 2, 3), each = 3) # seq(1, 5, by = 0.5) # print the results of the following commands 1: 5 rep(c(1, 2, 3), times = 3) rep(c(1, 2, 3), each = 3) seq(1, 5, by = 0.5) ## [1] 1 2 3 4 5 ## [1] 1 2 3 1 2 3 1 2 3 ## [1] 1 1 1 2 2 2 3 3 3 ## [1] 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 Vectors can also be created by means of functions corresponding to the names of the vector types (numeric,character, etc.) whereby as a parameter we specify the desired length of the vector. This is often done as a “preparation” of the vector for subsequent filling up of real values, ie, a kind of reservation of the place in memory. What is interesting is the fact that we can also create an “empty” vector of a certain type that is still a vector, with only the length of zero (for which, for example, with the function c we can add elements later). x &lt;- numeric(2) # vector is filled with &quot;null&quot; elements, in this case (0, 0) y &lt;- character(5) z &lt;- integer(0) # &quot;empty&quot; vector! z &lt;- c(z, 1) # add to the vector the element 1 (actually &quot;merge empty vector and element 1&quot;) Finally, to check if some vector contains a certain element we can use the operator %in%: 4 %in% seq(1, 10, 2) # returns FALSE &quot;d&quot; %in% c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;) # returns TRUE Let’s see now how to access some vector elements 3.1.2 Operator [ The vector element is accessed through the index operator []`, with the help of which we can modify the vector elements: a &lt;- c(2, 4, 6) a[1] # prints a value of 2 a[2] &lt;- 5 # element on the 2nd place becomes 5 a[5] &lt;- 7 # at the 5th place is added 7, and the &quot;hole&quot; is filled with NA a ## [1] 2 ## [1] 2 5 6 NA 7 Notice a somewhat unusual fact - the first element of the vector in R has the index 1, not 0! This is an important difference compared to the indexing of elements in other programming languages. The reason for this specificity is simple - R is primarily considered as a language for data analysis, especially in tabular form, and in practice it is much easier to count rows or columns in order they appear in a data set than to do constantly “add 1”. The example above actually shows a very simplified case for retrieving vector elements and modifying them. One of the specifics of the language R is the so called vectorization, that is, the principle that R often works “more things at once” - not so much in terms of parallel execution, but in terms of declaring what we want to do. Specifically, in the case of vector indexing, we rarely retrieve or modify elements one by one, but rather we include a greater number of elements at once using the principle of vectorization and recycling. Understanding these terms is crucial for learning the R language, so we will explain them in detail below. 3.1.3 Principle of vectorization and recycling The notion of vectorization or more precisely vectorized operations and functions simply means that the operations work on multiple elements at once. If we ask R to make an operation or function over a vector of values, R will perform the function or operation over each element separately and return the vector as a result. Likewise, if we carry out the binary operation between two vectors, it will be performed over the “paired” or “aligned” elements of both vectors (suppose for now that the vectors are of the same length). Assignment 3.4 - vectorization principle x &lt;- seq(-5, 5, 1) a &lt;- 1:3 b &lt;- 4:6 # call the abs function to calculate the absolute value # over the vector `x` and print the result # add vectors `a` and` b` with operator `+` # and print the result # multiply vectors `a` and` b` with operator `*` # and print the result # call the abs function to calculate the absolute value # over the vector `x` and print the result abs(x) cat(&quot;-----------\\n&quot;) # add vectors `a` and` b` with operator `+` # and print the result a + b cat(&quot;-----------\\n&quot;) # multiply vectors `a` and` b` with operator `*` # and print the result a * b ## [1] 5 4 3 2 1 0 1 2 3 4 5 ## ----------- ## [1] 5 7 9 ## ----------- ## [1] 4 10 18 Carefully consider the results of the previous task. If necessary, sketch the vector a andb on the paper with the vertically loaded elements and notice how to R does the “pairing” of the elements. Notice that we are not talking about “vector operations” here in a strict mathematical sense, but about aligning the elements of two vectors and performing simple operations over each of these pairs. This is especially evident in the last example where there is no “multiplication of vectors” in some of the mathematical interpretations, but rather simple multiplication of the parallel elements of the two vectors. What if the vectors are not of the same length? R in this case uses principle of recycling. The Recycle Principle states that when the vectors are uneven, the shorter vector is “recycled” as many times as needed to reach the longer length of the vector. The most common scenarios of using this principle are operations where on one side we have a vector with multiple elements and on the other a single element vector. What we should avoid is a recycling scenario where the length of a “big” vector is not a multiple of the “small” length - R will still recycle a shorter vector, only it will have to be “cut off”, which will result in an appropriate warning. Assignment 3.5 - recycle principle a &lt;- 1:4 b &lt;- c(1, 2) c &lt;- rep (5, 3) # duplicate vector `a` and print the result # divide vector `a` with vector` b` and print the result # multiply vectors `a` and` c` and print the result a &lt;- 1:4 b &lt;- c(1, 2) c &lt;- rep (5, 3) # duplicate vector `a` and print the result 2 * a # divide vector `a` with vector` b` and print result a / b # multiply vectors `a` and` c` and print the result a * c ## Warning in a * c: longer object length is not a multiple of shorter object ## length ## [1] 2 4 6 8 ## [1] 1 1 3 2 ## [1] 5 10 15 20 Now we can finally demystify the difference between “scalar” and “vector” logic operators. Scala logical operators are intended for use with single-element vectors, they return unique values of TRUE or FALSE and are suitable for use in various conditional terms. Vector logical operators use standard R’s vectorization and recycling principles, ie, they are intended to work with logical vectors and as a result give a logical vector Assignment 3.6 - scalar and vector logical operators a &lt;- c(T, F, F) b &lt;- c(T, T, F) # apply scalar and vector version of logical operator &quot;or&quot; # over the `a` and` b` vectors and print the result # apply scalar and vector version of logical operator &quot;or&quot; # over the `a` and` b` vectors and print the result a || b a | b ## [1] TRUE ## [1] TRUE TRUE FALSE We see that the scalar version will “use” only the first pair of logic vector elements. This means we can use it in theory in conditional logic instructions, although there is no justified reason for it, and R will in this case warnus to address the fact that we are probably using the “wrong” operator. The next example with the comparison operators may initially seem trivial, but it is important to pay special attention to the results we get since they will have a very important implications later on. So let’s take a look at what happens in the vectorization of the parallel operators. Assignment 3.7 - vectorization of parallel operators x &lt;- 1:5 y &lt;- seq(-10, 10, 5) # print x and y # print the result of the x &gt; y command and explain the result # print the result of the x &lt; 3 command and explain the result # print x and y x y cat(&quot;-----------\\n&quot;) # print the result of the x &gt; y command and explain the result x&gt; y cat(&quot;-----------\\n&quot;) # print the result of the x &lt; 3 command and explain the result x &lt; 3 ## [1] 1 2 3 4 5 ## [1] -10 -5 0 5 10 ## ----------- ## [1] TRUE TRUE TRUE FALSE FALSE ## ----------- ## [1] TRUE TRUE FALSE FALSE FALSE Thus, by vectorizing the comparison operators over the vectors (or combinations of vectors and scalars), as a result we get logical vectors. The interpretation of these results is crucial - it actually answers the question “on what indexes is the condition fulfilled by this expression”? In other words, the results actually represent a template that describes how to filter elements of a vector. This is the basic foundation of the so-called. logical Indexing, which is one of the indexing methods that we will learn below. 3.2 Index vectors We have already learned that a vector can be retrieved through a numerical index (and we did not forget the fact that the first element has an index 1). This concept can be expanded by taking multiple elements from the vector at once. which is often referred to as “slicing”. The basic principle of choosing multiple elements at once is simple - we only need to specify the indexes of the elements we want. R offers three basic ways of crawling: integer- or location- based indexing conditional or boolean-based indexing label-based indexing Which indexing we choose depends on whether we want to access the elements depending on their location, name, or condition, and each type of indexing essentially amounts to the use of a particular vector type as a parameter for the indexing operator. Such a vector is called an “index vector” because of its role. Let’s get to know each of the types of indexing in detail. 3.2.1 Location Indexing Location Indexing is the generalization of an already familiar indexing principle where we state the ordered numbers (indices) of elements we are interested in. If we want more elements, we simply put their indices “packed” into a numeric vector. Try solving the next task by using the appropriate numeric vectors as indexing parameters. Assignment 3.8 - location-based indexing x &lt;- 1:10 # print the first element of x # print the first three vector elements x # print the first, fifth, and seventh elements of the vector x # print the first element of x x[1] # print the first three vector elements x x[1:3] # print the first, fifth, and seventh elements of the vector x x[c(1,5,7)] ## [1] 1 ## [1] 1 2 3 ## [1] 1 5 7 Thus, the location index vector is nothing other than the ordinary numeric vector we use together with the index operator to determine which elements of another vector we want to “keep”. Let’s look at some of the features of the location index vector: Assignment 3.9 - location indexing (2) x &lt;- 1:10 # answer the following questions with the help of an appropriate example # what does index 0 return? # what does a negative index return? # what happens if you use an index outside of vector boundaries x &lt;- 1:10 # answer the following questions with the help of an appropriate example # what does index 0 return? x[0] # what does a negative index return? x[-1] # what happens if you use an index outside of vector boundaries x[20] ## integer(0) ## [1] 2 3 4 5 6 7 8 9 10 ## [1] NA Indexing is not only used to retrieve elements. By combining the indexing operator and the assignment operator we can change the vector elements (also by the principle of “multiple elements at once”: Assignment 3.10 - location-based indexing and assignment a &lt;- 1:10 # set all vector elements of `a` from the second to the eighth place to zero # print vector `a` b &lt;- 1:20 b [2 * 1:5] &lt;- 0 # Consider what the vector `b` looks like after the above command # print the vector `b` and explain the result a &lt;- 1:10 # set all vector elements of `a` from the second to the eighth place to zero # print vector `a` a [2:8] &lt;- 0 a b &lt;- 1:20 b [2 * 1:5] &lt;- NA # Consider what the vector `b` looks like after the above command # print the vector `b` and explain the result b ## [1] 1 0 0 0 0 0 0 0 9 10 ## [1] 1 NA 3 NA 5 NA 7 NA 9 NA 11 12 13 14 15 16 17 18 19 20 3.2.2 Conditional indexing If we carefully considered the results obtained with examples with vectorized comparison operators then we can very well grasp the way conditional indexing works. The principle is simple - for the index vector we set a logical vector of the same length as the vector whose elements we want to retrieve. The logic vector elements determine which elements are retained (the positions where the value is TRUE) and which we reject (positions where the value is FALSE). Assignment 3.11 - conditional indexing x &lt;- 1:10 # create a logical vector `y` of length 10 with an arbitrary combination of # TRUE and FALSE values # index the vector `x` with the` y` vector, print and explain the result # print all vector elements `x` which are less or equal to 5 # use the appropriate expression as a logical index vector x &lt;- 1:10 # create a logical vector `y` of length 10 with an arbitrary combination of # TRUE and FALSE values y &lt;-c(T, T, F, T, F, F, F, T, F, T) # index the vector `x` with the` y` vector, print and explain the result x[y] # print all vector elements `x` which are less or equal to 5 # use the appropriate expression as a logical index vector x[x &lt;= 5] ## [1] 1 2 4 8 10 ## [1] 1 2 3 4 5 The last command, while simple, is one of the key principles for filtering elements in the language R. The combination of the index operator and the conditional expression represents a concise but very powerful vector filtering mechanism. Let’s try this principle in a few more examples. Assignment 3.12 - conditional indexing y &lt;- seq (1, 100, 7) students &lt;- c(&quot;Ivo&quot;, &quot;Petra&quot;, &quot;Marijana&quot;, &quot;Ana&quot;, &quot;Tomislav&quot;, &quot;Tin&quot;) # print all even, and then all odd vector elements of `y` # print all vector elements from `students` which represent 3-letter names # (note: we use the `nchar` function to count the characters in R) y &lt;- seq (1, 100, 7) students &lt;- c(&quot;Ivo&quot;, &quot;Petra&quot;, &quot;Marijana&quot;, &quot;Ana&quot;, &quot;Tomislav&quot;, &quot;Tin&quot;) # print all even, and then all odd vector elements of `y` y[y %% 2 == 0] y[y %% 2 != 0] # print all vector elements from `students` which represent 3-letter names # (note: we use the `nchar` function to count the characters in R) students [nchar(students) == 3] ## [1] 8 22 36 50 64 78 92 ## [1] 1 15 29 43 57 71 85 99 ## [1] &quot;Ivo&quot; &quot;Ana&quot; &quot;Tin&quot; If the concept of conditional indexing with the help of conditional expressions is still unclear, one of the things that can help is to sketch the intermediate results - simply print the result of the expression within the square bracket of the index operator and then consider how that result affects the final solution. 3.2.3 Label-based indexing label-based indexing works on the principle of explicitly naming the elements we want to “keep”. In order to be able to use this type of indexing we must meet the necessary prerequisite - vector elements must have predefined “names”. The vectors we used so far did not have named elements. Each element had its predefined position within the vector and its value but did not have any special additional identifiers. Programming language R allows you to attach names to vector elements in a very simple way - using a combination of the names function, the assignment operator, and the character vector with selected names. We need to make sure that the vector name is of the same length as the original vector! Assignment 3.13 - label-based indexing height &lt;- c(165, 173, 185, 174, 190) names (height) &lt;- c(&quot;Marica&quot;, &quot;Pero&quot;, &quot;Josip&quot;, &quot;Ivana&quot;, &quot;Stipe&quot;) # print the vector `height` # print the height of Pero and Josip height &lt;- c(165, 173, 185, 174, 190) names (height) &lt;- c(&quot;Marica&quot;, &quot;Pero&quot;, &quot;Josip&quot;, &quot;Ivana&quot;, &quot;Stipe&quot;) # print the vector `height` height # print the height of Pero and Josip height [c(&quot;Pero&quot;, &quot;Josip&quot;)] ## Marica Pero Josip Ivana Stipe ## 165 173 185 174 190 ## Pero Josip ## 173 185 We see that label-based indexing needs a corresponding character vector as the index parameter. (NOTE: A more careful reader will notice an unusual fact in the program code - function call is used as a lvalue! Answer to the question why this works may require a little more knowledge of the internal functioning of the R language and for now it is enough to say that here it’s actually calling a function called names&lt;- which is “hidden” behind a much more intuitive and easy-to-understand syntax). If for some reason we want to delete the names of vector elements, simply forward NULL to names. names &lt;- NULL This will conclude the story of the index vectors. We learned different ways of creating a vector and getting and modifying its elements. Now is the time to try to add the additional “dimension” to the vectors - let’s get to know the matrices and the arrays. 3.3 Matrices and arrays The matrices and the arrays are simply - multidimensional vectors. Matrix is a vector of two dimensions, that is, a vector that organizes elements in “rows” and “columns”. Array is a vector with three or more dimensions. While matrices are used relatively often in practice, the arrays are somewhat more limited to specific scenarios. Because of this fact in this chapter we will mainly deal with matrices, although the concepts presented are very easily applicable to arrays. What is common to the matrices and arrays, which is a well-known fact to readers with programmatic experience, is that their multidimensionality is actually virtual. Both the matrix and the array are actually one-dimensional vectors with the attribute of dimensionality, and with this attribute the language R maps our multidimensional indexing to the “real” index of the element of the one dimensional vector. This fact does not limit us - we can still in most cases treat the matrix as if it is actually two-dimensional, and knowledge of one-dimensional nature can only give us additional flexibility in working with the matrices. There are several ways to create a new matrix: With the help of the matrix function to which we forward the one-dimensional vector and the desired number of rows and columns through the nrow and ncol parameters “manually” by setting dimensions of a one dimensional vector using the dim function and associating a two-element numeric vector with matrix dimensions “binding” rows or columns together with functions rbind (row-bind) andcbind (column-bind) We will demonstrate these ways in the following examples. Assignment 3.14 - matrix function x &lt;- 1:12 # create a matrix with 3 rows and 4 columns using the `matrix` function # print the result on the screen # repeat the procedure but add the parameter `byrow = T` to the calling function # print the result on the screen and compare it to the previous result # create a matrix with 3 lines and 4 columns using the `matrix` function # print the result on the screen matrix (x, nrow = 3, ncol = 4) # repeat the procedure but add the parameter `byrow = T` to the calling function # print the result on the screen and compare it to the previous result matrix (x, nrow = 3, ncol = 4, byrow = T) ## [,1] [,2] [,3] [,4] ## [1,] 1 4 7 10 ## [2,] 2 5 8 11 ## [3,] 3 6 9 12 ## [,1] [,2] [,3] [,4] ## [1,] 1 2 3 4 ## [2,] 5 6 7 8 ## [3,] 9 10 11 12 Note that unless explicitly requested otherwise, the R matrix is filled by columns. This is done because of the similarity of the matrix with the tabular representation of the data most often analyzed by looking at individual columns. But since we often feel that filling by the rows is more “natural”, we must not forget the very useful parameter of byrow. Assignment 3.15 - dim function m &lt;- 1:10 # print the result of call of the `dim` function to the vector` m` # use the `dim` function to vector` m` with the assigment of the vector c(2, 5) # print `m` and comment the result # print the call results of `nrow` and` ncol` on the matrix `m` m &lt;- 1:10 # print the result of call of the `dim` function to the vector` m` dim (m) # use the `dim` function to vector` m` with the assigment of the vector c(2, 5) dim (m) &lt;- c(2, 5) # print `m` and comment the result m # print the call results of `nrow` and` ncol` on the matrix `m` nrow(m) ncol(m) ## NULL ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1 3 5 7 9 ## [2,] 2 4 6 8 10 ## [1] 2 ## [1] 5 We see that the “ordinary” vector does not actually have a dimension, which is manifested by the NULL values we get as a result. By invoking the function dim we can add the attribute dim so it formally becomes a matrix (or array in general case). Exact dimensions are those that define how elements are organized in rows and columns, and by setting dimensions we need to be careful that they match the current number of elements. Once the matrix has dimensions added, we can retrieve them again by using the dim function, or just the number of rows or columns with the nrow and ncol functions. The resulting matrix is like the one in the previous example filled in by the columns. Since here we do not have the opportunity to use the byrow parameter, one of the ways to get a matrix filled by rows is to transpose the resulting result with the t function. m &lt;- t(m) # transpose the matrix and store it back in the variable `m` Finally, a matrix can be created by “gluing” rows and columns with the help of rbind andcbind. This is also a convenient way to add new rows and columns to an existing matrix. Assignment 3.16 - functions ‘rbind’ and ‘cbind’ a &lt;- 1:4 b &lt;- 5:8 c &lt;- c(0.0) # create a matrix `m` in which vectors `a` and `b` will be columns # add a new row to the top of the matrix `m` with vector elements` c` # print matrix `m` a &lt;- 1:4 b &lt;- 5:8 c &lt;- c(0.0) # create a matrix `m` in which vectors` a` and `b` will be columns m &lt;- cbind (a, b) # add a new row to the top of the matrix `m` with vector elements` c` # print matrix `m` m &lt;- rbind (c, m) m ## a b ## c 0 0 ## 1 5 ## 2 6 ## 3 7 ## 4 8 3.3.1 Matrix splicing All the learned principles for “cutting” the vector using index vectors can be applied on matrices. The differences are as follows: we index each dimension individually first we index the lines, then the columns, and divide the index vectors by comma If we want “all rows” or “all columns” we simply omit this index vector( but we still use a comma) # assume that `m &#39;is a matrix of 3 x 5, with column names from `a` to `e` m[1, 2:5] # first line, all columns from second to fifth m[c(F, T, T), c(&quot;a&quot;, &quot;b&quot;)] # second and third rows, columns `a` and` b` m[,] # all rows and all columns (same as just `m`) In practice, the matrix usually uses location-based and label-based indexing; conditional indexing is not too practical because of the two-dimensional nature of the matrix (although it is feasible, we just have to keep in mind that the logic index vector lengths correspond to the corresponding dimension). One of the things we need to keep in mind is the R-language tendency to “help” us by simplifying the result. Thus, the result of a row cutting operation that leaves only one row or column will automatically become a vector, ie lose the dimension attribute. This sometimes does not suit us, especially if we are working on scripts that expect a matrix further down the line, even though it has the dimension of rows or columns 1. In this case, we need to set an additional parameter drop = F for indexing. This often seems quite unattractive, which is why there are many R-language packages that “repair” this, that is to say, try to leave the results consistent. But the drop parameter set toFALSE should be taken into account, as it will appear in other places in a similar function. Assignment 3.17 - matrix splicing m &lt;- matrix (1:30, 6, 5, T) colnames (m) &lt;- c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;, &quot;e&quot;) # print all elements of the matrix m from the second to the fourth line # and from the third to the fifth column # set all elements in column &quot;c&quot; to zero # and then print the first two lines of matrix `m` # print only column &quot;d&quot; # rewrite column &quot;d&quot;, but add the `drop = FALSE` parameter when indexing # Separate the parameter with a comma (as if it was a &quot;third&quot; indexing dimension) m &lt;- matrix (1:30, 6, 5, T) colnames (m) &lt;- c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;, &quot;e&quot;) # print all elements of the matrix `m` from the second to the fourth line # and from the third to the fifth column m [2:4, 3:5] # set all elements in column &quot;c&quot; to zero # and then print the first two lines of matrix `m` m [, &quot;c&quot;] &lt;- 0 m [1:2] # print only column &quot;d&quot; m [, &quot;d&quot;] # rewrite column &quot;d&quot;, but add the `drop = FALSE` parameter when indexing # Separate the parameter with a comma (as if it was a &quot;third&quot; indexing dimension) m [, &quot;d&quot;, drop = F] ## c d e ## [1,] 8 9 10 ## [2,] 13 14 15 ## [3,] 18 19 20 ## [1] 1 6 ## [1] 4 9 14 19 24 29 ## d ## [1,] 4 ## [2,] 9 ## [3,] 14 ## [4,] 19 ## [5,] 24 ## [6,] 29 Here we will finish the story of the matrices. These structures are very useful in solving linear algebra tasks, so it is often convenient to look at the documentation of the R language to see which functions and operators are available to us for such a job. Similarly, some of the principles of matrix management will be useful in managing the so-called “data frames” - one of the most useful data structures in R. Finally, although we will not deal in detail with the arrays, we will show the example of the program code that creates a three-dimensional array and then uses standard cutting principles we encountered with the vectors and matrices. myArray &lt;- array(1:24, dim = c(2, 3, 4)) # array of dimension 2 x 3 x 4 myArray[, 1:2, 3, drop = FALSE] # print all rows, first and second columns # 3rd &quot;layer,&quot; with array type retention 3.4 Lists The list is an element in R used as a “universal container” of any data. Unlike the vector (ie, the concept of vector as we initially defined it), the list may contain different types of data or, more often, sets of different types of data. We create the list with the list function by which we add a string of names of elements and their content. These elements can be anything, even other lists. myList &lt;- list(a = 1, b = 2:100, c = list(x = 1, y = 2)) Try to create your own list in the following example. Assignment 3.18 - list creation # create a new list called `stuff` that will have the following elements # element called `numbers&#39; with integers from 1 to 3 # element called `letters&#39; with letters &#39;A&#39; and &#39;B&#39; # nameless element with logical vector `c(T, F)` # element called `titles&#39; with the concent of &#39;Mr&#39;, &#39;Mrs&#39; and &#39;Ms&#39; # print the `stuff` variable stuff &lt;- list(numbers = c(1,2,3), letters = c(&quot;A&quot;, &quot;B&quot;), c(T, F), titles = c(&quot;Mr&quot;, &quot;Mrs&quot;, &quot;Ms&quot;)) # print the `stuff` variable stuff ## $numbers ## [1] 1 2 3 ## ## $letters ## [1] &quot;A&quot; &quot;B&quot; ## ## [[3]] ## [1] TRUE FALSE ## ## $titles ## [1] &quot;Mr&quot; &quot;Mrs&quot; &quot;Ms&quot; Note that the list keeps the order of elements - the element without the name is shown in index 3. The str (“structure”) function allows us to inspect the properties and list contents without printing the entire list. This function is often used by analysts, both for viewing the list and for a quick insight into the already mentioned data frames that we will work with in the next chapter. Assignment 3.19 - list structure # print the structure of the `stuff` list # print the structure of the `stuff` list str(stuff) ## List of 4 ## $ numbers: num [1:3] 1 2 3 ## $ letters: chr [1:2] &quot;A&quot; &quot;B&quot; ## $ : logi [1:2] TRUE FALSE ## $ titles : chr [1:3] &quot;Mr&quot; &quot;Mrs&quot; &quot;Ms&quot; At the beginning of this lesson, we have said that the principle “all is a vector” is very important in R and that the vectors actually have arranged sets of elements of the same type. From this one could conclude that this fact is not valid for the lists - they obviously contain elements of different types. But the real answer is - the lists are actually vectors, and the definition is actually still consistent. That is, all the elements of the list are actually a small single-element lists, so all elements are formally of the same type. Assignment 3.20 - list element type # print the first element of the list `stuff` # check its type # print the first element of the list `stuff` stuff[1] # check its type typeof(stuff[1]) ## $numbers ## [1] 1 2 3 ## ## [1] &quot;list&quot; So, we’ve proved that the list elements are actually a small list, as seen from printing the item itself, as well as checking its type. It may seem that the elements above created lists should be vectors, since we have created the list by “stacking” different vectors, however in the process of list creation all these elements are wrapped in single-element lists. Often we do not want to work with a list element as a “small list”, but want to have it in its “original” form. For this we use the operator [[, ie the operator of “double angular brackets”. Assignment 3.21 - operator [[ # print the first element of the list `stuff` using the operator `[[` # check its type # print the first element of the list `stuff` using the operator `[[` stuff[[1]] # check its type typeof(stuff[[1]]) ## [1] 1 2 3 ## [1] &quot;double&quot; The aforementioned operator is most often used to retrieve the selected element of the list defined by the number or (if defined) by the element name. In this approach, we must use the syntax such as list[[name_element]] symbol, which is somewhat clumsy for typing. Because of this, R offers an alternative way of accessing the list elements by their name using the $ operator, ie list$name_element. Assignment 3.22 - operator ‘$’ # print the `letters` element of the `stuff` list # using `[[` the operator # print the `letters` element of the `stuff` list # using the `$` operator # print the `letters` element of the `stuff` list # using `[[` the operator stuff[[&quot;letters&quot;]] # print the `letters` element of the `stuff` list # using the `$` operator stuff$letters ## [1] &quot;A&quot; &quot;B&quot; ## [1] &quot;A&quot; &quot;B&quot; The lists are an extremely popular type of object in R, as they represent a universal template for more complex data structures, including more complex objects in the narrower sense (as we will see later). The list is also the “foundation” for the most popular and most commonly used element of the R-language: the data frame - which we will learn in the next lesson. Finally, we learn how to add an element to the list. This is easiest to do using the aforementioned operator $ - such as list$newElement &lt;- newElementContents. The element is deleted by assigning the value NULL. Assignment 3.23 - adding list elements # in the `stuff` list add the `evenNumbers` element which contains # all even numbers from 1 to 100 # delete the third element from the list # print the `stuff` list # in the `stuff` list add the `evenNumbers` element which contains # all even numbers from 1 to 100 stuff$evenNumbers &lt;- seq(2, 100, 2) # delete the third element from the list stuff[[3]] &lt;- NULL # print the `stuff` list print(stuff) ## $numbers ## [1] 1 2 3 ## ## $letters ## [1] &quot;A&quot; &quot;B&quot; ## ## $titles ## [1] &quot;Mr&quot; &quot;Mrs&quot; &quot;Ms&quot; ## ## $evenNumbers ## [1] 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 ## [18] 36 38 40 42 44 46 48 50 52 54 56 58 60 62 64 66 68 ## [35] 70 72 74 76 78 80 82 84 86 88 90 92 94 96 98 100 In the next lesson, we will finally get to know the often-mentioned data frames as the most popular and most commonly used data structures of the language R. Exercises Create the following vectors: (11, 12, 13,…, 99) (0, 0, 0, 0, … , 0) (100 zeris) (0, 0.1, 0.2, …., 1.0) What is the sum of all numbers from 101 to 1001, if we skip all numbers divisible by 10? Use the sum function. Create a 3 x 3 matrix by performing the following commands (the sample function will be done in one of the following lessons): # we create a matrix of 3x3 randomly selected elements from 1 to 100 set.seed(1234) m &lt;- matrix(c(sample(1:100, 9, T)), nrow = 3, ncol = 3, byrow = T) Calculate the inverse matrix with the solve function. Make sure the multiplication of the original and inverse matrix result with the unit matrix (use the % *% operator to multiply the matrices). Initialize the stuff list used in the lesson. Do the following: print the class of the second element of the list print the element in the third place of the element of the list named letters check the length of the element called titles and add the title ‘Prof’ to the last position check if the number 4 is contained in the first element of the list add a new list of three vectors a,b and c which all contain elements (1,2,3) to the last place of the list, Program in Ru &lt;/ span&gt; by Damir Pintar is licensed under Creative Commons Attribution-NonCommercial-NoDerivative 4.0 International License Based on a work at https://ratnip.github.io/FER_OPJR/ "],
["okviri.html", "4 Data frames and factors 4.1 Data frames 4.2 Selecting rows and columns 4.3 Adding and deleting rows and columns 4.4 Factors Exercises", " 4 Data frames and factors 4.1 Data frames As already mentioned, the data frame is by far the most popular element of the programming language R. The language R’s primary fnction is data analysis, and the data frame is actually an object representation of the data set we intend to analyze. In other words, the data frame is an object similar in function to a sheet in Microsoft Excel or a table in a relational database. Almost every session in R is oriented around manipulating data frames - but while in Excel we manage the table with the help of a graphical interface, and in relational database via the query language SQL, in R we manage data programatically through the data frames. Take for example the following table: zipCode cityName avgSalKn population cityTax 10000 Zagreb 6359.00 790017 18 51000 Rijeka 5418.00 128384 15 21000 Split 5170.00 167121 10 31000 Osijek 4892.00 84104 13 20000 Dubrovnik 5348.00 28434 10 This data set that contains certain parameters related to cities in the Republic of Croatia (these valuesdo not necessarily correspond to the current state but are used only for demonstration). We can easily imagine how to write this data in Excel or create a relational table of names, for example, where we store the data. Let’s show now how to manipulate the same information within the R language, ie let’s try to create a data frame that will contain this data. In the last lesson we noted that the list as a complex type which serves as a kind of “template” with the help of which we can collect a number of different objects within the same structure. The data frame is in fact nothing but a list - that is, a “container” that can contain other data containers of different types. But while the “list” is actually a universal container, ie we do not have the limitations about stuff we place into it, the data frame has certain restrictions. The most important limit imposed by the data frame is that each element within the data frame must have the same number of elements. Why is that so? Let’s imagine a list in which each element has the same number of sub-elements. If each element is sketched vertically, with subelements written one below another, then these sub-elements will be “aligned” by the rows, which means that we have achieved classical data organization in columns (list elements) and rows (aligned sub-elements). Thus, this restriction directly imposes a “tabular” or “matrix” list structure with clearly defined rows and columns, which actually allows us to manage the data frame by using not only list methods, but also methods primarily intended for matrices. There are several ways to create data frames, and we’ll show two of the most frequently encountered scenarios in practice: Programatic creation via the data.frame function Load data from an external source using the read.csv function Let’s see both of these cases. First, we will create a data frame programatically. Assignment 4.1 - creating data frames programatically cities &lt;- data.frame(zipCode = c(10000, 51000, 21000, 31000, 2000), cityName = c(&quot;Zagreb&quot;, &quot;Rijeka&quot;, &quot;Split&quot;, &quot;Osijek&quot;, &quot;Dubrovnik&quot;), avgSalKn = c(6359., 5418., 5170., 4892., 5348.), population = c(790017, 128384, 167121, 84104, 28434), cityTax = c(18, 15, 10, 13, 10)) # print the variable `cities` cities &lt;- data.frame(zipCode = c(10000, 51000, 21000, 31000, 2000), cityName = c(&quot;Zagreb&quot;, &quot;Rijeka&quot;, &quot;Split&quot;, &quot;Osijek&quot;, &quot;Dubrovnik&quot;), avgSalKn = c(6359., 5418., 5170., 4892., 5348.), population = c(790017, 128384, 167121, 84104, 28434), cityTax = c(18, 15, 10, 13, 10)) # print the variable `cities` cities ## zipCode cityName avgSalKn population cityTax ## 1 10000 Zagreb 6359 790017 18 ## 2 51000 Rijeka 5418 128384 15 ## 3 21000 Split 5170 167121 10 ## 4 31000 Osijek 4892 84104 13 ## 5 2000 Dubrovnik 5348 28434 10 If you like, try creatint the upper data frame again, but with different numbers of vector element numbers that make up the columns. This operation will result in an error with a appropriate message and the data frame will not be created - R tries to keep the matrix nature of the frame always preserved. A little note regarding terminology: the “data frame” is often referred to as simply a “frame” or a “table”. Likewise, we often use the “column”, “variable” or “attribute” terms for the vertical data frame elements, while referring to the horizontal frame elements as “rows” or “observations”. These terms are in line with the standard way of referencing the table elements and the statistical terms referring to the datasets. If there is a chance of ambiguity from the context, the term that clearly describes the referenced element will be used. Let’s try to load the table from an external source now. Although R allows different forms of external data, we will assume that the data is obtained in a standard “CSV form” (CSV - comma-separated values). This form is one of the most popular data storage methods in a pure text format that has the advantage of being easy to create manually, and most data management tools implement logic to import/export data as a CSV file. Below we can see an example of a CSV file that matches the data frame created in the previous example. Suppose the file is named cities.csv. The data is separated by a comma (no spaces!), Every observation in your line, and the optional first line represents the column names. zipCode,cityName,avgSalKn,population,cityTax 10000,Zagreb,6359.00,790017,18 51000,Rijeka,5418.00,128384,15 21000,Split,5170.00,167121,10 31000,Osijek,4892.00,84104,13 20000,Dubrovnik,5348.00,28434,10 One of the potential problems with CSV files is that they use the comma as separator (delimiter) of column elements, and in certain languages, as a standard, a “decimal comma” is used instead of a decimal point. Because of this fact, there is an “alternative” CSV standard that uses a semi-colon as a separator so that our CSV file in this case looks like this (let’s call it citiesAlt.csv): zipCode;cityName;avgSalKn;population;cityTax 10000;Zagreb;6359,00;790017;18 51000;Rijeka;5418,00;128384;15 21000;Split;5170,00;167121;10 31000;Osijek;4892,00;84104;13 20000;Dubrovnik;5348,00;28434;10 Since the decimal point is a standard in the Republic of Croatia, in working with CSV files we have to be careful which of the two standard records is used. Luckily, the R language offers support functions for both standards, so we do not have to adapt the input files specifically, just be careful what function we will choose. Suppose we have these two files in the workbook: cities.csv citiesAlt.csv (If we do not have these files available, we can easily create them with the help of plain text editors (eg Notepad or gedit) and copying the above rows.) To create data frames from CSV files we use the following functions: - read.csv - for “normal” CSV files with a comma as separator - read.csv2 - for an alternative CSV standard that use semi-colons The main input parameter of these functions is the path to the CSV file that we load. Functions also have a rich set of additional parameters that allow them to be adapted to different scenarios, and if we get some of the “exotic” forms of CSV files, it is worth looking at the read.table function which is very flexible with respect to the number of different parameters and data load settings. Functions read.csv andread.csv2 are actually derived from the read.table function by defaulting certain parameters to standard CSV features. Some of the parameters and associated values of read.csv (or read.table) functions that are useful to know are: header = FALSE - for files without a header sep = \"#\" - for files that use some more “exotic” separator, in this case # na.strings = 'NULL' - the term used in the dataset to represent the missing values that will become NA in R nrows = 2000 - maximum number of lines to be read, in this case 2000 stringsAsFactors = F - preventing automatic creation of factor columns (which we will learn in the lesson below) encoding = 'UTF-8' - for non-ASCII text encoding standards (especially if we are working with Croatian speaking area data using diacritical characters) Let’s try to load data from available CSV files now. This data will not require special parameters, and will only be loaded by providing the path to the associated files (one that uses the comma and the other that uses the semi-colon as a separator). Assignment 4.2 - reading CSV files # load data from files `cities.csv` and` citiesAlt.csv` # save data in variables called `cities2` and` cities3` # print `cities2` and` cities3` # load data from files `cities.csv` and` citiesAlt.csv` # save data in variables called `cities2` and` cities3` cities2 &lt;- read.csv(&quot;cities.csv&quot;) cities3 &lt;- read.csv2(&quot;citiesAlt.csv&quot;) # print `cities2` and` cities3` cities2 cat(&quot;-----------\\n&quot;) cities3 ## zipCode cityName avgSalKn population cityTax ## 1 10000 Zagreb 6359 790017 18 ## 2 51000 Rijeka 5418 128384 15 ## 3 21000 Split 5170 167121 10 ## 4 31000 Osijek 4892 84104 13 ## 5 20000 Dubrovnik 5348 28434 10 ## ----------- ## zipCode cityName avgSalKn population cityTax ## 1 10000 Zagreb 6359 790017 18 ## 2 51000 Rijeka 5418 128384 15 ## 3 21000 Split 5170 167121 10 ## 4 31000 Osijek 4892 84104 13 ## 5 20000 Dubrovnik 5348 28434 10 Let’s look at some useful functions for working with data frames, ie tables. A good part of them is already known from experience in working with lists and matrices: nrow - number of lines ncol orlength - the number of columns (since the frame behaves both as a matrix and as a list) dim - table dimensions names - column names head - prints several rows from the beginning of the table tail - prints several rows from the end of the table str - prints table structure summary - summarizes the statistical information about table columns Let’s try some of these functions: Assignment 4.3 - working with data frames # print the dimensions of the data frame `cities` # print the table structure of `cities` # print the first few rows of the data frame `cities` # print summarized statistical information about `cities` # print the dimensions of the data frame `cities` dim(cities) cat(&quot;-----------\\n&quot;) # print the table structure of `cities` str(cities) cat(&quot;-----------\\n&quot;) # print the first few rows of the data frame `cities` head(cities) cat(&quot;-----------\\n&quot;) # print summarized statistical information about `cities` summary(cities) ## [1] 5 5 ## ----------- ## &#39;data.frame&#39;: 5 obs. of 5 variables: ## $ zipCode : num 10000 51000 21000 31000 2000 ## $ cityName : Factor w/ 5 levels &quot;Dubrovnik&quot;,&quot;Osijek&quot;,..: 5 3 4 2 1 ## $ avgSalKn : num 6359 5418 5170 4892 5348 ## $ population: num 790017 128384 167121 84104 28434 ## $ cityTax : num 18 15 10 13 10 ## ----------- ## zipCode cityName avgSalKn population cityTax ## 1 10000 Zagreb 6359 790017 18 ## 2 51000 Rijeka 5418 128384 15 ## 3 21000 Split 5170 167121 10 ## 4 31000 Osijek 4892 84104 13 ## 5 2000 Dubrovnik 5348 28434 10 ## ----------- ## zipCode cityName avgSalKn population ## Min. : 2000 Dubrovnik:1 Min. :4892 Min. : 28434 ## 1st Qu.:10000 Osijek :1 1st Qu.:5170 1st Qu.: 84104 ## Median :21000 Rijeka :1 Median :5348 Median :128384 ## Mean :23000 Split :1 Mean :5437 Mean :239612 ## 3rd Qu.:31000 Zagreb :1 3rd Qu.:5418 3rd Qu.:167121 ## Max. :51000 Max. :6359 Max. :790017 ## cityTax ## Min. :10.0 ## 1st Qu.:10.0 ## Median :13.0 ## Mean :13.2 ## 3rd Qu.:15.0 ## Max. :18.0 4.2 Selecting rows and columns We’ve already said that data frames behave both as matrices and as lists, which is a feature we often use when selecting rows and columns of data frames. Specifically, we use the following: for selecting rows and columns: two-dimensional indexing with the help of index vectors for select a column: operator $ Here we are actually quite flexible - we can, for example, first get certain matrix rows with the help of location indexing and then only single column with the help of $. In practice, one of the most common combinations is the conditional selection of rows with the named selection of columns (SQL experts will recognize this as a standard combination of WHERE and SELECT). Let’s try to apply our knowledge of index vectors, matrices, and lists. Assignment 4.4 - selecting rows and columns # print the table `cities` (for reference) # print the first three rows, the third and fifth column # print column &quot;cityTax&quot; # print zipCodes and city names of all cities which # have cityTax greater than 12% and a population of more than 100,000 # print the table `cities` (for reference) cities cat(&quot;-----------\\n&quot;) # print the first three rows, the third and fifth column cities[1:3, c(2,5)] cat(&quot;-----------\\n&quot;) # print column &quot;cityTax&quot; cities$cityTax cat(&quot;-----------\\n&quot;) # print zipCodes and city names of all cities which # have cityTax greater than 12% and a population of more than 100,000 cities[cities$cityTax &gt; 12 &amp; cities$population &gt; 100000, c(&quot;zipCode&quot;, &quot;cityName&quot;)] ## zipCode cityName avgSalKn population cityTax ## 1 10000 Zagreb 6359 790017 18 ## 2 51000 Rijeka 5418 128384 15 ## 3 21000 Split 5170 167121 10 ## 4 31000 Osijek 4892 84104 13 ## 5 2000 Dubrovnik 5348 28434 10 ## ----------- ## cityName cityTax ## 1 Zagreb 18 ## 2 Rijeka 15 ## 3 Split 10 ## ----------- ## [1] 18 15 10 13 10 ## ----------- ## zipCode cityName ## 1 10000 Zagreb ## 2 51000 Rijeka Notice the similarity between the last expression and SQL query: SELECT zipCode, cityName FROM cities WHERE cities.cityTax &gt; 12 AND cities.population &gt; 100000 Choosing columns and rows is not difficult if we are well acquainted with index vector knowledge, but as it can be seen in the last example of syntax, it is often not too readable (as compared to, for example, the SQL syntax that performs the same job). For this reason, there are different R extensions that greatly facilitate this work, which we will elaborate in detail in one of the future lessons. 4.3 Adding and deleting rows and columns To add and delete rows and columns, remember that the data frame is a kind of hybrid of a matrix and a list, ie if we know how to add rows and columns to a matrix or add new elements in a list, then we can add data to the data frame in an equivalent way. When working with data frames, adding new columns is a bit more common than adding rows. As we have said, we add columns to the data frame in the same way as we add list elements - while paying attention that the added column has the same number of elements as the other columns. New columns are often derived from existing columns to create new binary indicators, the results of arithmetic data expressions using other columns, etc. Assignment 4.5 - adding new columns to a data frame # add the logical column` highCityTax` to the `cities` table # which will indicate whether the cityTax is greater than 12% # assume the following (imaginary) way of calculating city tax income # - cities have about 60% of working population # - each worker pays a tax that is roughly equal to 10% of net salary # - income from city tax per worker is (cityTaxa percentage)*(tax income) # # add a `monthlyTaxIncome` column which will use the average salary, city tax # rate and population to estimate hte monthly tax income of the cities # (in million Kns) # i broja stanovnika procijeniti koliki prihod pojedino cities dobija od cityTaxa # round up to two decimals ( using the `round` function, # .e.g. : round(100.12345, 2) ==&gt; 100.12 ) # print `cities` # add the logical column` highCityTax` to the `cities` table # which will indicate whether the cityTax is greater than 12% cities$highCityTax &lt;- cities$cityTax &gt; 12 # assume the following (imaginary) way of calculating city tax income # - cities have about 60% of working population # - each worker pays a tax that is roughly equal to 10% of net salary # - income from city tax per worker is (cityTaxa percentage)*(tax income) # # add a `monthlyTaxIncome` column which will use the average salary, city tax # rate and population to estimate hte monthly tax income of the cities # (in million Kns) # i broja stanovnika procijeniti koliki prihod pojedino cities dobija od cityTaxa # round up to two decimals ( using the `round` function, # .e.g. : round(100.12345, 2) ==&gt; 100.12 ) cities$monthlyTaxIncome&lt;- round(0.6 * cities$population * 0.01 * cities$avgSalKn * 0.01 * cities$cityTax / 1e6 , 2) # print `cities` cities ## zipCode cityName avgSalKn population cityTax highCityTax ## 1 10000 Zagreb 6359 790017 18 TRUE ## 2 51000 Rijeka 5418 128384 15 TRUE ## 3 21000 Split 5170 167121 10 FALSE ## 4 31000 Osijek 4892 84104 13 TRUE ## 5 2000 Dubrovnik 5348 28434 10 FALSE ## monthlyTaxIncome ## 1 5.43 ## 2 0.63 ## 3 0.52 ## 4 0.32 ## 5 0.09 You can also add rows and columns similar to adding rows and columns to a matrix - with the rbind andcbind functions. For rbind we usually add a new data frame with rows that have the appropriate order and type of elements, while we can add a completely new vector with cbind, keeping in mind that the number of elements corresponds to the number of rows of the original data frame. Let’s try these functions on small “artificial” data frames to better demonstrate their functionality. Assignment 4.6 - data frames and rbind/cbind functions df1 &lt;- data.frame(a = c(1,2,3), b = c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;), c = c(T, F, T)) df2 &lt;- data.frame(a = 1, b = &quot;A&quot;, c = 3) # make a data frame which has `df1` and `df2` as rows # name it `df12` # add a `firstNames` columns containing names Ivo, Ana, Pero and Stipe # use `cbind` # print `df12` df1 &lt;- data.frame(a = c(1,2,3), b = c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;), c = c(T, F, T)) df2 &lt;- data.frame(a = 1, b = &quot;A&quot;, c = 3) # make a data frame which has `df1` and `df2` as rows # name it `df12` df12 &lt;- rbind(df1, df2) # add a `firstNames` columns containing names Ivo, Ana, Pero and Stipe # use `cbind` df12 &lt;- cbind(df12, firstNames = c(&quot;Ivo&quot;, &quot;Ana&quot;, &quot;Pero&quot;, &quot;Stipe&quot;)) # print `df12` df12 ## a b c firstNames ## 1 1 A 1 Ivo ## 2 2 B 0 Ana ## 3 3 C 1 Pero ## 4 1 A 3 Stipe For deleting rows and columns we can also use the same methods for managing the matrices and lists. Specifically: deleting rows and columns can be done by using two-dimensional indexing of rows and columns that we want to “keep” deleting columns can be done by assigning the value NULL to the selected column Let’s try this in the example. *** Assignment 4.7 - deleting rows and columns # delete the first row and second column from `df12` # use the indexing method # delete the `firstNames` column by using the `NULL` method # print `df12` # delete the first row and second column from `df12` # use the indexing method df12 &lt;- df12[-1, -4] # delete the `firstNames` column by using the `NULL` method df12$firstNames &lt;- NULL # print `df12` df12 ## a b c ## 2 2 B 0 ## 3 3 C 1 ## 4 1 A 3 Data frames will continue to be dealt with in the chapter on data management, where we will learn how to work with frameworks with far more data than the examples we have used in this lesson, and how to work with additional packages that significantly facilitate frequently used data actions in data frameworks. Below we will deal with another new (and slightly controversial) data structure. 4.4 Factors Factors in R are actually the type of data that represents what is referred to in statistics as a nominal or categorical variable. Namely, the attribute of some observation often takes on a value from a set of previously known categories (eg gender, age category, education, city of birth, political party preference, etc.). Categories are often identified by a unique set of characters, and in the process of analyzing them we often spend various aggregations and groupings (for example, in a sports race we can calculate the average time depending on gender or age category) or we share a set of data depending on the category. Factors in R are often the subject of discussion since it is a construct that can facilitate data processing but also cause problems, especially if we are not aware that at some point we are working with a factor (this scenario is actually easily avoided, as we will explain at the end of this chapter). To begin with, we will show you what factors are, with the help of a simple example. Let’s imagine that the next character vector describes the blood pressure level in ten patients: bloodPressure &lt;- c (&quot;low&quot;, &quot;high&quot;, &quot;high&quot;, &quot;normal&quot;, &quot;normal&quot;, &quot;low&quot;, &quot;high&quot;, &quot;low&quot;, &quot;normal&quot;, &quot;normal&quot;) This is obviously a “categorical” variable since it can take one of three discrete values - low, normal and high. Thus, this vector is a typical candidate for “factorizing”, ie for converting to a factor class object. We create these vectors by using the factor function to which we forward the character vector of category names as a parameter. Assignment 4.8 - factorizing a character vector bloodPressure &lt;- c (&quot;low&quot;, &quot;high&quot;, &quot;high&quot;, &quot;normal&quot;, &quot;normal&quot;, &quot;low&quot;, &quot;high&quot;, &quot;low&quot;, &quot;normal&quot;, &quot;normal&quot;) # print the variable `bloodPressure` # print its class # create a variable `bloodPressure.f` by factorizing # the variable `bloodPressure` # print the variable `bloodPressure.f` # print its class bloodPressure &lt;- c (&quot;low&quot;, &quot;high&quot;, &quot;high&quot;, &quot;normal&quot;, &quot;normal&quot;, &quot;low&quot;, &quot;high&quot;, &quot;low&quot;, &quot;normal&quot;, &quot;normal&quot;) # print the variable `bloodPressure` bloodPressure # print its class class(bloodPressure) # create a variable `bloodPressure.f` by factorizing # the variable `bloodPressure` bloodPressure.f &lt;- factor(bloodPressure) cat(&quot;-----------\\n&quot;) # print the variable `bloodPressure.f` bloodPressure.f # print its class class(bloodPressure.f) ## [1] &quot;low&quot; &quot;high&quot; &quot;high&quot; &quot;normal&quot; &quot;normal&quot; &quot;low&quot; &quot;high&quot; ## [8] &quot;low&quot; &quot;normal&quot; &quot;normal&quot; ## [1] &quot;character&quot; ## ----------- ## [1] low high high normal normal low high low normal normal ## Levels: high low normal ## [1] &quot;factor&quot; We see that the factor print was given an additional attribute Levels. This means that its actually a categorical variable with listed exact categories it is allowed to take. If we try to add a new value to a factor that is not present in the current categories (e.g., “very high”) we get a warning, and instead of the category we have specified, a new item will have the value of NA. This is sometimes not the scenario we want. If we know in advance that the character vector we categorize does not contain all the possible categories that may generally appear, we have the option of adding the levels parameter in which we will explicitly specify a series of\" possible \"categories with the help of the vector vector. Assignment 4.9 - using the levels attribute # add an 11th element to `bloodPressure.f` called &quot;very low&quot; # print the variable `bloodPressure.f` # create a variable `bloodPressure.f2` by factorizing # the variable `bloodPressure` # add the `levels` attribute which will also contain # values &quot;very low&quot; and &quot;very high&quot; # add an 11th element to `bloodPressure.f2` called &quot;very low&quot; # print the variable `bloodPressure.f2` # add an 11th element to `bloodPressure.f` called &quot;very low&quot; bloodPressure.f[11] &lt;- &quot;very low&quot; ## Warning in `[&lt;-.factor`(`*tmp*`, 11, value = &quot;very low&quot;): invalid factor ## level, NA generated # print the variable `bloodPressure.f` bloodPressure.f cat(&quot;-----------\\n&quot;) # create a variable `bloodPressure.f2` by factorizing # the variable `bloodPressure` # add the `levels` attribute which will also contain # values &quot;very low&quot; and &quot;very high&quot; bloodPressure.f2 &lt;- factor(bloodPressure, levels = c(&quot;very low&quot;, &quot;low&quot;, &quot;normal&quot;, &quot;high&quot;, &quot;very high&quot;)) # add an 11th element to `bloodPressure.f2` called &quot;very low&quot; bloodPressure.f2[11] &lt;- &quot;very low&quot; # print the variable `bloodPressure.f2` bloodPressure.f2 ## [1] low high high normal normal low high low normal normal ## [11] &lt;NA&gt; ## Levels: high low normal ## ----------- ## [1] low high high normal normal low high ## [8] low normal normal very low ## Levels: very low low normal high very high What is the advantage of using factors? Why not leave the variables in the original, “character” form? The reason for the factoring of categorical columns, ie variables, is that certain statistical and visualization functions “know” how to correctly interpret and use factors and treat them differently from ordinary “character” columns. For this reason, it is a very good long-term strategy that, at the beginning of R’s learning, we are used to factorizing columns that are really categorical variables (but we also note that we must not factorize columns that are not categorical variables, which can happen if we are not careful). One of the frequently asked questions about the category variables is - how are individual categories represented? The answer to this question is provided by the table function to which we pass the selected factor. Assignment 4.10 - the table function # print how are the categories represented in `bloodPressure.f2` # print how are the categories represented in `bloodPressure.f2` table(bloodPressure.f2) ## bloodPressure.f2 ## very low low normal high very high ## 1 3 4 3 0 The table function does not necessarily require a factor and will work even with the character vector. But in that case, we would not get information about categories that were not represented at all. The categorical variable in our examples actually has the so-called ordinal nature. Ordinal category variables means that the categories have a natural order (“low” blood pressure is lower than “normal” which in turn is lower than “high”). If desired, this fact can be “embedded” in the initialization factor by simply adding the order parameter set to TRUE. The advantage of the ordinal factor is that it allows us to compare factor values with the help of comparative operators. Assignment 4.11 - ordinal factor # create a variable `bloodPressure.f3` by factorizing # the variable `bloodPressure` # add the `levels` attribute which will also contain # values &quot;very low&quot; and &quot;very high&quot; # also set the `order` paramater to `TRUE` # watch out for category ordering! # print the variable `bloodPressure.f3` # check if it is in fact the ordinal factor now # use the `is.ordered` function # check if the first patient has lower blood pressure # than the third # create a variable `bloodPressure.f3` by factorizing # the variable `bloodPressure` # add the `levels` attribute which will also contain # values &quot;very low&quot; and &quot;very high&quot; # also set the `order` paramater to `TRUE` # watch out for category ordering! bloodPressure.f3 &lt;- factor(bloodPressure, levels = c(&quot;very low&quot;, &quot;low&quot;, &quot;normal&quot;, &quot;high&quot;, &quot;very high&quot;), order = TRUE) # print the variable `bloodPressure.f3` bloodPressure.f3 # check if it is in fact the ordinal factor now # use the `is.ordered` function is.ordered(bloodPressure.f3) # check if the first patient has lower blood pressure # than the third bloodPressure.f3[1] &lt; bloodPressure.f3[3] ## [1] low high high normal normal low high low normal normal ## Levels: very low &lt; low &lt; normal &lt; high &lt; very high ## [1] TRUE ## [1] TRUE We have already seen that in R “everything is a vector” - numbers are one-dimensional numeric vectors, matrices are vectors with added dimension parameter, lists are vectors of small lists, data frames are lists with added restrictions. We may wonder - what are factors? The implementation of a factor is actually a coded or enumerated set of values of initially defined character sequences, along with their associated code table. Simplifiedly, vector factorization includes: a “listing” of all detected categories (or ones set with the levels parameter) assigning numeric values in the order of each category (eg: “low” -&gt; 1, “normal” -&gt; 2 etc.) “packing” together a newly created numeric vector and the associated “code table” Although these steps R works out automatically, the internal factor structure can be determined by converting the factor into a pure numeric or pure character type. Assignment 4.12 - internal structure of a factor # print `bloodPressure.f3` converted to character # print `bloodPressure.f3` converted to numeric # print `bloodPressure.f3` converted to character as.character(bloodPressure.f3) # print `bloodPressure.f3` converted to numeric as.numeric(bloodPressure.f3) ## [1] &quot;low&quot; &quot;high&quot; &quot;high&quot; &quot;normal&quot; &quot;normal&quot; &quot;low&quot; &quot;high&quot; ## [8] &quot;low&quot; &quot;normal&quot; &quot;normal&quot; ## [1] 2 4 4 3 3 2 4 2 3 3 By converting the factor to the character type we actually do the operation of inverse factorization, ie, we get the original character vector. On the other hand, by converting the factor into a numerical type, we actually get “pre-coded” numbers that the internal factor is used to represent each category. We may wonder - why would this be useful to us? The answer is - no, this is in practice usually has no direct benefit. Butthis is something which can help us understand a potential big problems which can catch us unaware. The above-mentioned “controversy” around the factor as a data type is actually contained in the numerical conversion of factors. It is not actually related to the factors themselves, but rather to some of the default R settings of R when it comes to loading data. Let’s look at the detailed steps that happen during the creation of the data frame from a CSV file and try to figure out where a potential problem arises: R opens a CSV file Based on the read data R tries to determine column types all character columns are automatically factorized unless the stringsAsFactors = FALSE parameter is specified The final data frame is formed Do you see a possible problem? Suppose the following scenario: in a single numeric column, a non-numeric value has somehow ocurred (e.g. a NULL character string due to the missing value in the database). This can be just one value in more than one million rows, but that’s enough for R to classify the column as a “character column”. Since R automatically factorizes the character columns, the numeric column in the final data frame becomes categorical (although the category names are actually numbers). A clumsy analyst (or an automatic script) does not notice that it is a factor, and performs the conversion of said column into a numeric type - and obtains a completely semantically meaningless set of “transcoded” integers that, if not noticed, can be used as input data for further analysis. How to avoid this scenario? The procedure is actually very simple: When using the read.csv orread.table functions, always use the string stringsAsFactors = FALSE carefully check column type data of loaded data frame perform appropriate column conversions check the results again If we adhere to these steps we will almost never come to the situation which causes us real problems. There are alternate procedures (for example, explicitly telling column types read from a file with the help of colClasses parameters, or using the as.numeric (as.character ()) syntax for converting columns to a numeric type that will perform the correct conversion independently of whether it is a character column or a “concealed” factor), but the above steps should in most cases be completely sufficient. The most important thing to remember is the key role of the parameter setting stringsAsFactors = FALSE (which is sometimes described as in literature as “mandatory”, often without any further explanation). Exercises In the folder along with this notebook, locate the file citiesNOHEADER.csv which represents the file that is the same as cities.csv except for the following features: column names are missing the gap is used as a separator Try using the documentation to load the data from this file into the variable ’citiesNH` which will be the same variable as the cities used in the lesson. In the folder along with this notebook, locate the file receipt.csv and load it into the receipt variable. Make sure that character sequences are not automatically converted into factors. Print to the screen: the number of rows in this table the number of columns in the table column names For the receipt table, do the following: factorize the itemCategory column print the code, name and price of all items of the category “sweets and snacks” cheaper than 12 Kn print how many products each category has in the receipt add a total column that will contain the total price of eacg bought item set using the price and quantity calculate the total amount of the account In the folder along with this note find the file citiesNULL.csv which, if loaded without thestringsAsFactors = FALSE parameter, can result in a problematic scenario described in the lesson. Try to do the following: load the data from this file into the variable citiesNULL where you deliberately omit the stringstringsAsFactors = F add avgSal1 column that will represent the result of using the as.numeric function over the avgSalKn column add avgSal2 column that will represent the result of using the as.character / as.numeric functions over the avgSalKn column (be careful of the order) print out citiesNULL and comment the results Programirajmo u R-u by Damir Pintar is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License.Based on a work at https://ratnip.github.io/FER_OPJR/ "],
["control.html", "5 Flow control and objects 5.1 Flow control commands 5.2 Program Loops 5.3 Object models in R language Exercises", " 5 Flow control and objects 5.1 Flow control commands Under flow control commands we mostly refer to constructs for conditional execution and so-called “program loops” where the segment of the program is continually executed until (optionally) certain conditions are fulfulled. 5.1.1 Conditional execution of commands For conditional execution we use the so-called if-then block: if (condition) {block} else {block} This command is pretty straightforward. In the next exercise we will show how to avoid a relatively common error when writing if commands. Try to spot and correct it. Assignment 5.1 - if command # execute the next conditional execution command if (2 &gt; 1) print (&quot;Success!&quot;) # find the error in the next `if-else` command and correct it if (1 &gt; 2) print (&quot;Success!&quot;) else print (&quot;Fail!&quot;) # execute the next conditional execution command if (2 &gt; 1) print (&quot;Success!&quot;) # find the error in the next `if-else` command and correct it if (1 &gt; 2) {print (&quot;Success!&quot;) } else print (&quot;Failed!&quot;) ## [1] &quot;Success!&quot; ## [1] &quot;Failed!&quot; The error occurs because R is an interpreting language that normally executes a row per line, unless we forward an “incomplete” command to R, whereby it will wait for the “rest” before it starts executing. In the previous example, the second ifcommand is actually completed in the first line, so R is “surprised” when the next line starts with else. In order to prevent this scenario, it is sufficient to explain in some way to R that the command has not yet been completed, which is most easily accomplished by opening the block in the first line and closing it in the line with else. The readers who are programming in C or Java will recognize the so-called “ternary operator” that actually represents a compact version of the if-else block: x = (a &lt;b) ? c: d # not R language! The role of this operator in R is performed by the ifelse function. Assignment 5.2 - ifelse function a &lt;- 1:3 b &lt;- c(0, 2, 4) # what does the vector `x` look like after executing the following command? # Think about the answer and then check the correctness of the solution x &lt;- ifelse (a &lt; b, 2, 5) # what does the vector `x` look like after executing the following command? # Think about the answer and then check the correctness of the solution x &lt;- ifelse (a &lt;b, 2, 5) x ## [1] 2 2 2 2 Note that the ifelse function is vectorized, which is why it is particularly suitable for creating new columns of data frames derived from certain conditions related to the existing columns. 5.2 Program Loops In the programming language R we have three types of loops: repeat - infinite loop while - loop with checking conditions at the beginning for - iterator loop (“loop with known number of repetitions”) 5.2.1 Loop repeat repeat is the simplest loop. It has the following syntax: repeat {block} It is an “infinite” loop where, once the block is completed, it is re-executed again so on. The only way to exit this loop is to use the break command. In addition to this we have a next command that will skip the rest of the block, but it will not exit loop. Let’s see how this loop works in the next exercise. Assignment 5.3 - repeat loop # answer the following questions before running the next block: # - will the loop run indefinitely? # - what will be printed on the screen? i &lt;- 1 repeat { i &lt;- i + 1 if (i %% 2 == 0) next print (i) if (i&gt; 10) break } ## [1] 3 ## [1] 5 ## [1] 7 ## [1] 9 ## [1] 11 We often know the exact condition for exiting the loop and want to place it in a clearly visible place so it is not “hidden” in the body of the loop. Hence, we may find the while loop to be more useful. 5.2.2 while loop while represents the “purest” form of the program loop whose syntax literally means “while the condition is met, repeat the specified code”: while (condition) {block} Assignment 5.4 - while loop # add the loop condition so it loops # exactly 7 times i &lt;- 1 while () { print(i) i &lt;- i + 1 } # add the loop requirement so that it does # exactly 7 times i &lt;- 1 while (i &lt;= 7) { print(i) i &lt;- i + 1 } ## [1] 1 ## [1] 2 ## [1] 3 ## [1] 4 ## [1] 5 ## [1] 6 ## [1] 7 With this loop we have to make sure that in a certain iteration conditions for exiting will be met, otherwise it also becomes an “infinite” loop. We are also free to use the next andbreak commands, which have the same function as in the repeat loop. 5.2.3 for loop A for loop or “iterator loop” serves to easily “walk” over a data structure (most commonly vector), getting element by element and doing something with it. It uses the for keyword, the name of the new iterator variable, the keywordin, and the vector whose values are taken one by one and used within the loop (note that the specified in is not the same as the %in% operator which checks if some element is contained in the vector!). The syntax of this loop is as follows: for (i in v) {do something with i} Note that here the variable i is not a “counter” - in every loop iteration it becomes the value of the element we come across. If we want to iterate by the indexes, not by the elements themselves, then we can use the construct for (i in 1:length (a)). Assignment 5.5 - for loop a &lt;- seq(-10, 10, 4) # print vector elements of `a` one by one # with the help of `for` loop # access the elements directly # do the same, but iterate by indexes a &lt;- seq(-10, 10, 4) # print vector elements of `a` one by one # with the help of `for` loop # access the elements directly for (i in a) print(i) # do the same, but iterate by indexes for (i in 1:length (a)) print(a[i]) ## [1] -10 ## [1] -6 ## [1] -2 ## [1] 2 ## [1] 6 ## [1] 10 ## [1] -10 ## [1] -6 ## [1] -2 ## [1] 2 ## [1] 6 ## [1] 10 Notice that the second way is better if you want to change the vector elements or need information on where the element is located in the original vector. Now that now that we have learned the loop syntax, it is important to emphasize one fact - in programming language R, it is generally not recommended to use program loops. Although this may initially seem unexpected and somewhat shocking, the reason is simple - R language is designed to work by the declarative “all at once” principle. We have already seen that the principle of vectorization and recycling effectively perform jobs that would require a loop in other programming languages, and in the chapters that follow, we will see that R also offers many other constructs that avoid explicit code repetition with the requirement of a declarative syntax that automatically performs it. For example, the following example is syntactically correct: # example of unnecessary loop usage a &lt;- 1: 5 b &lt;- 6:10 c &lt;- numeric() for (i in 1:length (a)) c[i] &lt;- a[i] + b[i] but probably works slower and is much less readable than: # R syntax a &lt;- 1:5 b &lt;- 6:10 c &lt;- a + b All of this does not mean that we should not use loops in R, but that their use should be accompanied by an additional consideration of whether the loop is really needed and whether there is an alternative syntax that completes the same task but can be written declaratively (and is potentially faster, since many routines in R are implemented in language C). Early adoption of the “R” mode of thinking will result in long-term benefits that will be reflected in a more compact, cleaner, and often more effective program code. 5.3 Object models in R language R is designed as an object-oriented language, together with the mechanisms that the object-oriented paradigm requires - encapsulation (enveloping various attributes into a common entity), polymorphism (using the same function over different objects results in different operations depending on the nature of the object) and inheritance (creating new objects from the existing ones by expanding them with additional elements). R has taken its initial way of object modeling from the S language and therefore such objects are known as “S3 objects” (according to the S language version from which they were originally taken). This object model, which we will learn very soon, is actually very unconventional and simple but also as such fairly suitable for using in R, it being primarily a domain-oriented language. With the increasing number of programmersin the R community, the pressure to bring support to objects that will be more similar to the way they work in the other programming languages has increased, in order to bring better robustness in design and management of objects. All this has led to the fact that today we have formally four types of objects in the programming language R: “base”\" classes - basic R elements (functions, vectors, data frames) S3 objects - principle of object design taken from S language (version 3) S4 objects - a more formal and rigorous way of creating objects approaching standard object-oriented mechanisms from other languages RC objects (reference classes) - the most recent way of creating objects (introduced in version 2.12) that replicates the “classical” object-oriented principles based on the exchange of messages The existence of three different models of defining objects (we can ignore the basic since we can not formally expand it with new objects) can seem disheartening - is it necessary to learn all three models? How to distinguish them? Which one to choose? However, despite the fact that the story about the object nature of R during its development is (unnecessarily) complicated, the good news is that for most needs, it’s quite enough to learn how the S3 model works, which is also the simplest. A large number of popular R packages use only S3 classes and it is possible to work in language R for a very long time without meeting the need for learning S4 or RC models. For this reason, we will focus on the S3 objects only (readers who want more information on other models can look at a very good book called Advanced R, by the author Hadley Wickham, which deals, among other things, with the object models in R). 5.3.1 Overview of the S3 object model As already mentioned, S3 objects are actually taken from S programming language and represent a relatively primitive implementation of the concept of “object”, as far as expectations go regarding standard methods of object creation. S3 object is actually just a list with the additional class attribute. # we are creating a new object of a class `Person` pero &lt;- list(oib = &quot;12345678&quot;, lastname = &quot;Peric&quot;, weight = 78) class(pero) &lt;- &quot;Person&quot; And that’s it! Notice that we do not have a formally defined “template” of the class that we instance in the object as is the established practice in other programming languages. For S3 objects, we simply create a list and then declare that that list is an object of a certain class, although the structure of that class is only implied by the object’s appearance (and does not have to match the structure of another object declared to belong to the same class). Of course, such a convenient way of constructing objects is not advisable so we recommend that we in practice do not construct the class “manually”, but rather to write and use a special constructor function whose parameters will actually define the object’s appearance (this will be learned in the lesson about creating user-defined functions) What about inheritance, where class-child inherits properties from its parent? R enables inheritance, but also in a very informal and relatively trivial way. Rather than just listing a single “class” name with the class attribute, we create a character vector where the first element will be the class name, and the other elements will be parent classes, sorted by ‘importance’. For example, if we created a new mate object of class Employee which inherits the class Person, then it is enough to do the following: mate &lt;- list(oib = &quot;12345678&quot;, lastname = &quot;Peric&quot;, weight = 78, yearEmpl = 2001) class(mate) &lt;- c (&quot;Employee&quot;, &quot;Person&quot;) We note that all the attribute inheritance work must be done “manually”, that is, we have to make sure that the parent has the attributes of the Person class. 5.3.2 Generic functions Looking at the above way of the object design, it is legitimate to ask an additional question - where are the methods? As we know, standard object-oriented principles assume the encapsulation of the attribute but also the methods in the form of an object. Here is the basic difference between the S3 object and the “standard” objects from the other programming languages - in S3 object model the method is defined outside of the object, in the form of so-called generic functions. Why is that so? The idea is the following: when working with objects, the user (programmer, analyst) often calls the same functions (eg “print”, “draw”, “summary description”) over objects of different types. The function of the same name will work differently depending on the object over which it is called - hence the name generic function. So, the print function always results in some kind of print, but how the printing actually works depends on the object we are printing. This way of designing an object can be extremely unconventional, but the fact is that the call of a generic function makes it look far more intuitive, especially for users who do not have a big experience in programming. Specifically, let’s compare the command: start(car, speed = 20) with the command: car.start(speed = 20) By reading the first command, the car is perceived as an “object” (in terms of word service in a sentence), that is, something we do “to” this object. The second command sets the car as a subject, which is common practice in object-oriented languages, but is not in line with the general understanding of doing things to some objects. In working with programming language R we often do “similar” tasks over different objects - we print their contents, draw them on a graph, look for some concise details of them, etc. For this reason, and the fact that R is often interactive, R is designed in a way that we think what we want to do instead of asking ourselves where the function we want to call is. If we want to print an object, it is logical to just forward it to the print function ,if we want to draw it the plot function, ans if we want to summarize it - the summary function. How does a single function “know” what to do with an object? The answer is simple - the generic function is just “interface” to the “right” function we call, and the logic to find the right function is trivial - if the generic name of the function is genFun, and the name of the object class is className, the function actually being called is genFun.className. If no such function exists, the function genFun.default is called. This can easily be seen in the next exercise. Assignment 5.6 - generic functions # print the `summary` function (only the function name!) # print a function that is actually being called when you call # the `summary` function of the `factor` class object # print the `summary` function (only the function name!) summary # print a function that is actually being called when you call # the `summary` function of the `factor` class object summary.factor ## standardGeneric for &quot;summary&quot; defined from package &quot;base&quot; ## ## function (object, ...) ## standardGeneric(&quot;summary&quot;) ## &lt;environment: 0x0000000011da1008&gt; ## Methods may be defined for arguments: object ## Use showMethods(&quot;summary&quot;) for currently available ones. ## function (object, maxsum = 100L, ...) ## { ## nas &lt;- is.na(object) ## ll &lt;- levels(object) ## if (ana &lt;- any(nas)) ## maxsum &lt;- maxsum - 1L ## tbl &lt;- table(object) ## tt &lt;- c(tbl) ## names(tt) &lt;- dimnames(tbl)[[1L]] ## if (length(ll) &gt; maxsum) { ## drop &lt;- maxsum:length(ll) ## o &lt;- sort.list(tt, decreasing = TRUE) ## tt &lt;- c(tt[o[-drop]], `(Other)` = sum(tt[o[drop]])) ## } ## if (ana) ## c(tt, `NA&#39;s` = sum(nas)) ## else tt ## } ## &lt;bytecode: 0x000000001bd2cff0&gt; ## &lt;environment: namespace:base&gt; By understanding the principle of generic functions, we have completed a picture of S3 objects. The most important thing we have to adopt is that in this model functions are not part of the object itself, they are defined separately, and the link between the object and its “method” is only in the function name by which R “links” the generic function and that object. Although this principle is primitive and vulnerable to faults in the hands of inattentive developers, it is easy to use and very effective. Finally, let’s note that this approach is not entirely unique to the language R - similar principles can be found in other programming languages. It is specific to R in that the principle is used openly and almost exclusively. Objects and generic functions will be re-visited when we learn to create our own functions, which will enable us to create both the constructors of our objects and their generic functions. 5.3.3 Conclusion on S3 objects In short, the conclusions about S3 objects can be as follows: S3 objects function in in a simple, informal way - they are simply lists with the arbitrary value of class attributes Much of this is left to the responsibility of the programmer the S3 object methods are not encapsulated within the objects, but are designed “out of” objects in the form of generic functions S3 objects are not suitable for complex object models due to heavy model maintenance and large potential of errors Exercises Create a data frame with the following command: cities &lt;- data.frame(zipcode = c(10000, 51000, 21000, 31000, 2000), cityName = c(&quot;Zagreb&quot;, &quot;Rijeka&quot;, &quot;Split&quot;, &quot;Osijek&quot;, &quot;Dubrovnik&quot;), cityTax= c(18, 15, 10, 13, 10)) Add thie “taxLevel” which will be the ordinal factor variable with the levels “small”, “medium” and “high” depending on whether the percentage of tax is strictly smaller than 12, between 12 and 15 or strictly greater than 15. Use the ifelse command. Replace the loops in the next block with equivalent vectorized operations (for the second loop, review the sum function documentation). a &lt;- numeric() i &lt;- 1 while (i &lt;= 100) { a &lt;- c(a, i) i &lt;- i + 1 } total &lt;- 0 for (i in a) { if (i %% 2 == 0) total &lt;- total + i * i } print (total) Create a class object Block with the attributes height, width and depth equal to10, 20 and 30 respectively. Program in Ru &lt;/ span&gt; by Damir Pintar is licensed under Creative Commons Attribution-NonCommercial-NoDerivative 4.0 International License Based on a work at https://ratnip.github.io/FER_OPJR/ "],
["packages.html", "6 Packages, built-in functions and environments 6.1 Working with packages 6.2 Built-in functions 6.3 Environments Exercises", " 6 Packages, built-in functions and environments 6.1 Working with packages The standard R distribution comes with two collections of packages (called r-base and r-recommended) that contain a kind of “core” of the language R-set of elements sufficient for conducting standard types of data analysis using the programming language R. In addition, CRAN (Comprehensive R Archive Network) is a rich repository of additional packages for a wide variety of applications, from “repairing” the basic elements of the language R to strictly specialized packages for specific types of analyses. As is common practice in other programming languages, R uses a “package” or “library” system to logically organize already “programmed” collections of data, functions, and compiled code. When starting the R environment, certain packages are automatically loaded in memory, making their content available for use. The list of loaded packages can be obtained using the search function. Assignment 6.1 - search path # Call the `search` function (no parameters) # and see which packages are loaded into the environment # Call the `search` function (no parameters) # and see which packages are loaded into the environment search() ## [1] &quot;.GlobalEnv&quot; &quot;package:corrplot&quot; &quot;package:broom&quot; ## [4] &quot;package:caret&quot; &quot;package:e1071&quot; &quot;package:car&quot; ## [7] &quot;package:carData&quot; &quot;package:Hmisc&quot; &quot;package:Formula&quot; ## [10] &quot;package:survival&quot; &quot;package:lattice&quot; &quot;package:sn&quot; ## [13] &quot;package:stats4&quot; &quot;package:gridExtra&quot; &quot;package:RSQLite&quot; ## [16] &quot;package:hflights&quot; &quot;package:lubridate&quot; &quot;package:GGally&quot; ## [19] &quot;package:forcats&quot; &quot;package:stringr&quot; &quot;package:dplyr&quot; ## [22] &quot;package:purrr&quot; &quot;package:readr&quot; &quot;package:tidyr&quot; ## [25] &quot;package:tibble&quot; &quot;package:ggplot2&quot; &quot;package:tidyverse&quot; ## [28] &quot;package:MASS&quot; &quot;package:stats&quot; &quot;package:graphics&quot; ## [31] &quot;package:grDevices&quot; &quot;package:utils&quot; &quot;package:datasets&quot; ## [34] &quot;package:methods&quot; &quot;Autoloads&quot; &quot;package:base&quot; We see that most packages are listed as package::package_name. The layout of the package also represents their “priority” in the namespace traversal path, which will be discussed later. If we want to load a new package into our environment, we can do this by using the library function with the package name provided as parameter (without quotes). Assignment 6.2 - loading packages in the working environment # load the `dplyr` package # load the `dplyr` package library(dplyr) The command from the previous example can have two outcomes: if the package exists on a local computer (in a folder set for additional packages), it will be loaded into the working environment. Package loading can be accompanied by messages about objects that are “masked” after loading. This in particular means that the new package has temporarily denied access to certain elements from previously loaded packages because their names match. This often does not pose any problems, but if the user has the need to access the masked elements they will have to use their “full name” - e.g. they will also have to specify the name of the package where they are located. For example, if the filter function of the stats package is masked after loading the new package, it is still available through the full name stats:: filter, but not directly through the namefilter, as this will call the function from the latest loaded package. More details about how R resolves the names of variables and functions will be given below in this lesson. If we do not have the above package on the local computer, we receive an error message that this package does not exist. In this case, it is necessary to retrieve the package from the CRAN repository using the install.packages function, which gives the name of one or more packages (with quotation marks!) as parameters. This function assumes that the R environment has a defined CRAN mirror ie. the specific address of the CRAN repository from where the package will be downloaded. A large number of countries have their own “copy” of the CRAN repository, but unfortunately, for unclear reasons, the Republic of Croatia no longer has its own CRAN repository, and at the time of writing this notebook there is no indication that the same will be established. If we are working in the RStudio interface, we have very likely placed the CRAN repository at the first startup (ie we have selected the Global option which our package installation requests automatically forward to the nearest CRAN repository), but if we did not do or work in another development then we need to look for a method of setting up a CRAN repository with the help of documentation, if we want to load additional packages. Assignment 6.3 - installing a package from a CRAN repository # install the `dplyr` package from the CRAN repository # (You can do this even if you already have the specified package) # load the package again into the work environment # print the search path # install the `dplyr` package from the CRAN repository # (You can do this even if you already have the specified package) install.packages(&quot;dplyr&quot;) # load the package again into the work environment library(dplyr) # print the search path search() ## [1] &quot;.GlobalEnv&quot; &quot;package:corrplot&quot; &quot;package:broom&quot; ## [4] &quot;package:caret&quot; &quot;package:e1071&quot; &quot;package:car&quot; ## [7] &quot;package:carData&quot; &quot;package:Hmisc&quot; &quot;package:Formula&quot; ## [10] &quot;package:survival&quot; &quot;package:lattice&quot; &quot;package:sn&quot; ## [13] &quot;package:stats4&quot; &quot;package:gridExtra&quot; &quot;package:RSQLite&quot; ## [16] &quot;package:hflights&quot; &quot;package:lubridate&quot; &quot;package:GGally&quot; ## [19] &quot;package:forcats&quot; &quot;package:stringr&quot; &quot;package:dplyr&quot; ## [22] &quot;package:purrr&quot; &quot;package:readr&quot; &quot;package:tidyr&quot; ## [25] &quot;package:tibble&quot; &quot;package:ggplot2&quot; &quot;package:tidyverse&quot; ## [28] &quot;package:MASS&quot; &quot;package:stats&quot; &quot;package:graphics&quot; ## [31] &quot;package:grDevices&quot; &quot;package:utils&quot; &quot;package:datasets&quot; ## [34] &quot;package:methods&quot; &quot;Autoloads&quot; &quot;package:base&quot; Note: as a rule, we install packages only once, through the console so that there is never a need to install the package installations in R Markdown documents; also, for easier organization of reports, loading all required packages is usually done by at the beginning of the document, in the code snippet called setup. We can notice that installing and loading packages will automatically acquire and load all the packages that are prerequisites for using the requested package, which greatly facilitates the user’s work, whu does not have to worry about what “extra” needs to be installed in order to use package features. If we want to find out more information about a package, we can also do this by using the library function with the help parameter set to the package name. library(help = dplyr) # recommendation: try directly in the console Another quite popular way of perusing documentation of the package is with the help of the so-called “vignettes”. Vignettes are actually “mini-tutorials” of a package done in HTML which present the functionality of the package in an accessible, easlly legible way with the help of detailed explanations and the associated program code. We can look at which vignettes are installed on the system by calling the browseVignettes() function without parameters (or optionally adding as the parameter the package name if we only care about its vignettes). If the package has only one vignette (for example, stringr), we can also open the vignette immediately with the help of the vignette option. vignette(&quot;stringr&quot;) # recommendation: try directly in the console 6.2 Built-in functions In previous chapters, we have already introduced some of the functions that we get together with our R distribution. These are, for example, numeric functions (log,abs, sqrt,round, etc.), vector creation functions (rep,seq, etc.), package functions (install.packages, library, etc.) and so on. R rarely uses the term “built-in” functions since - as it was already shown - the R environment automatically loads some commonly used packages whose elements are immediately available for use, without necessarily indicating the name of the package they are in. For example. the stats package contains a rich set of functions related to statistical processing. One of these functions is rnorm, which returns the numerical vector of the desired length whose elements are randomly selected from the normal distribution with arithmetic mean 0 and standard deviation 1 (these values can also be changed using themean and sd parameters). If we want, we can invoke this function using the package_name::function_name(parameters) syntax. Assignment 6.4 - calling a package function # create a vector `x` that will have 10 random elements # drawn from standard normal distribution # use the full name of the `rnorm` function of the` stats` package # round the elements of vector `x` to two decimal places # use the full name of the `round` function from the `base` package # print vector `x` # create a vector `x` that will have 10 random elements # drawn from standard normal distribution # use the full name of the `rnorm` function of the` stats` package x &lt;- stats::rnorm(10) # round the elements of vector `x` to two decimal places # use the full name of the `round` function from the `base` package x &lt;- base::round(x, 2) # print vector `x` x ## [1] 0.04 0.11 1.43 0.98 -0.62 -0.73 -0.52 -1.75 0.88 1.37 Although this is a syntactically correct way of calling a function, R allows us to exclude package names and simply name the function directly. Assignment 6.5 - removing the package reference # create vector `y` by the same principle as vector x # use only one line of code # use the function names without the name of the package # print vector `y` # create vector `y` by the same principle as vector x # use only one line of code # use the function names without the name of the package y &lt;- round(rnorm(10), 2) # print vector `y` y ## [1] -1.69 -0.63 0.02 0.71 -0.65 0.87 0.38 0.31 0.01 -0.04 We can ask ourselves - how does R know where the function we want to call is, if we did not explicitly load or specify a package containing that function? We will explain the exact reason in the next chapter dealing with the environmenst and already seen concept of the “search path”. Listingg all available functions, even those that are more commonly used, would be redundant since R language is best learned with specific application in mind, and after a certain time of use the user slowly creates his/her “own” collection of functions that represent his usual “analysis toolset”. In any case, it is recommended that you select one of the publicly available R reminders (reference cards or cheat sheets) which will then always be at hand during R programming. Some R reference cards are also available on the CRAN itself (it is enough to enter the CRAN reference card in the search engine and view the results), but here we will take the opportunity and recommend excellent cheat sheets on the pages of the RStudio interface available on the following link: https://www.rstudio.com/resources/cheatsheets/ (Alternatively, type RStudio cheat sheets into the search engine). These reminders cover the majority of useful things related to the general elements of the language R (Base R, Advanced R), but also specific packages that we will later learn (dplyr, ggplot2) and represent very valuable resources both for learning R and for long-term use. To easily follow the future lessons, we recommend that you print out the mentioned reminders and get acquainted with the elements they contain. Finally, let’s say that R allows us to quickly get help on the function by simply calling ? function_name or help(function_name), and that we can get examples of using the function through example(function_name). We should use these calls very often even if we think that we are well acquainted with the function we are calling - it is possible that there is an additional parameter (or a related function that is also often listed in the documentation), which will further help us in carrying out the task relted to the function we want to use. 6.3 Environments As already mentioned, work in R is usually related to managing different objects. In order to manage these objects at all, we need mechanisms to help us refer to the objects concerned. In R (and other programming languages), this is called “binding”. When we create the x variable of the numeric type and assign it the number 5, we actually created a (one-element) numerical vector and created a reference to that data using a character string x, which we can then use to acquire that data. Therefore, when we want to access some variables, R must search its internal “records” which state whichvariables currently exist, of what types they are and how to access them. In order to find the variable, R uses a mechanism called “lexical scoping”\" based on the concept of “environments”. The “environment” is often referred to as a “bag of names”. It helps us to logically group the names of the objects we use and to help R find the name in other environments if the variable does not exist in the current environment. The latter is enabled with the help of a fact that (almost) each environment has a parent environment. This system of parent-environment links creates a kind of “environment hierarchy”, often referred to as “search path”; R, looking for the default variable name, searches the environments “upwards” until it finds the first appearance of the requested name or encounters an the final environment (the so-called “empty environment” ). What is interesting is the fact that the environment itself is an object - we can create a reference to it, send it to functions, and so on. The “default” environment in which we work and in which we create new variables is the so-called “global environment”, or .GlobalEnv (watch out for the dot!). It is at the bottom of the environment hierarchy. We can get a reference to it via the mentioned name of the variable, or by using the globalenv() function. Assignment 6.6 - global environment # create a variable `e` which refers to the global environment # print `e` # create a variable `x` and assign the number `5` to it # execute the `ls` function, without parameters # execute the `ls` function with `e` as a parameter # print `x` # print `e$x` (notice the list syntax!) # create a variable `e` which refers to the global environment e &lt;- .GlobalEnv # or e &lt;- globalenv() # print `e` e # create a variable `x` and assign the number `5` to it x &lt;- 5 # execute the `ls` function, without parameters #ls() # try directly on console! # execute the `ls` function with `e` as a parameter #ls(e) # try directly on console! # print `x` x # print `e$x` (notice the list syntax!) e$x ## &lt;environment: R_GlobalEnv&gt; ## [1] 5 ## [1] 5 From the last example, we can see that the environment also references itself, so this is completely correct (although unnecessarily complicated) syntax for printing the x variable: e$e$e$e$e$e$e$e$e$e$e$e$e$e$e$e$e$e$e$e$x The environments are somewhat similar to lists, which are in fact a kind of “encapsulation” of a number of objects in a unique structure. The most important differences between the environment and the list are: the order of elements in the environment is irrelevant the environment (as a rule) has a link to the environment of the parent Let’s look at who the parent environment is to the global environment with the help of parent.env function. Assignment 6.7 - parent environments # print out the parent environment of the global environment and explain the result # print out the parent environment of the global environment and explain the result parent.env(e) ## &lt;environment: package:corrplot&gt; ## attr(,&quot;name&quot;) ## [1] &quot;package:corrplot&quot; ## attr(,&quot;path&quot;) ## [1] &quot;C:/R/R-3.5.1/library/corrplot&quot; Slightly unexpectedly, the parent of the global environment is the last loaded package! This is not really unusual - the global environment has a “priority” in referencing the variable, but immediately below it are the objects and functions that we last loaded into the work environment (which suits us as the assumption holds that the “most recent” package is the one that we intend to use immediately). In other words, by loading a package, the new package is always “set” between the global environment and the package that was previously loaded last. When we called search function, we actually got the hierarchy of environments that represented loaded packages. This environmenthierarchy is the already mentioned “search path”. With the help of parent.env, we can determine what environment the environment will consider as a parent. In this way, we can make our own hierarchy of environments. Furthermore, the variables we create do not have to use references from the global environment (which is actually the basic function of the operator &lt;-), we can store them in any environment we want, but we need to use assign and get, or a combination of operators $ and &lt;-. Look at the example below to get the feeling how custom environments work. # example - a small hierarchy of custom environments e2 &lt;- new.env() e3 &lt;- new.env() # hierarchy `e3` --&gt; `e2` --&gt; `e` (global) parent.env(e2) &lt;- e parent.env(e3) &lt;- e2 # creating variable `x` in `e2` assign(&quot;x&quot;, 5, e2) # or e2$x &lt;- 5 # checking if there is an `x` in `e2` exists(&quot;x&quot;, e2) # printing `x` from `e2` get(&quot;x&quot;, e2) # or e2$x ## [1] TRUE ## [1] 5 Why use the environment in practice? The environment is a convenient way of “wrapping” a set of variables that we can then send together in a function - which is especially convenient if the relevant variables refer to some large datasets. As we will see in the next lesson, R does not support the so-called call-by-reference principle when sending objects to the function, but rather uses the so-called copy-on-modify mechanism. This means that the function will use the reference to the original object sent to it as a parameter up to the command that will try to change the object; at that moment a copy of that object is created and all the changes pertain to the copy. This can lead to a significant slowdown of programs with developers who are not familiar with this fact and who, for example, program a function that transforms the data frame. If the function instead of referring to the data frame sends a reference to the environment in which the “wrapped” frame is copied, then copying the variable will not occur because the environment is the only object for which copy-on-modify is not valid. We note that the special case of this method is to “send” the global environment to a function, which is actually reduced to the use of a “global variable” - something that is often avoided in other programming languages, but is not that uncommon in R. Finally, let’s demonstrate the attach function that analysts often use to speed up the analysis process, but which can cause problems if we are not careful with its use. This function will insert the data frame directly into the search path to allow us “easier” access to variables, and with potential accidental side effects. Let’s take a look at this example. Assignment 6.8 - attach function cities &lt;- data.frame( zipcode = c(10000, 51000, 21000, 31000, 2000), cityName = c(&quot;Zagreb&quot;, &quot;Rijeka&quot;, &quot;Split&quot;, &quot;Osijek&quot;, &quot;Dubrovnik&quot;), avgSalary = c(6359., 5418., 5170., 4892., 5348.), population = c(790017, 128384, 167121, 84104, 28434), tax = c(18, 15, 10, 13, 10)) # execute function `attach` with `cities` as parameter # do this only once! # print the search path and comment on the result # print the `zipcode` variable # change the third element from the `tax` variable to 12 # print `cities` # execute the `ls` function # use the `detach` function to remove `cities` from the search path # execute function `attach` with `cities` as parameter # do this only once! attach(cities) ## The following object is masked from package:tidyr: ## ## population # print the search path and comment on the result search() cat(&quot;-------------------------\\n&quot;) # print the `zipcode` variable zipcode cat(&quot;-------------------------\\n&quot;) # change the third element from the `tax` variable to 12 tax[3] &lt;- 12 # print `cities` cities cat(&quot;-------------------------\\n&quot;) # execute the `ls` function #ls() # try it on console! # use the `detach` function to remove `cities` from the search path detach(cities) ## [1] &quot;.GlobalEnv&quot; &quot;cities&quot; &quot;package:corrplot&quot; ## [4] &quot;package:broom&quot; &quot;package:caret&quot; &quot;package:e1071&quot; ## [7] &quot;package:car&quot; &quot;package:carData&quot; &quot;package:Hmisc&quot; ## [10] &quot;package:Formula&quot; &quot;package:survival&quot; &quot;package:lattice&quot; ## [13] &quot;package:sn&quot; &quot;package:stats4&quot; &quot;package:gridExtra&quot; ## [16] &quot;package:RSQLite&quot; &quot;package:hflights&quot; &quot;package:lubridate&quot; ## [19] &quot;package:GGally&quot; &quot;package:forcats&quot; &quot;package:stringr&quot; ## [22] &quot;package:dplyr&quot; &quot;package:purrr&quot; &quot;package:readr&quot; ## [25] &quot;package:tidyr&quot; &quot;package:tibble&quot; &quot;package:ggplot2&quot; ## [28] &quot;package:tidyverse&quot; &quot;package:MASS&quot; &quot;package:stats&quot; ## [31] &quot;package:graphics&quot; &quot;package:grDevices&quot; &quot;package:utils&quot; ## [34] &quot;package:datasets&quot; &quot;package:methods&quot; &quot;Autoloads&quot; ## [37] &quot;package:base&quot; ## ------------------------- ## [1] 10000 51000 21000 31000 2000 ## ------------------------- ## zipcode cityName avgSalary population tax ## 1 10000 Zagreb 6359 790017 18 ## 2 51000 Rijeka 5418 128384 15 ## 3 21000 Split 5170 167121 10 ## 4 31000 Osijek 4892 84104 13 ## 5 2000 Dubrovnik 5348 28434 10 ## ------------------------- Let’s explain what happened in the example above. With the attach function, the cities data frame became a “mini-environment”, i.e. its columns became available within the search path. The obvious benefit of this is that we can refer to the columns directly, without referencing the original data frame and operator $. But this seemingly practical trick has hidden traps - first, if the column names match those of the global environment, then those columns will not be visible (we will be notified of this with an adequate warning). Second - and much more problematic - if we try to change the column of the data frame by directly referencing it, R will prevent it and will quietly apply the copy-on-modify principle by creating a new, global variable that will be a copy of the referenced column. An inattentive analyst can miss the fact that the changes are not reflected at the data frame itself, which can have far-reaching consequences. These potential problems are very widespread among R beginners, so in the R literature it is commonly suggested that the attach function is not used unless it is deemed very necessary. For example. Google’s R-style guide says “the error potentials for using the attach function are numerous, so avoid it” . If we really want to simplify our code and avoid repeating the name of the data frame every time we refer to one of its columns, it is recommended to use additional packages that are designed to facilitate the management of data frames. These packages will be introduced in some of the next chapters. Exercises Load the following packages in the working environment: magrittr, dplyr, tidyr,ggplot2. Print the search path and check where the loaded packages are. The following commands will create a vector of 20 randomly selected natural numbers from 1 to 100. # 20 random natural numbers from 1 to 100, with repetition set.seed(1234) a &lt;- sample(1:100, 20, replace = T) Use the cheat sheets and/or official documentation to find built-in functions that perform the following tasks. Print vector a the values of the vector a arranged in reverse order unique values from the vector a the values of the vector a sorted in ascending order We mentioned that loaded packages are actually “environments”. If we want to get a direct reference to them, we need to use as.environment and the package name. Try to get a reference to the package::magrittr package in the form of an environment, and use ls to check which names are contained in it. Programirajmo u R-u by Damir Pintar is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License.Based on a work at https://ratnip.github.io/FER_OPJR/ "],
["user.html", "7 User Defined Functions 7.1 How to define a function Exercises", " 7 User Defined Functions Although R is an object-oriented programming language, it leans heavily towards the realm of functional programming. This functional paradigm is not new (it dates back to the 1950s), but has recently gained considerable popularity as a kind of complement to object-oriented programming, which might be said was the dominant programming paradigm in the last few decades. In order not to delve too deep into the subject of functional programming and its relationship to object-oriented principles, we will list only a few indicative features and guidelines related to these two paradigms. Object-oriented programming in principle, sees the program as a system of nouns where the components are realized in the form of objects that encapsulate the attributes of the mentioned noun and the methods that perform certain tasks related to the given noun. Also, object-oriented systems focus on the controlled change of the components’ state as a result of the exchange of messages between them. Functional programming sees the program as system of verbs where the functions, ie the tasks we want to execute, have priority over the components over which these tasks are executed. Functional programming models the information system through components that generally do not change their own state, so that the result of the program is strictly dependent on the inputs, which facilitates testing and maintenance. In short, the distinction between object-oriented programming and function programming is often stated as follows: “For object-oriented programming, we create data that contains functions, and in fuctional programming we create functions containing data”. We do not have to be too bothered with the features of functional programming to learn R, nor should it be necessary to adopt a completely new programming paradigm. But for successful mastering of R, adapting some of the functional programming concepts can prove to be extremely useful since it will allow us to write a cleaner and more efficient code that will be in accordance with the way in which R is designed as a language. In R, the following is true: functions are “first-order” objects, we can reference them with a variable of the selected environment, send them to functions as arguments, receive as return values function and store them in data structures, such as a list. The function in R is simply an “executable” object. A large number of functions - especially those that replace the program loop construct - work on the principle of functional languages where we perform the work in a manner that declaratively specifies which function we want to apply on which data structure, and let the programming language itself perform low-level tasks such as iteration by structure and preparation of the result. Examples of this will be learned soon, and now let’s first get to know the syntax of user defined functions in R. 7.1 How to define a function In the general case, the definition of a new function looks like this: function_name &lt;- function(input arguments) { function_body return_statement } We can notice that in the function definition we use the operator &lt;-. This is not a coincidence - the definition of a function is nothing else than creating a `function ’object that we then associate with a particular variable; the name of the variable is actually the “name” of the function. In R we do not define the types of input and output arguments. Input arguments have the name and optional default value. The function formally returns one value, which is not necessarily restrictive if we want to return more values -we simply encapsulate them in the form of a vector or list. The return keyword is optional - the function returns the result of the last expression in the function, so it is often enough to specify only the variable that represents the return value as the last order of the function. Finally, if we want to increase the robustness of the function in such a way that we reject the execution of the logic within a function if certain conditions are not satisfied, we can use the stopifnot function. This function calculates the default logical expression and terminates the function if the specified condition is not true. Assignment 7.1 - first user defined function # write the function `parallelMax` which requires two numeric vectors as input # and returns a vector of the same size containing the larger # between two corresponding elements of the original vectors # if one or both vectors aren&#39;t numeric or aren&#39;t the same size # the function must throw an error # do not use loops! # execute this new function over the following vector pairs # c(T, F, T) i c(1, 2, 3) # c(1, 2, 3, 4) i c(5, 6, 7) # c(1, 2, 3) i c(0, 4, 2) # (second part of the exercise should be tested inside the console!) # write the function `parallelMax` which requires two numeric vectors as input # and returns a vector of the same size containing the larger # between two corresponding elements of the original vectors # if one or both vectors aren&#39;t numeric or aren&#39;t the same size # the function must throw an error # do not use loops! parallelMax &lt;- function(a, b) { stopifnot(is.numeric(a) &amp;&amp; is.numeric(b) &amp;&amp; length(a) == length(b)); ifelse(a &gt; b, a, b) } When calling a function, we can optionally specify the parameter names, and R will even allow the mixing of named and unnamed parameters (although this is not something we should often use in practice). When R connects the sent values with formal parameters, the named parameters will have priority and will be resolved first, after which the unnamed parameters will be resolved in te order they were provided. We can see this in the next exercise, in which we will use the opportunity to showcase a very useful function - paste. This function concatenates character strings with the addition of a space separator (there is an alternative function paste0 for joining without spaces). Assignment 7.2 - function parameters printABC &lt;- function(a, b, c) { print(paste(&quot;A:&quot;, a, &quot;B:&quot;, b, &quot;C:&quot;, c)) } # think before executing - what will be printed with the following command? printABC(1, a = 2, 3) printABC &lt;- function(a, b, c) { print(paste(&quot;A:&quot;, a, &quot;B:&quot;, b, &quot;C:&quot;, c)) } printABC(1, a = 2, 3) ## [1] &quot;A: 2 B: 1 C: 3&quot; In practice, we should stick to conventions to first use unnamed parameters, and then named. It is customary to set only those named parameters whose default value does not suit us, whereas strict ordering does not matter (although using the order provided in the function definition will increase the legibility of our code). If we want to write a function that receives an arbitrary number of arguments, we use the element ..., i.e. the ellipsis. An example of this feature is the built-in paste function which can receive an arbitrary number of character strings. If we use the ellipsis in our functions, we should place them at the end of the list of input arguments, and within the function itself we simply convert it into a list, and then access its parameters in the way that suits us. Assignment 7.3 - function with an arbitrary number of parameters printParams &lt;- function(...) { params &lt;- list(...) for (p in params) print(p) } # call the above function with any random parameters printParams(c(1, 2, 3), 5, T, data.frame(x = 1:2, y = c(T, F))) ## [1] 1 2 3 ## [1] 5 ## [1] TRUE ## x y ## 1 1 TRUE ## 2 2 FALSE Finally, let’s recall the chapter in which we talked about S3 objects and the fact that R does not have a formal system for creating and using objects, but it is recommended that you use a separate constructor function that replaces the “manual” matching of the object and the declaration of its class. Now that we know how to create our own functions, we can look at how a potential constructor of the ‘Person’ class would look. # constructor of the `Person` class Person &lt;- function(id, surname, weight) { stopifnot(is.character(id)) stopifnot(is.character(surname)) stopifnot(is.numeric(weight) &amp;&amp; weight &gt; 0) p &lt;- list(id = id, surname = surname, weight = weight) class(p) &lt;- &quot;Person&quot; p } Let’s try to create a new object of the Person class with the help of this constructor. Assignment 7.4 - constructor function # create `john`, a `Person` with the following characeristics: # id: 1357135713, surname: Watson, weight: 76 # print `john` john &lt;- Person(&quot;135135713&quot;, &quot;Watson&quot;, 76) john ## $id ## [1] &quot;135135713&quot; ## ## $surname ## [1] &quot;Watson&quot; ## ## $weight ## [1] 76 ## ## attr(,&quot;class&quot;) ## [1] &quot;Person&quot; The advantage of the constructor is additional robustness in the form of exactly defined attribute names, as well as the capabilities of embedding additional checks (eg id and surname must be character strings and the weight must be a number, etc.). If we create our own S3 objects, it is recommended that we define the corresponding constructor functions for them. 7.1.1 The “copy-on-modify” principle One of the more common questions raised when learning a new programming language is whether the functions work in “call-by-value” or “call-by-reference” mode. The difference is basically whether the function may change the content of the variables sent at the place of the formal argument or not; call-by-value principle forward only copies of original arguments. On the other hand, the call-by-reference principle makes it so the function receives “references” of the original variables, i.e. it behaves as if the original variables were passed to the function and all changes to them would be reflected in the calling function or program. The language R uses a hybrid principle known as copy-on-modify. With this principle, references are forwarded to the function, which allows us to transmit “large” variables without fear of unnecessary copying. But this is only valid if the function does not change the value of the resulting variables - at the moment when the function attempts to make any changes, copying the variable is carried out and the function continues to work on the copy. Because of this it is said that R as such does not support call-by-reference (one reason for introducing “reference class” objects, i.e. “RC object model” into language R, is precisely the introduction of this principle). Let’s check the above statements in a following example. # attempt to change variables from the calling environment f &lt;- function() { cat(&quot;x inside function:&quot;, x, &quot;\\n&quot;) x &lt;- x + 1 cat(&quot;x after change:&quot;, x, &quot;\\n&quot;) } x &lt;- 5 f() cat(&quot;x after return:&quot;, x, &quot;\\n&quot;) ## x inside function: 5 ## x after change: 6 ## x after return: 5 The function actually creates a new temporary environment within which it stores “local” variables. In the above example, the f function introduces a new x variable that masks the external variable x so that all changes no longer reflect the value of the external variable. It is important to note that the function could access an external variable even without sending it to the function, since by referencing the x variable that does not exist in the local environment the R search function will continue in the parent’s environment, which in this case would be the global environment. Attempting to change this variable would still fail - R would detect an attempt to change the variable and create a local copy of the same name. Does this mean that a function can never change variables from the calling environment? Of course not. One way to do this is to send a reference to the environment within which the object we are changing exists (or, alternatively, let the function itself get a reference to the global environment if the variable is there). Assignment 7.5 - changing global environment variables # implement function `f` which gets a reference to the global environment # and increases the &quot;outer&quot; `x` by 1 x &lt;- 5 # call `f(x)` and then print `x` # implement function `f` which gets a reference to the global environment # and increases the &quot;outer&quot; `x` by 1 f &lt;- function() { e &lt;- globalenv() e$x &lt;- e$x + 1 } x &lt;- 5 # call `f(x)` and then print `x` f() print(x) ## [1] 6 A simpler way of solving the above task would be using the &lt;&lt;- operator. This operator’s function is to change the variable of the given name that is located somewhere in the search path. R will follow the search path, and change the first occurrence of the specified variable. If the variable of this name does not exist anywhere in the search path, R will create a new variable in the first environment above the environment of the function. # operator `&lt;&lt;-` f &lt;- function(x) { x &lt;&lt;- 7 x &lt;- 6 } x &lt;- 5 f() x ## [1] 7 This operator is potentially unpredictable and we will achieve greater robustness using the assign or$operator with reference to the environment where the variable we want to change is. Finally, we must mention one feature of functions in R - the so-called. “lazy evaluation”. This simply means that R will not evaluate the received parameter until it is explicitly used. Up to that moment, this object is so-called. “promise” - R “knows” how to evaluate that object but it will not do so until it really needs it. This increases the efficiency of the language; if a parameter is used only in a conditional branch, then in scenarios when it is not needed it will not consume memory. But equally, we need to be careful because a lazy evaluation can lead to unexpected problems if we do not take into account its existence. 7.1.2 Function as an object We have already said that R has good support for the so-called “functional programming” which represents a programming paradigm that puts emphasis on designing functions without reliance on objects with interchangeable states. One of the characteristics of such languages are “first class functions”, which means that the language supports the definition of functions in such a way that they are equal objects to all other types of objects - they can be stored in a variable, used as an input argument of another function or as a return value, stored in other data structures, etc. Let us show this on a trivial example. We know that R offers the function sum within the base package, and this function calculates the arithmetic sum of the vector elements we send to it as an input parameter. But sum is actually the name of the variable that references the code that implements this function. If we want, we can very easily bind this function to some other variable by which we have effectively changed its name, or - better said - provided an alternative way of calling from a completely different environment. sum2 &lt;- sum sum2(1:10) # same as sum(1:10) This is easiest to understand in a way that the function is simply an “callable variable”, whereby the “call” refers to the use of a syntax that includes a reference to the function and input arguments framed in parentheses, which will return some value after execution in the R environment. The function can also be a return value from another function. funcCreator &lt;- function() { f &lt;- function(x) x + 1 return(f) } newFunc &lt;- funcCreator() # we get the &quot;add one&quot; function newFunc(5) ## [1] 6 The function simply creats a new function and returns it to the calling program as it would have done with any other object. The return value is stored in the variable that is now “callable” - if we add brackets and parameters it will be executed in the way it is defined within the function that created it. Note that we could use the fact that the function returns the result of the last expression and define the function even shorter: # shorter definition funcCreator &lt;- function() { function(x) x + 1 } These functions are often referred to as “factories” or “generators” of functions, and in contrast to the above example, in practice, the function generator often receives some parameters that determine how the returned function will behave. Try to create a function factory that returns the multiplication functions using a pre-set parameter. Assignment 7.6 - function factory # create the `multiplicationFactory` function that creates # multiplication functions by the pre-set constant # use the above function to create the `times2` function # which doubles the received number # call the `times2` function with parameter 3 and print out the result # create the `multiplicationFactory` function that creates # multiplication functions by the pre-set constant multiplicationFactory &lt;- function(x) { function(a) a*x } # use the above function to create the `times2` function # which doubles the received number times2 &lt;- multiplicationFactory(2) # call the `times2` function with parameter 3 and print out the result times2(3) ## [1] 6 The multiplicationFactory function actually creates a “family” of functions that all provide the multiplication option with the selected number - i.e. parameter selected by the programmer itself. This way of managing functions may be initially confusing, but by using it in practice (which we will show in the next chapter), it is easy to notice the added flexibility and effectiveness of such an approach. If we define a function, and do not bind it to some variable, then we created the so-called “anonymous function”. # anonymous function function(x) x * x We can notice that each function is initially “anonymous”. If we return to the function definition syntax, we see that it is actually a combination of creating an anonymous function and binding it to a new variable. Of course, leaving the function anonymous as we did in the example above does not make much sense, just as it does not make sense to define a vector or list without creating a reference to that object - in that case the created object is not in any way usable because there are no links to it and will be quickly deleted by R within the “garbage collection” routine. We can ask ourselves - is there a scenario where the anonymous function is meaningful and useful? Explicit anonymous functions are used when a “disposable” function is needed, for example, as an argument for some other function. If the function we want to send as an argument is easy to define in a single line, and we do not plan to use it afterwards in the program, then it makes no sense to define it separately and assign it its own reference. An example of this will be seen in the apply family of functions. At the end of this section, we repeat the most important things - in R, the function is an object like any other, the only specificity is that it is an object that is “executable”, ie, which, using the function call syntax, does some work and returns some value. Even anonymous function can be executed (although only once, since we do not have a reference for future calls). # calling the anonymous function (function(x) x + 1)(2) ## [1] 3 7.1.3 Generic functions We have already mentioned generic functions in the chapter on objects, but let’s briefly recall what they are all about. The programming language R is not based on the so-called “message exchange principle” of the object-oriented languages, where, for example, the drawing of the graph might be performed like this: # the usual OOP principle invokes object methods graph.draw() but like this: # R just calls the method unto an object draw(graph) In the first case, a graph is an object that implements a special drawing method, and we have to call it in order to get the required graph image. In the second, there is an “external” function that “knows” how to draw a graph. This function is called a “generic function”. The properties of the generic function are as follows: the function has an intuitive, clearly defined purpose expected mode of operation is similar for multiple object types (eg drawing will always result in an image of sorts) each type of object requires its own implementation depending on the characteristics of the object (e.g. the way the method of drawing the circle differs from drawing a square) The method of implementing generic functions (for S3 objects!) is actually extremely simple, which is probably the reason for their wide acceptance and great popularity in the R community. The process is reduced to three simple steps: choose the name of a generic function (eg infoAbout) and declare that it is a generic function alternatively, choose one of the existing generic functions create an object and declare its class (eg Person) we implement the function called gen_function_name.class_name (e.g. infoAbout.Person) And that’s all! R does not require any additional steps, the above is quite sufficient for R to recognize the new generic function and apply it to all objects for whose class this generic function is implemented in the form gen_function_name.class_name (or gen_function_name.default for all classes for which there is no special implementation). In the next exercise, let’s try to implement the generic infoAbout method for the Person class. Assignment 7.7 - new generic function peter &lt;- Person(id = &quot;12345678&quot;, surname = &quot;Parker&quot;, weight = 78) # create a new generic `infoAbout` function using the `UseMethod` function infoAbout &lt;- function(x) UseMethod(&quot;infoAbout&quot;) # implement a `infoAbout.Person` function which takes a `Person` # and writes the following on screen # ID: &lt;id&gt;, surname: &lt;surname&gt;, weight: &lt;weight&gt; # use the `paste` function to prepare the printout # and `cat` to put it on screen # implement a `infoAbout.default` function # which simply fowards the input parameter to the `cat` function # call `infoAbout` with `peter` as parameter # call `infoAbout` with `1:5` as parameter # implement a `infoAbout.Person` function which takes a `Person` # and writes the following on screen # ID: &lt;id&gt;, surname: &lt;surname&gt;, weight: &lt;weight&gt; # use the `paste` function to prepare the printout # and `cat` to put it on screen infoAbout.Person &lt;- function(p) { rez &lt;- paste(&quot;ID:&quot;, p$id, &quot;, surname:&quot;, p$surname, &quot;, weight:&quot;, p$weight, &quot;\\n&quot;) cat(rez) } # implement a `infoAbout.default` function # which simply fowards the input parameter to the `cat` function infoAbout.default &lt;- function(x) cat(x) # call `infoAbout` with `peter` as parameter infoAbout(peter) # call `infoAbout` with `1:5` as parameter infoAbout(1:5) ## ID: 12345678 , surname: Parker , weight: 78 ## 1 2 3 4 5 Of course, we did not necessarily have to create our own function to get the “print” functionality for our new class - it would probably be better to use already existing generic functions such as print or cat. Assignment 7.8 - augmenting the existing generic functions # make sure `print` is a generic function # (print out its source code by referencing its name) # augmnet the `print` function so it allows pretty printing # of the `Person` class # (you may use the already created `infoAbout.Person` class) # call `print` with `peter` as parameter (or try the autoprint!) # make sure `print` is a generic function # (print out its source code by referencing its name) print # augmnet the `print` function so it allows pretty printing # of the `Person` class # (you may use the already created `infoAbout.Person` class) print.Person &lt;- infoAbout.Person # call `print` with `peter` as parameter (or try the autoprint!) print(peter) ## function (x, ...) ## UseMethod(&quot;print&quot;) ## &lt;bytecode: 0x000000000e3f29f0&gt; ## &lt;environment: namespace:base&gt; ## ID: 12345678 , surname: Parker , weight: 78 Finally, we demonstrate the ability of R to list all the currently known implementations of a generic method. To do this we simply use the methods function to which we pass the name of the method concerned. With the same function we can also check which generic functions implementations exist for a particular class. For this we use the class parameter to which we are passing the class name for which we are interested in finding generic functions implemented for it. Assignment 7.9 - methods function # list all implementations of the `summary` function # check with generic function implementations exist for the `factor` class # list all implementations of the `summary` function #methods(summary) # try on console! cat(&quot;-----------------------\\n&quot;) # check with generic function implementations exist for the `factor` class methods(class = &quot;factor&quot;) ## ----------------------- ## [1] - / [ [[ [[&lt;- ## [6] [&lt;- + all.equal Arith as.character ## [11] as.data.frame as.Date as.duration as.interval as.list ## [16] as.logical as.period as.POSIXlt as.vector as_date ## [21] as_datetime as_factor brief cbind2 coerce ## [26] Compare corresp droplevels fixed format ## [31] histogram initialize is.na&lt;- is_vector_s3 length&lt;- ## [36] levels&lt;- Logic Math Ops plot ## [41] print rbind2 recode relevel relist ## [46] rep scale_type show slotsFromS3 summary ## [51] Summary type_sum xtfrm ## see &#39;?methods&#39; for accessing help and source code Exercises R has a which function which converts a logical vector into a numeric one containing indexes where the original vector has a TRUE value (so c(T, F, F, F, F, T, F, T) becomes c(1, 6, 8)). Create a function which replicates this behaviour. Take the numerical vector x of length n. In statistics, the standardized moment of the k-th order is calculated like this: \\[\\frac{1}{n}\\sum_{i=1}^n{(x_i - \\bar{x})}^{k+1}\\] Create a factory of moment functions (moment(k)) for calculating the standardized central moment of the k-th order. Create the functions zero_moment (x) and first_moment (x) with parameter values k set to 0 and 1 accordingly. Test the functions on vector 1:1000. Compare the results given by the sd (standard deviation) function over the vector 1: 1000 and root of the first moment you have calculated. Implement a constructor for the class Employee which inherits the Person class defined by the following constructor and print implemenation: Person &lt;- function(id, surname, weight) { p &lt;- list(id = id, surname = surname, weight = weight) class(p) &lt;- &quot;Person&quot; p } print.Person &lt;- function(p) { rez &lt;- paste(&quot;ID:&quot;, p$id, &quot;, surname:&quot;, p$surname, &quot;, weight:&quot;, p$weight, &quot;\\n&quot;) cat(rez) } Employee has all the attributes of the Person class as well as the superior attribute which represents a reference to a employee who is his/her superior (if such exists, otherwise it should be NA). Create two objects of the Employee class (one superior to another) and print them with the print function. Then implement your own version of the generic function print for the Employee class that prints employee data and his/her superior employee data (if it exists, otherwise it prints a message that there is no superior employee). Reprint both employees with the print function. Programirajmo u R-u by Damir Pintar is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License.Based on a work at https://ratnip.github.io/FER_OPJR/ "],
["apply.html", "8 ‘Apply’ family of functions 8.1 What are apply functions? 8.2 The apply function 8.3 The lapply, sapply and vapply functions 8.4 Other functions from the apply family and the available alternatives Exercises", " 8 ‘Apply’ family of functions 8.1 What are apply functions? Very often, knowledge of the basics of the language R is reflected by the skill of using the so-called apply family of functions, available in thebase package. These functions are specifically designed to perform repetitive tasks over various data structures, and as such they replace the program logic that would usually be realized in a through program loops. Additionally, these functions typically receive other functions as input arguments and, to a certain extent, encourage the functional programming paradigm. The family name comes from the fact that these functions commonly have a suffix “apply”. Some of the functions from this family are: apply lapply sapply vapply tapply,mapply, rapply … All of these functions work in a similar way - they receive a data set, a function that we want to apply to elements of that set, and optional additional parameters, and as the output give a set of function results, most often “packaged” in an appropriate format. The difference is mainly in the types of input and output arguments, as well as specific details about the implementation of the function itself and/or the way results are prepared. This family of functions is best learned through examples. We will begin with the “basic” function - apply. 8.2 The apply function The apply function is the only one that literally shares the name with the family of these functions. It is intended to work with matrices (actually with arrays, but since it is relatively rare to use data structures that have more than two dimensions, here we will focus only on matrices). The command syntax is as follows: result &lt;- apply( &lt;matrix&gt;, &lt;rows (1) or columns (2)&gt;, &lt;function_name&gt; ) Or, described in words, to implement the apply function, we: choose a matrix decide whether to “cut it” by rows or columns declare which function we want applied to each row (or column) Depending on how function works, as a result we get a matrix or (which is a more frequent case) a vector. Let’s try to use this function in a concrete example. Assignment 8.1 - the apply function m &lt;- matrix(1:9, nrow = 3, ncol = 3, byrow = TRUE) # print matrix `m` # use the `apply` function to calculate # and print the column sums of the `m` matrix # use the `apply` function to calculate # and print the multiplicaton of row elements # from the `m` matrix # print matrix `m` m cat(&quot;------------\\n&quot;) # use the `apply` function to calculate # and print the column sums of the `m` matrix apply(m, 2, sum) cat(&quot;------------\\n&quot;) # use the `apply` function to calculate # and print the multiplicaton of row elements # from the `m` matrix apply(m, 1, prod) ## [,1] [,2] [,3] ## [1,] 1 2 3 ## [2,] 4 5 6 ## [3,] 7 8 9 ## ------------ ## [1] 12 15 18 ## ------------ ## [1] 6 120 504 If we want to perform a custom task to the rows/columns, we often use an anonymous function, for example: apply(m, 1, function(x) x[1]) # return the first element of each row Assignment 8.2 - the apply function and anonymous functions # for each row of `m` calculate the natural logarithm # of the sum of row elements # rounded to 2 decimals # use the `apply` function # for each row of `m` calculate the natural logarithm # of the sum of row elements # rounded to 2 decimals # use the `apply` function apply(m, 1, function(x) round(log(sum(x)),2)) ## [1] 1.79 2.71 3.18 Let’s repeat - apply (and related functions) implicitly * disassembles the input data structure into elements. In the examples above, these elements - rows or columns - are actually numeric vectors. The argument x received by an anonymous function is exactly that vector, or, better said each of these vectors* that are sent one by one. The results of the function are “remembered” and “packed” into the final result. Let’s try to program the last example without using the apply function. Assignment 8.3 - loop as the alternative to the apply function # for each row of `m` calculate the natural logarithm # of the sum of row elements # rounded to 2 decimals # use the for program loop # for each row of `m` calculate the natural logarithm # of the sum of row elements # rounded to 2 decimals # use the for program loop rez &lt;- numeric(nrow(m)) for (i in 1:nrow(m)) rez[i] &lt;- round(log(sum(m[i,])), 2) rez ## [1] 1.79 2.71 3.18 If we compare the syntax of the examples with and without using the apply function, we can see how much the syntax which uses apply s actually “cleaner”. If we use loops, we must explicitly specify the logic of passing through the structure, which draws attention away from the job description that we actually want to do. What if we want to send to the apply function a function which needs several parameters? For example, let’s say that instead of the upper function that extracts the first line element we want a function with two parameters - the first a vector (matrix row or column), and the second an integer that indicates the index of the element to extract. The answer is simple - just add additional parameters at the end of the function call. # apply function and input function with multiple parameters apply(m, 1, function(x,y) x[y], 2) # second element of each row Finally, it should be noted that for similar processing of data in the matrix form, we do not necessarily need to use apply - many popular operations such as adding row or column elements, calculating the average of the elements of the rows and columns, and the like. This has already been implemented through functions such as rowSums,colSums, rowMeans,colMeans and the like. They are easier to use, but specialized - for more flexibility, the most common option is apply. 8.3 The lapply, sapply and vapply functions The name of the lapply function comes from list apply - i.e. apply the function to the elements of lists. To put simply - it is a function that will receive the list and a function as the input arguments, apply the functions to each individual list element and return again a new list as a result. Assignment 8.4 - the lapply function l &lt;- list(a = 1:3, b = rep(c(T, F), 10), c = LETTERS) # use the `lapply` function to calculate the length (number of elements) # of each element of the `l` list # use the `lapply` function to calculate the length (number of elements) # of each element of the `l` list lapply(l, length) ## $a ## [1] 3 ## ## $b ## [1] 20 ## ## $c ## [1] 26 Just like with the apply function, for thelapply function, we often use anonymous functions as a parameter. The following task has no special practical use, but it will help us understand the functionality of the lapply function in combination with a slightly more complex anonymous function. Assignment 8.5 - the lapply function and anonymous functions # process the elements of the `l &#39;list as follows: # - Calculate the mean value if it is a numerical vector # - count the values of TRUE if it is a logical vector # - calculate the length of the vector for all other cases # use the `lapply` function and an anonymous function # do not forget that anonymous function can also use blocks! # process the elements of the `l &#39;list as follows: # - Calculate the mean value if it is a numerical vector # - count the values of TRUE if it is a logical vector # - calculate the length of the vector for all other cases # use the `lapply` function and an anonymous function # do not forget that anonymous function can also use blocks! lapply(l, function(x) { if (is.numeric(x)) { mean(x) } else if (is.logical(x)) { sum(x) } else length(x) }) ## $a ## [1] 2 ## ## $b ## [1] 10 ## ## $c ## [1] 26 The lapply function is essentially quite simple to use and is very popular due to this fact. But once we use it for a while, we can find it irritating that t it always returns the list as a result, although some other data structure would be more suitable for us - for example a vector, especially if the resulting list has just simple numbers as elements. For this reason, R offers the unlist function to simplify the list to a vector. Assignment 8.6 - the unlist function l &lt;- list(a = 1:10, b = 10:20, c = 100:200) # calculate the mean value of the elements of the `l` list # print the results as a numeric vector # use `lapply` and `unlist` # calculate the mean value of the elements of the `l` list # print the results as a numeric vector # use `lapply` and `unlist` unlist(lapply(l, mean)) ## a b c ## 5.5 15.0 150.0 The displayed combination of lapply andunlist will give us as a result a one-dimensional vector, which in many cases is what we want. But sometimes some other data structure would suit us - for example, a matrix. In this case we need an additional step in transforming a one-dimensional vector into a matrix using the matrix function, with the number of rows and columns being explicitly assigned. The question may arise - why is lapply not able to check the structure of the result it has created and determine the optimal data structure for formatting it (vector, matrix, or list)? That’s exactly the idea behind the sapply function, or simplified list apply. This function first performs lapply internally, and then simplifies the result to a vector, matrix or array, depending on the characteristics of the results obtained. Assignment 8.7 - the sapply function l &lt;- list(a = 1:10, b = 10:20, c = 100:200) # calculate the median of elements of the `l` list # and collect the results in a numeric vector # use the `sapply` function # extract the first and last element of each of the elements of the `l` list # use `sapply` and anonymous function # calculate the median of elements of the `l` list # and collect the results in a numeric vector # use the `sapply` function sapply(l, median) cat(&quot;------------\\n&quot;) # extract the first and last element of each of the elements of the `l` list # use `sapply` and anonymous function sapply(l, function(x) c(x[1], x[length(x)])) ## a b c ## 5.5 15.0 150.0 ## ------------ ## a b c ## [1,] 1 10 100 ## [2,] 10 20 200 Note that as a result in the last example, we received a matrix, but that R formed it “by columns”. If we wanted a matrix with elements arranged in rows, we can not use sapply for this directly, because the matrix is formed internally, without the possibility of forwarding thebyrow = T parameter. To obtain such a matrix, one option is already mentioned with the combination of lapply,unlist and matrix, or - more simply - transposing thesapply results using t function (from transpose). The sapply function is quite popular due to its simplicity and efficiency, so it is relatively often used in interactive analysis. On the other hand, the use of this function in program scripts is not recommended since its result is unpredictable in the general case - e.g. the script can expect a matrix in the continuation of the code, and the sapply function, due to the specificity of the input data, returns the vector, which can cause unforeseen results, which is not easy to spot later and diagnose where the error occurred. If we are developing our own programs in R and want to use sapply, then the better choice will be thevapply function, which works identically to sapply, but uses an additional parameter called FUN.VALUE with which we explicitly define what kind of “simplification” we expect. For example. numeric(3) means that the result of applying the function to each element of the original list should be a numeric vector of three elements. If the result for any list item differs from the expected one, the function will raise an error. Assignment 8.8 - the vapply function myList &lt;- list(numbers &lt;- c(1:5), names &lt;- c(&quot;Ivo&quot;, &quot;Pero&quot;, &quot;Ana&quot;), alphabet &lt;- LETTERS) # think which of the following calls will be successful, # and which will throw out the error # check the results on the console vapply(myList, length, FUN.VALUE = numeric(1)) vapply(myList, function(x) as.character(c(x[1], x[2])), FUN.VALUE = character(2)) vapply(myList, function(x) as.logical(x), FUN.VALUE = character(1)) Finally, let’s return briefly to lapply and consider one important fact - it is intended for use on lists, and data frames are actually lists. In other words, the lapply function is very handy for processing tabular datasets when we want to apply a particular function to the columns of the data frame. One of the more frequent operations performed in data analysis is the so-called. “normalization” of the numeric columns of the data frame - i.e. reducing all numerical values to “normal” distribution with the arithmetic mean of 0 and standard deviation of 1. This can be done by reducing each individual value by the arithmetic mean of the column (the mean function) and dividing with standard deviation of the column (function sd). This is a great way to demonstrate the use of lapply with data frames. Assignment 8.9 - the lapply function and data frames df &lt;- data.frame( a = 1:10, b = seq(100, 550, 50), c = LETTERS[1:10], d = rep(c(T,F), 5), e = -10:-1) # normalize numerical columns using `lapply` # do not change the remaining columns # round the normalized values to three decimal places # save the result in the df variable # print df # normalize numerical columns using `lapply` # do not change the remaining columns # round the normalized values to three decimal places # save the result in the df variable df &lt;- lapply(df, function(x) { if (is.numeric(x)) { round((x - mean(x))/sd(x), 3) } else x }) # print df df ## $a ## [1] -1.486 -1.156 -0.826 -0.495 -0.165 0.165 0.495 0.826 1.156 1.486 ## ## $b ## [1] -1.486 -1.156 -0.826 -0.495 -0.165 0.165 0.495 0.826 1.156 1.486 ## ## $c ## [1] A B C D E F G H I J ## Levels: A B C D E F G H I J ## ## $d ## [1] TRUE FALSE TRUE FALSE TRUE FALSE TRUE FALSE TRUE FALSE ## ## $e ## [1] -1.486 -1.156 -0.826 -0.495 -0.165 0.165 0.495 0.826 1.156 1.486 We see that after using lapply we get a list and that if we want the result in the form of a data frame we need to add another step using theas.data.frame function. If we are looking for a simpler way that immediately gives the data frame as a result, there is one convenient “trick” that we will explain below. Let’s look at the solution of the previous problem, and put the following little change in the assignment of the result of the lapply function: df[] &lt;- lapply(...) In this way, R will not create a “new” variable named df, but rather thelapply result will be entered in the ’all rows and columns of the df data frame. This made it possible for us to get the result in the form of a data frame, which we actually wanted. For this very reason, in R scripts, we will often see a similar syntax (df [] &lt;- lapply ...). Try to modify the above example in the above manner and make sure that the result will be a data frame. Another commonly used trick in working with data frames is the following command: sapply(df, class) This command actually gives us the answer to the question - which types are the columns of the given data frame? Although there are other ways to get this information, this method is popular both because of the compactness of the results and the independence of additional packages. 8.4 Other functions from the apply family and the available alternatives In the previous chapters, we have probably listed the most popular members of the apply family. This family has more members, including some who do not have a suffix -apply: mapply, which works in parallel over multiple data structures rapply, which recursively applies functions within the structure tapply, which applies functions over sub-groups within a structure defined by factors Map, themapply version, which does not simplify the result by, thetapply version for data frames etc. The reason why these functions will not be explained in detail is twofold: firstly, as already mentioned, these functions are in practice applied much less often than the functions we have shown in the previous chapters. Secondly, with the increase in popularity of the language R, a large number of packages are oriented to improve the existing functions of the language R in the sense of easier and more efficient programming, especially when working with data frames. If we are looking for convenient alternative functions to those from the apply family, it’s recommended to look at some of the following packages plyr - an extremely popular package that, among other things, offers a number of functions very similar to apply functions, but derived in a way that they have a consistent signature and explicitly defined input and output formats that are easily read from the function name itself (in particular, the first letters ); so the llply function uses a list as both the input and the output, whilemdply needs a matrix as input and outputs a data frame purrr - a package that replaces the functions of theapply family with functions corresponding to similar functions from other programming languages; since the application of the same function to a number of elements of a data structure in functional languages is often called “mapping”, the set of functions of this package carries the prefix maps_, and the function names often correspond to the expected results (for examplemap2_lgl means that as a result we expect a logical vector , and the map2_df a data frame) dplyr - a relatively new package, which in a certain sense represents the successor of the plyr package but oriented almost exclusively toward data frames; the functions of this package are not so much oriented to replace the apply family functions as providing a specific platform for working with data frames in a manner similar to languages oriented precisely for this purpose, such as, for example, the SQL language In future lectures we will introduce the dplyr package precisely because this package greatly facilitates and accelerates the data analysis process and is extremely well accepted in the R community. Exercises Take the m matrix created by the following command: m &lt;- rbind(1:5, seq(2, 10, 2), rep(3, 5), 3:7, seq(100, 500, 100)) With the apply function and the new anonymous function, create a vector that will contain the first even element of each row, or zero if the corresponding row does not have even elements. The following commands will create a list of 100 elements where each element will be a numeric vector of a random length of 1 to 10. set.seed(1234) myList &lt;- replicate(100, sample(1:10, sample(1:10, 1))) With the help of lapply / sapply (and additional commands if necessary), create: the numerical vector v with the lengths of the list elements list l with normalized numerical vectors of the original list numerical vector ind4 with indexes of all list elements containing number 4 the df5 data frame containing columns which have all the elements of the length of 5 from the original list Programirajmo u R-u by Damir Pintar is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License.Based on a work at https://ratnip.github.io/FER_OPJR/ "],
["pipe.html", "9 Pipeline operator and tidy data 9.1 Tidy data Exercise Tasks", " 9 Pipeline operator and tidy data Let’s look at the following example: imagine that in R we want to create 100 random real variables from the range of [0,100], round them to two decimals, select a sample of 10 variables from this set, calculate the arithmetic mean of the sample and print it on the screen. One possible solution could be: set.seed(1234) # (for repeatability) res &lt;- runif(100, 0, 100) # 100 random variables from uniform distribution from 0 to 100 res &lt;- round(res, 2) res &lt;- sample(res, 10) res &lt;- mean(res) res ## [1] 51.123 This kind of code has a lot of unnecessary repetition - in every line we use the res variable as well as the assignment operator res variable so we can store the in-between results. Alternatively, we could do everything in one row: Assignment 9.1 - Nested functions set.seed(1234) # repeat the above example, but with only one line of code mean(sample(round(runif(100, 0, 100), 2), 10)) ## [1] 51.123 Here we see a typical example of a “code sandwich” which is not only often seen in R, but also appears in most programming languages. When we use the result of one function as an input to another, we have to “nest” it in the subsequent function call ending up with an error-prone code which is equally difficult to both write and read. A natural way of “solving” a series of instructions from the previous example would be to resolve them sequentially, i.e. from “left to right”; when we finish one task, the result becomes the input for the following task and so on until the process is complete. It would be benefitial if we apply this approach through programming code. This was precisely the motivation for the development of the so-called pipeline operator, provided by the package magrittr (Bache and Wickham 2014). The weird name package is actually inspired by the name of abstract painter Rene Magritte, more specifically his famous painting La trahison des images depicting a pipe under which are the words Ceci n’est pas une pipe. In the same way, the magrittr package delivers a pipeline or pipe operator %&gt;%which is “not really a pipe”. Whatever you may think about this play on words or the painting itself which inspired this package, what is undeniable is the fact that the pipeline operator makes the code much more readable and as such has quickly become a strong favorite in the R community, especially when programming heavily relies on chaining function calls. How does the %&gt;% operator work? It’s rather simple - we place it between function calls to designate that the output from the function on the left should be the input for the function on the right. The place where the result from the previous function should go is denoted by the . symbol. We can do this as many times as we want, that is, depending on how many functions calls we “chain”. h(g(f(x), y), z, w) # code without the %&gt;% operator f(x) %&gt;% g(., y) %&gt;% h(., z, w) # code with the %&gt;% operator If the result of the previous function the first argument of the next function, then the . symbol (or in effect the whole argument) can be thrown out, so the syntax is even shorter: f(x) %&gt;% g(y) %&gt;% h(z, w) # code without dots Notice that these function calls are actually formally incorrect because they have an “invisible” first argument. Nevertheless, many R programmers prefer this syntax because it requires less typing and is somewhat more transparent, while the mentioned irregularity does not really matter in practice as long as the developer is aware of the existence of an “invisible” argument. Now let’s try to reformat our first example with the help of the `%&gt;% ’operator. Assignment 9.2 - operator %&gt;% set.seed(1234) # solve the first example again using the %&gt;% operator set.seed(1234) runif(100, 0, 100) %&gt;% round(2) %&gt;% sample(10) %&gt;% mean() %&gt;% print() ## [1] 51.123 Note that by reading the above program code, we very easily interpret the meaning of that line of program code, especially when compared to the same command written in the form of a “sandwich”. We can store the end result of our “chain” of functions in the usual way: sum1to10 &lt;- 1:10 %&gt;% sum # result is stored in variable &#39;sum1to10&#39; but it may be more visually consistent to use the “inverted” assignment operator: -&gt;. 1:10 %&gt;% sum -&gt; sum1to10 # works the same as the example above Note that in situations where the result of the previous function is the only parameter of the following function, we can completely leave out the parentheses (so in the examples above, sum,sum()orsum(.) would all work equally). Now let’s try to combine the %&gt;% operator and lapply with the example already seen in the section on functions in theapply family. Assignment 9.3 - lapply function and %&gt;% operator l &lt;- list(a = 1:10, b = 10:20, c = 100:200) # create a matrix which contains the first and last element of each list element # these elements must be in their own rows # use functions lapply, unlist and matrix as well as the %&gt;% operator # save the result in the `res` variable # print &#39;res&#39; l &lt;- list(a = 1:10, b = 10:20, c = 100:200) l %&gt;% lapply(function(x) c(x [1], x [length(x)])) %&gt;% unlist %&gt;% matrix(ncol = 2, byrow = T) -&gt; res res ## [,1] [,2] ## [1,] 1 10 ## [2,] 10 20 ## [3,] 100 200 The pipeline operator is very convenient in conjunction with “classic” functions, but we may encounter a problem when we want to combine it with other operators. The cause of the problem is the syntax - the pipeline operator achieves its efficiency by imposing a new, “sequential” syntax, which is not compatible with the syntax imposed by other operators, such as +, %% or [. If it is really important for us to have a “continuous” function call chain in our code that will contain not only functions but also other operators, then one solution is to use operators as “ordinary” functions. Namely, each operator is actually a function that shares a name with the operator (using the backtick quotation mark which allows the use of symbols as variable names), so the following pairs of expressions are actually equivalent: Example - operators as functions # each pair of instructions is equivalent 2 + 3 `+`(2, 3) 1:5 `:`(1, 5) x &lt;- c(1, 2, 3) `&lt;-`(&quot;x&quot;, c(1,2,3)) x[1] `[`(x, 1) ## [1] 5 ## [1] 5 ## [1] 1 2 3 4 5 ## [1] 1 2 3 4 5 ## [1] 1 ## [1] 1 Let’s try to use this principle in the next exercise. Assignment 9.4 - combining pipe operator with other operators set.seed(1234) # &quot;clean up &quot; the following command with the help of the pipeline operator matrix(table(sample(round(sqrt(sample(1:10000, 10000, replace = T))), 100))[1:9], 3, 3) ## [,1] [,2] [,3] ## [1,] 1 2 1 ## [2,] 1 3 1 ## [3,] 1 1 2 set.seed(1234) # &quot;clean up &quot; the following command with the help of the pipeline operator 1:10000 %&gt;% sample(10000, replace = T) %&gt;% sqrt %&gt;% round %&gt;% sample(100) %&gt;% table %&gt;% `[`(1:9) %&gt;% matrix(3, 3) ## [,1] [,2] [,3] ## [1,] 1 2 1 ## [2,] 1 3 1 ## [3,] 1 1 2 The %&gt;% operator is particularly well suited when dealing with data frames, especially in scenarios when we have a defined data transformation procedure (e.g. filtering some rows, then selecting columns, then grouping the data according to a categorical variable etc.). With the help of this operator, the code itself acts as a readily interpretable representation of our data transformation process which we can easily customize and extend later. Future examples and exercises will often extensively rely on this operator, so we recommend that you master it well before proceeding with the lessons that follow. 9.1 Tidy data It is often stated in various literature that in data analysis, data preparation is often the most time consuming segment of the process. The book “Exploratory Data Mining and Data Cleaning” mentions that preparation often requires from 50% to 80% of the total time devoted to analysis. Also, as Hadley Wickham states in his article “Tidy Data”, data preparation is often not just the first step of the analysis, but rather a process that gets repeated as new knowledge is discovered or new data is collected. Hadley Wickham also introduced the term “tidy data” to refer to the organization of a data in such a way as to facilitate its further processing and analysis. In practice, datasets we get are often not originally intended for the purposes of analysis and as such are not organized in a way that would allow them to be easily used in the analytical process. “Tidy data” is actually a set of principles which may guides us how to “rearrange” data so that its structure matches the standard, expected metadata. Tidz data principles have similarities with the relational data model, but are defined in a way that is more appropriate for statisticians and developers. These principles can be summarized as follows: the data is organized into a table each line represents an observation each column represents a property or variable of that observation Since this may sound too trivial, let’s take a look at what properties Hadley lists as typical of “messy” data: column names are not variable names but rather their values multiple different variables are stored in the same column variables are saved in rows multiple types of different observations are stored in the same table one type of observation is stored in multiple tables Below, we will give some examples of tables that do not fully conform to the definition of tidy data, and show how to easily “tidy them up”. For this, we will leverage functions provided by the tidyr package. 9.1.1 The gather andspread functions The workbook which corresponds to this leeson should have a file called students.csv. Let’s load it into our work environment. Since the file is stored using UTF-8 encoding (since it contains Croatian letters), you can also add a fileEncoding =\"UTF-8\" parameter to the read.csv command to get the special characters correctly printed. Assignment 9.5 - students data set # load data from `students.csv` file into` students` variable # don&#39;t forget the `stringsAsFactors` parameter! # familiarize yourself with the data using standard functions for this purpose # (names, sapply/class, str, head, summary ...) # in the following examples for this process, we will use the phrase &quot;briefly explore the data frame&quot; # as a shortcut for the above process students &lt;- read.csv(&quot;students.csv&quot;, fileEncoding =&quot;UTF-8&quot;, stringsAsFactors = F) str(students) head(students) ## &#39;data.frame&#39;: 27 obs. of 10 variables: ## $ JMBAG : int 1341 1342 1343 1344 1345 1346 1347 1348 1349 1350 ... ## $ Surname : chr &quot;Anic&quot; &quot;Babic&quot; &quot;Crnoja&quot; &quot;Crnjac&quot; ... ## $ Name : chr &quot;Iva&quot; &quot;Josip&quot; &quot;Petra&quot; &quot;Lucija&quot; ... ## $ Math.1 : chr &quot;2&quot; &quot;5&quot; &quot;4&quot; &quot;2&quot; ... ## $ Physics.1 : chr &quot;2&quot; &quot;3&quot; &quot;3&quot; &quot;5&quot; ... ## $ Programming : chr &quot;NULL&quot; &quot;4&quot; &quot;4&quot; &quot;2&quot; ... ## $ Electronics : chr &quot;NULL&quot; &quot;3&quot; &quot;2&quot; &quot;2&quot; ... ## $ Digital.Logic: chr &quot;4&quot; &quot;NULL&quot; &quot;3&quot; &quot;3&quot; ... ## $ Math.2 : chr &quot;2&quot; &quot;5&quot; &quot;4&quot; &quot;3&quot; ... ## $ Algorithms.1 : chr &quot;2&quot; &quot;5&quot; &quot;3&quot; &quot;4&quot; ... ## JMBAG Surname Name Math.1 Physics.1 Programming Electronics ## 1 1341 Anic Iva 2 2 NULL NULL ## 2 1342 Babic Josip 5 3 4 3 ## 3 1343 Crnoja Petra 4 3 4 2 ## 4 1344 Crnjac Lucija 2 5 2 2 ## 5 1345 Dizla Stipe NULL 4 3 5 ## 6 1346 Ermic Igor NULL 3 NULL 5 ## Digital.Logic Math.2 Algorithms.1 ## 1 4 2 2 ## 2 NULL 5 5 ## 3 3 4 3 ## 4 3 3 4 ## 5 2 2 2 ## 6 5 5 5 Note that this dataset has a lot of missing values that are written as NULL. Because R does not recognize this as a missing value, it loaded the data as character strings (or as factors if we forgot the stringsAsFactors parameter). Because the columns that relate to the ratings are obviously numeric, we can easily convert them to such using the as.numeric () (or as.numeric(as.character()) command if they are factors!). But there is a simpler way - if we know how the missing value is represented in the dataset, we can directly embed it in the read.csv command using the na.strings parameter. Assignment 9.6 - using the na.strings parameter # reload the data from the `students.csv` file into the` students` variable # add the `na.strings` parameter to the` read.csv` command with a character string representing NA # briefly explore the `students` data frame students &lt;- read.csv (&quot;students.csv&quot;, fileEncoding = &quot;UTF-8&quot;, stringsAsFactors = F, na.strings = &quot;NULL&quot;) str(students) head(students) ## &#39;data.frame&#39;: 27 obs. of 10 variables: ## $ JMBAG : int 1341 1342 1343 1344 1345 1346 1347 1348 1349 1350 ... ## $ Surname : chr &quot;Anic&quot; &quot;Babic&quot; &quot;Crnoja&quot; &quot;Crnjac&quot; ... ## $ Name : chr &quot;Iva&quot; &quot;Josip&quot; &quot;Petra&quot; &quot;Lucija&quot; ... ## $ Math.1 : int 2 5 4 2 NA NA NA 3 3 4 ... ## $ Physics.1 : int 2 3 3 5 4 3 3 4 2 2 ... ## $ Programming : int NA 4 4 2 3 NA 3 3 3 4 ... ## $ Electronics : int NA 3 2 2 5 5 3 4 2 2 ... ## $ Digital.Logic: int 4 NA 3 3 2 5 2 4 3 5 ... ## $ Math.2 : int 2 5 4 3 2 5 2 5 NA 4 ... ## $ Algorithms.1 : int 2 5 3 4 2 5 5 4 3 2 ... ## JMBAG Surname Name Math.1 Physics.1 Programming Electronics ## 1 1341 Anic Iva 2 2 NA NA ## 2 1342 Babic Josip 5 3 4 3 ## 3 1343 Crnoja Petra 4 3 4 2 ## 4 1344 Crnjac Lucija 2 5 2 2 ## 5 1345 Dizla Stipe NA 4 3 5 ## 6 1346 Ermic Igor NA 3 NA 5 ## Digital.Logic Math.2 Algorithms.1 ## 1 4 2 2 ## 2 NA 5 5 ## 3 3 4 3 ## 4 3 3 4 ## 5 2 2 2 ## 6 5 5 5 We see that the columns are now of the appropriate type - but still the data frame does not fully fit the definition of “tidy data”. The names of the columns are actually the categories of the variable course and the ‘observation’ in this table is represented by the student, even though a more logical granularity would be to have each grade as a specific observation. Now, adding a new grade from a course is only possible by adding a new column, making sure to add grades for all students, which would inevitably mean immediately entering a lot of NA values for all student/course combinations where no grade yet exists (and maybe never will, if the student never takes that course at all). Let’s try to reorganize the table so each row represents “a grade student received in a particular course”. Consider what steps should be taken to create such a table. We need to: create a categorical variable course which would have the names of all the courses (currently spread over column names) create all student/course combinations that make sense (i.e. have a grade assigned to them) fill in the combinations with the corresponding grade value This process is not impossible, but it does require a lot of jumping through hoops to redesign the data frame. To simplify this process, we can use the gather function from thetidyr package, which performs the exact procedure we described above: it “gathers” columns into a single variable and then populates the valuesof that variable with the existing column/row combinations. The function signature looks like this: gather(data, key, value, ..., na.rm = FALSE, convert = FALSE, factor_key = FALSE) You can get a detailed description of the function by calling ?gather, but here we will just briefly explain some of the parameters: data obviously represents our data frame key represents the name of the new column - the categorical variable we want to create (in our case course); this function does not require that the column name be quoted value represents the name of the new column (variable) which will hold values with values (in our case grade) ... represents the set of columns we “gather”; we can specify column names separated by commas (quotes are also not required), use the syntax first_column:last_column, or even just name the columns we do NOT want to gather by prefacing them with the - sign na.rm describes whether we want to omit observations with NA values convert will convert the data if we deem it necessary factor_key asks us if we want to factorize the new variable we are creating Let’s try out this function with our untidy data frame. Assignment 9.7 - gather function # create a &#39;grades&#39; data frame by using the &#39;gather&#39; function on the &#39;students&#39; data frame # briefly explore the `grades` data frame #library(tidyr) # if necessary grades &lt;- gather(students, course, grade, Math.1:Algorithms.1, na.rm = T, factor_key = T) str(grades) head(grades) ## &#39;data.frame&#39;: 168 obs. of 5 variables: ## $ JMBAG : int 1341 1342 1343 1344 1348 1349 1350 1351 1352 1353 ... ## $ Surname: chr &quot;Anic&quot; &quot;Babic&quot; &quot;Crnoja&quot; &quot;Crnjac&quot; ... ## $ Name : chr &quot;Iva&quot; &quot;Josip&quot; &quot;Petra&quot; &quot;Lucija&quot; ... ## $ course : Factor w/ 7 levels &quot;Math.1&quot;,&quot;Physics.1&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... ## $ grade : int 2 5 4 2 3 3 4 2 4 5 ... ## JMBAG Surname Name course grade ## 1 1341 Anic Iva Math.1 2 ## 2 1342 Babic Josip Math.1 5 ## 3 1343 Crnoja Petra Math.1 4 ## 4 1344 Crnjac Lucija Math.1 2 ## 8 1348 Grubiša Ivan Math.1 3 ## 9 1349 Gobac Davor Math.1 3 The spread function does the inverse job from the gather function. It will “expand” data from a combination of a categorical column and a corresponding “value” column so the categories become column names, and the values from the value column get appropriately “spread” over the corresponding columns. The function signature looks like this: spread(data, key, value, fill = NA, convert = FALSE, drop = TRUE, sep = NULL) Documentation of this function is easily retrieved by using ?spread, and some elements can already be easily identified by using the knowledge of how the gather function works. Some of the parameters which might be useful to us will be described bellow: fill describes what value to put in “non-existent” combinations after the spread is performed drop describes whether columns for non-existent categories should be worked (if we were spreading a factor column and not or levels were represented) sep allows us to have a column name which is not only a category value but a combination of the name of an category column we are spreading and the category level (with a defined separator) Let’s try to “reconstruct” the original students data frame with this command. Assignment 9.8 - spread function # &quot;spread&quot; the &#39;grades&#39; data frame to reconstruct the # original data frame # store the results in a `students2` data frame # compare `students` and` students2` data frames students2 &lt;- spread(grades, course, grade) head(students) head(students2) str(students) str(students2) ## JMBAG Surname Name Math.1 Physics.1 Programming Electronics ## 1 1341 Anic Iva 2 2 NA NA ## 2 1342 Babic Josip 5 3 4 3 ## 3 1343 Crnoja Petra 4 3 4 2 ## 4 1344 Crnjac Lucija 2 5 2 2 ## 5 1345 Dizla Stipe NA 4 3 5 ## 6 1346 Ermic Igor NA 3 NA 5 ## Digital.Logic Math.2 Algorithms.1 ## 1 4 2 2 ## 2 NA 5 5 ## 3 3 4 3 ## 4 3 3 4 ## 5 2 2 2 ## 6 5 5 5 ## JMBAG Surname Name Math.1 Physics.1 Programming Electronics ## 1 1341 Anic Iva 2 2 NA NA ## 2 1342 Babic Josip 5 3 4 3 ## 3 1343 Crnoja Petra 4 3 4 2 ## 4 1344 Crnjac Lucija 2 5 2 2 ## 5 1345 Dizla Stipe NA 4 3 5 ## 6 1346 Ermic Igor NA 3 NA 5 ## Digital.Logic Math.2 Algorithms.1 ## 1 4 2 2 ## 2 NA 5 5 ## 3 3 4 3 ## 4 3 3 4 ## 5 2 2 2 ## 6 5 5 5 ## &#39;data.frame&#39;: 27 obs. of 10 variables: ## $ JMBAG : int 1341 1342 1343 1344 1345 1346 1347 1348 1349 1350 ... ## $ Surname : chr &quot;Anic&quot; &quot;Babic&quot; &quot;Crnoja&quot; &quot;Crnjac&quot; ... ## $ Name : chr &quot;Iva&quot; &quot;Josip&quot; &quot;Petra&quot; &quot;Lucija&quot; ... ## $ Math.1 : int 2 5 4 2 NA NA NA 3 3 4 ... ## $ Physics.1 : int 2 3 3 5 4 3 3 4 2 2 ... ## $ Programming : int NA 4 4 2 3 NA 3 3 3 4 ... ## $ Electronics : int NA 3 2 2 5 5 3 4 2 2 ... ## $ Digital.Logic: int 4 NA 3 3 2 5 2 4 3 5 ... ## $ Math.2 : int 2 5 4 3 2 5 2 5 NA 4 ... ## $ Algorithms.1 : int 2 5 3 4 2 5 5 4 3 2 ... ## &#39;data.frame&#39;: 27 obs. of 10 variables: ## $ JMBAG : int 1341 1342 1343 1344 1345 1346 1347 1348 1349 1350 ... ## $ Surname : chr &quot;Anic&quot; &quot;Babic&quot; &quot;Crnoja&quot; &quot;Crnjac&quot; ... ## $ Name : chr &quot;Iva&quot; &quot;Josip&quot; &quot;Petra&quot; &quot;Lucija&quot; ... ## $ Math.1 : int 2 5 4 2 NA NA NA 3 3 4 ... ## $ Physics.1 : int 2 3 3 5 4 3 3 4 2 2 ... ## $ Programming : int NA 4 4 2 3 NA 3 3 3 4 ... ## $ Electronics : int NA 3 2 2 5 5 3 4 2 2 ... ## $ Digital.Logic: int 4 NA 3 3 2 5 2 4 3 5 ... ## $ Math.2 : int 2 5 4 3 2 5 2 5 NA 4 ... ## $ Algorithms.1 : int 2 5 3 4 2 5 5 4 3 2 ... In the previous example, we demonstrated the inverse functionality of the gather andspread functions, but our usage spread did not serve to tidy up the data, it just enabled us to revert to the original data frame. Let us now look at an example where spread is sed to actually make the data tidier. Let’s retrieve data from the cars.csv file that stores the technical characteristics of specific cars. Assignment 9.9 - cars data set #load the `cars.csv` file into a data frame called `cars` #briefly explore the `cars` data frame cars &lt;- read.csv(&quot;cars.csv&quot;, fileEncoding = &quot;UTF-8&quot;, stringsAsFactors = F) str(cars) head(cars) ## &#39;data.frame&#39;: 18 obs. of 3 variables: ## $ Car.model : chr &quot;Opel Astra&quot; &quot;Opel Astra&quot; &quot;Opel Astra&quot; &quot;Opel Astra&quot; ... ## $ Technical.Characteristic: chr &quot;Cylinders&quot; &quot;HP&quot; &quot;Length m&quot; &quot;Mass kg&quot; ... ## $ Value : num 4 125 4.27 1285 4 ... ## Car.model Technical.Characteristic Value ## 1 Opel Astra Cylinders 4.000 ## 2 Opel Astra HP 125.000 ## 3 Opel Astra Length m 4.267 ## 4 Opel Astra Mass kg 1285.000 ## 5 Audi A4 Cylinders 4.000 ## 6 Audi A4 HP 136.000 This table clearly violates the principles of tidy data which dictate that only one type of variable should be stored in a column - the technical characteristics of the car are placed in a unique column called Technical.Characteristic and in the value column holds very heterogenous values (we have mass in kilograms, length in meters, etc.). Try tidying up this data frame with the spread function. Assignment 9.10 - spread function (2) # create an `cars2` data frame which will have # the tidied data from the `cars` data frame # briefly explore the `cars2` dat frame cars2 &lt;- spread (cars, Technical.Characteristic, Value) str(cars2) head(cars2) ## &#39;data.frame&#39;: 5 obs. of 6 variables: ## $ Car.model: chr &quot;Audi A4&quot; &quot;Citroen C6&quot; &quot;Fiat 500L&quot; &quot;Opel Astra&quot; ... ## $ Cylinders: num 4 6 2 4 4 ## $ HP : num 136 215 103 125 110 ## $ Length m : num 4.7 NA NA 4.27 4.56 ## $ Mass kg : num 1470 1816 1260 1285 NA ## $ Valves : num NA NA NA NA 16 ## Car.model Cylinders HP Length m Mass kg Valves ## 1 Audi A4 4 136 4.703 1470 NA ## 2 Citroen C6 6 215 NA 1816 NA ## 3 Fiat 500L 2 103 NA 1260 NA ## 4 Opel Astra 4 125 4.267 1285 NA ## 5 Renault Grand Scenic 4 110 4.560 NA 16 The gather andspread commands are not only used for ‘messy’ data. They can be very useful when converting so-called “wide” data into “long” data. Let’s show this on the example of shopping cart data. Shopping cart data (or “consumer basket data”) is a record of items purchased by a customer during their visit to the store (be it a virtual store or a real retail outlet). If we write consumer basket information in a “wide” format, then we organize the information so that the columns represent individual items and the rows represent one successful “visit” to the story (or simply “invoice”). Here, a value of 1 means that the item ultimately ended up in the cart, while 0 means it was not present there (alternative way would be to use numbers over 1 to denote the exact quantity). This kind of representation is suitable for different types of analysis, but it is not economical - if the store has a large number of items on offer, data will often have long rows mostly filled with “zeros”. The “long” format on the other hand simply puts a combination of a shopping cart identifier (or account number) and the name (or code) of the item purchased in each row. Such a record will have significantly more rows, but it is much more suitable in cases where the number of items in the range is far greater than the number of items in the average basket. Assignment 9.11 - Consumer basket data # Load data from the `ConsumersBasket.csv` file into the &#39;invoices&#39; data frame # briefly explore the `invoioces` data frame invoices &lt;- read.csv(&quot;ConsumerBasket.csv&quot;, stringsAsFactors = F, encoding = &quot;UTF-8&quot;) str(invoices) head(invoices) ## &#39;data.frame&#39;: 104 obs. of 21 variables: ## $ invoiceID : int 15671 15672 15673 15674 15675 15676 15677 15678 15679 15680 ... ## $ Coca.cola.2l : int 0 1 1 0 1 0 0 0 0 0 ... ## $ Chips.150g : int 0 0 1 0 0 0 0 1 0 0 ... ## $ Nutella.400.g : int 0 1 0 0 1 0 0 0 0 0 ... ## $ Light.Beer : int 0 1 0 0 0 0 0 0 0 1 ... ## $ Dark.Beer : int 1 0 0 1 0 0 0 0 0 1 ... ## $ Fabric.Softener.1.5l: int 0 0 0 0 0 0 0 1 0 1 ... ## $ Water.2l : int 0 0 1 0 0 0 0 0 1 1 ... ## $ Oranges : int 0 0 0 1 1 0 0 0 1 1 ... ## $ Apples : int 1 0 0 1 0 0 0 0 0 0 ... ## $ Pineapples : int 0 0 1 0 0 0 0 1 1 1 ... ## $ Napkins : int 0 0 0 0 1 1 0 0 0 0 ... ## $ Patte : int 0 0 1 1 0 0 0 0 0 1 ... ## $ Ketchup : int 1 0 0 1 0 0 0 0 0 1 ... ## $ Mustard : int 1 0 0 0 0 0 0 1 0 0 ... ## $ Milk.0.5l : int 0 1 0 1 1 0 0 0 0 0 ... ## $ Sour.Cream : int 0 1 0 0 0 0 1 0 0 1 ... ## $ Feta.cheese : int 0 0 0 1 0 0 0 0 0 0 ... ## $ Sardines : int 0 0 0 0 0 0 0 0 0 1 ... ## $ Tuna.patte : int 1 1 1 0 0 1 0 0 0 0 ... ## $ Nescaffe : int 0 0 1 0 0 0 0 0 1 0 ... ## invoiceID Coca.cola.2l Chips.150g Nutella.400.g Light.Beer Dark.Beer ## 1 15671 0 0 0 0 1 ## 2 15672 1 0 1 1 0 ## 3 15673 1 1 0 0 0 ## 4 15674 0 0 0 0 1 ## 5 15675 1 0 1 0 0 ## 6 15676 0 0 0 0 0 ## Fabric.Softener.1.5l Water.2l Oranges Apples Pineapples Napkins Patte ## 1 0 0 0 1 0 0 0 ## 2 0 0 0 0 0 0 0 ## 3 0 1 0 0 1 0 1 ## 4 0 0 1 1 0 0 1 ## 5 0 0 1 0 0 1 0 ## 6 0 0 0 0 0 1 0 ## Ketchup Mustard Milk.0.5l Sour.Cream Feta.cheese Sardines Tuna.patte ## 1 1 1 0 0 0 0 1 ## 2 0 0 1 1 0 0 1 ## 3 0 0 0 0 0 0 1 ## 4 1 0 1 0 1 0 0 ## 5 0 0 1 0 0 0 0 ## 6 0 0 0 0 0 0 1 ## Nescaffe ## 1 0 ## 2 0 ## 3 1 ## 4 0 ## 5 0 ## 6 0 Assignment 9.12 - converting data frame to a ‘long’ format # convert `invoices` data frame to a &quot;long&quot; format # each row needs to have invoiceID and itemName # only bought items need to be present # name the new data frame `invoicesLong` # store the new data frame in a new CSV file # called &quot;ConsumerBasketLong.csv&quot; invoicesLong &lt;- gather(invoices, item, value, -invoiceID) invoicesLong &lt;- invoicesLong[invoicesLong$value != 0, 1:2] invoicesLong &lt;- invoicesLong[order(invoicesLong$invoiceID),] head(invoicesLong) write.csv(invoicesLong, file = &#39;ConsumerBasketLong.csv&#39;, row.names = F) ## invoiceID item ## 417 15671 Dark.Beer ## 833 15671 Apples ## 1249 15671 Ketchup ## 1353 15671 Mustard ## 1873 15671 Tuna.patte ## 2 15672 Coca.cola.2l Assignment 9.13 - converting data frame to a ‘wide’ format # try formatting the &quot;long&quot; format back to &quot;wide&quot; # store the new data frame in a new CSV file # called &quot;ConsumerBasketWide.csv&quot; invoicesWide &lt;- invoicesLong invoicesWide$value &lt;- 1 invoicesWide &lt;- spread(invoicesWide, item, value, fill = 0) write.csv(invoicesWide, file = &#39;ConsumerBasketWide.csv&#39;, row.names = F) 9.1.2 The separate and unite functions The tidyr package has a number of other useful features for efficient transformation and clean up of our data frames, and here we will address two more commonly used onesm called separate and unite. The separate function is useful when a column has “complex” values that we want to separate into two or more columns. Assignment 9.14 - Deparments data set # read data from the `departments.csv` file into the` departments` variable # briefly explore the `departments` data frame departments &lt;- read.csv(&quot;departments.csv&quot;, stringsAsFactors = F) str(departments) head(departments) ## &#39;data.frame&#39;: 28 obs. of 4 variables: ## $ Department: chr &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; ... ## $ Quarter : chr &quot;Q1-2015&quot; &quot;Q2-2015&quot; &quot;Q3-2015&quot; &quot;Q4-2015&quot; ... ## $ RevenuesKn: num 12416 224290 10644 191229 258697 ... ## $ ExpensesKn: num 23101 63886 35468 12249 61515 ... ## Department Quarter RevenuesKn ExpensesKn ## 1 A Q1-2015 12416.2 23100.5 ## 2 A Q2-2015 224290.1 63886.1 ## 3 A Q3-2015 10643.7 35467.8 ## 4 A Q4-2015 191229.3 12249.1 ## 5 A Q1-2016 258697.4 61514.6 ## 6 A Q2-2016 121865.3 46092.6 This table shows the revenues and expenditures of a company departments by quarter. Quarters are currently stored in a complex variable called Quarter consisting of identifiers of the annual quarter (Q1, Q2, Q3 or Q4) and year. For analysis purposes, it would probably be more convenient to break this down into two columns - Quarter (which would store only the quarter identifier) and Year. The tidyr package for this purpose offers a separate function with the following signature: separate(data, col, into, sep = &quot;[^[:alnum:]] +&quot;, remove = TRUE, convert = FALSE, extra = &quot;warn&quot;, fill = &quot;warn&quot;, ...) The complete documentation of the function can be viewed with the command ?separate while here we explain some important parameters: col - column to be separated (no quotes required) into - names of new columns (character vector is recommended) sep - value separator in original column, default value is actually a regular expression for “something that is not an alphanumeric character” remove - describes whether or not to remove the original column Let’s try to apply this function to the departments data frame. Assignment 9.15 - the separate function # Separate the `Quarter` column into the` Quarter` and `Year` columns while removing the original column # save the resulting data grame to the `departments2` variable # try to do everything within one command with the help of the `%&gt;%` operator # briefly explore the `departments2` data frame departments %&gt;% separate(Quarter, c(&quot;Quarter&quot;, &quot;Year&quot;), &quot;-&quot;) -&gt; departments2 str(departments2) head(departments2) ## &#39;data.frame&#39;: 28 obs. of 5 variables: ## $ Department: chr &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; ... ## $ Quarter : chr &quot;Q1&quot; &quot;Q2&quot; &quot;Q3&quot; &quot;Q4&quot; ... ## $ Year : chr &quot;2015&quot; &quot;2015&quot; &quot;2015&quot; &quot;2015&quot; ... ## $ RevenuesKn: num 12416 224290 10644 191229 258697 ... ## $ ExpensesKn: num 23101 63886 35468 12249 61515 ... ## Department Quarter Year RevenuesKn ExpensesKn ## 1 A Q1 2015 12416.2 23100.5 ## 2 A Q2 2015 224290.1 63886.1 ## 3 A Q3 2015 10643.7 35467.8 ## 4 A Q4 2015 191229.3 12249.1 ## 5 A Q1 2016 258697.4 61514.6 ## 6 A Q2 2016 121865.3 46092.6 Note that the Quarter andYear columns are actually categorical variables so it would probably be a good idea to factorize them if we are to use them in further analysis. The separate function is often used to disassemble dates (eg 2016-10-28 into the year, month and day), but in such situations it is recommended to use the lubridate package created precisely for easier date management. We will introduce this package in one of the following chapters. Finally, let’s learn the unite function, which is somewhat less commonly used and is actually an inverse of the separate function. The unite function signature is: unite(data, col, ..., sep = &quot;_&quot;, remove = TRUE) In this case, too, we can easily retrieve the documentation for unite, and will just briefly describe the parameters that potentially require additional explanation: col - new column name (quotes not required) ... - the names of the columns we merge - we don’t have to use quotation marks, and if there are many columns we can use the same syntax to select as with the gather function Let’s try using this function on the departments2 data frame. Assignment 9.16 - the unite function # merge the `Quarter` and` Year` columns from the `departments2` table into a unique` Quarter` column # remove the previous `Quarter` and `Year` columns # use `-` as a separator # save the result to the `departments3` variable # use the `%&gt;% &#39;operator to put everything in one line # compare `departments` and` departments3` data frames departments2 %&gt;% unite(Quarter, Quarter, Year, sep = &quot;-&quot;) -&gt; departments3 str(departments2) head(departments2) str(departments3) head(departments3) ## &#39;data.frame&#39;: 28 obs. of 5 variables: ## $ Department: chr &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; ... ## $ Quarter : chr &quot;Q1&quot; &quot;Q2&quot; &quot;Q3&quot; &quot;Q4&quot; ... ## $ Year : chr &quot;2015&quot; &quot;2015&quot; &quot;2015&quot; &quot;2015&quot; ... ## $ RevenuesKn: num 12416 224290 10644 191229 258697 ... ## $ ExpensesKn: num 23101 63886 35468 12249 61515 ... ## Department Quarter Year RevenuesKn ExpensesKn ## 1 A Q1 2015 12416.2 23100.5 ## 2 A Q2 2015 224290.1 63886.1 ## 3 A Q3 2015 10643.7 35467.8 ## 4 A Q4 2015 191229.3 12249.1 ## 5 A Q1 2016 258697.4 61514.6 ## 6 A Q2 2016 121865.3 46092.6 ## &#39;data.frame&#39;: 28 obs. of 4 variables: ## $ Department: chr &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; ... ## $ Quarter : chr &quot;Q1-2015&quot; &quot;Q2-2015&quot; &quot;Q3-2015&quot; &quot;Q4-2015&quot; ... ## $ RevenuesKn: num 12416 224290 10644 191229 258697 ... ## $ ExpensesKn: num 23101 63886 35468 12249 61515 ... ## Department Quarter RevenuesKn ExpensesKn ## 1 A Q1-2015 12416.2 23100.5 ## 2 A Q2-2015 224290.1 63886.1 ## 3 A Q3-2015 10643.7 35467.8 ## 4 A Q4-2015 191229.3 12249.1 ## 5 A Q1-2016 258697.4 61514.6 ## 6 A Q2-2016 121865.3 46092.6 Exercise Tasks Initialize the random number generator using the command set.seed (1234). Then, with the help of a single command and the `%&gt;% ’operator, perform the following: create 100,000 random numbers drawn from the normal distribution with arithmetic mean of 10000 and standard deviation of 1000 round the numbers to the first larger integer drop duplicates from the set sort the set in ascending order randomly select 100 elements from the set organize these 100 elements into a 10x10 matrix, arranged in rows calculate sums of rows of the matrix print the mean of line sums on the screen. The weather.csv file contains meteorological station meteorological data which measures the temperature, pressure, humidity and wind speed every hour (data are downloaded and adapted from the data set of theweatherData package available on CRAN). Do the following: load the file into the data frame and examine the loaded data (names, str, summary,head …) answer: is it a tidy dataset? Why? Take the appropriate steps to obtain a data frame that complies with the tidy data principle save the new data frame to the file called weatherClean.csv Programirajmo u R-u by Damir Pintar is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License.Based on a work at https://ratnip.github.io/FER_OPJR/ References "],
["dates.html", "10 Working with dates and character strings 10.1 Working with dates 10.2 Working with character strings Exercise Tasks", " 10 Working with dates and character strings 10.1 Working with dates Managing dates and timestamps is always a challenge when working with real-life datasets, since we have to take into account things like: different date and time format specifications different internal representations different time zones difference between mathematical and calendar understanding of time periods One of the more commonly used standards is the so-called “unix time” (or “POSIX time”), which counts as the number of seconds elapsed since midnight on January 1, 1970 UTC (Coordinated Universal Time). This is not an universal standard for all operations systems or applications; for example, Microsoft Excel has its own format where it counts the days since 1.1.1900 and then the number of hours, minutes and seconds elapsed since midnight. The R programming language has offers three main classes for date/time management: Date for dates POSIXct for compact representation of timestamps POSIXlt for “long” representation of timestamps (in list format) 10.1.1 The Date class We use the Date class when we want to note the date but not the exact time of an observation or event. This class does not have a constructor, and we most commonly create Date objects using the following functions: Sys.Date() which returns today’s date as.Date(&lt;date_string&gt;) which accepts a character string as a parameter The function as.Date() by default accepts dates in the following format: %Y-%m-%d. Here, %Y represents a four-digit year, while %m and %d are two-digit months and days. If we want to interpret a date that is written in another format then we need to add an additional parameter called format which will parametrically describe the format we are using (e.g. for 28/10/1978 the format parameter value should be %d/%m/%Y). Other format specifications can easily be viewed using the ?strptime command, although a much simpler approach entails using functions of thelubridate package (to be introduced later in this chapter). Assignment 10.1 - class Date # print out today&#39;s date # convert the following character strings to a `Date` type object and print the result on the screen: # &#39;1986-12-27&#39;, &#39;2016-31-05&#39;, &#39;17.10.2015 &#39;, &#39;01#01#2001&#39; # print out today&#39;s date Sys.Date() # convert the following character strings to a Date type object and print the result on the screen: # &#39;1986-12-27&#39;, &#39;2016-31-05&#39;, &#39;17.10.2015 &#39;, &#39;01#01#2001&#39; as.Date(&quot;27/12/1986&quot;) as.Date(&#39;2016-31-05&#39;, format = &#39;%Y-%d-%m&#39;) as.Date(&#39;17.10.2015&#39;, format = &#39;%d.%m.%Y&#39;) as.Date(&#39;01#01#2001&#39;, format = &#39;%d#%m#%Y&#39;) ## [1] &quot;2020-01-20&quot; ## [1] &quot;0027-12-19&quot; ## [1] &quot;2016-05-31&quot; ## [1] &quot;2015-10-17&quot; ## [1] &quot;2001-01-01&quot; Dates allow using simple arithmetic operations. We can add or substract days from a date by using operators + and - with integers, or we can calculate the difference between two dates with the operator -. *** Assignment 10.2 - date arithmetics # print out what date was 1000 days before today # add one day to 2/28/2015. and 2/28/2016 and print the result # print out how many days have passed since 1.1.2000. until today # print out what date was 1000 days before today Sys.Date() - 1000 # add one day to 2/28/2015. and 2/28/2016 and print the result as.Date(&#39;2015-02-28&#39;) + 1 as.Date(&#39;2016-02-28&#39;) + 1 # print out how many days have passed since 1.1.2000. until today Sys.Date() - as.Date(&#39;2000-01-01&#39;) ## [1] &quot;2017-04-25&quot; ## [1] &quot;2015-03-01&quot; ## [1] &quot;2016-02-29&quot; ## Time difference of 7324 days The last expression will actually result in an object of class difftime which is an object representation of a time interval. Printing this object uses so-called “automatic” unit selection (specifically, the units parameter is by defaykt set to auto) that will attempt to select the most appropriate time unit for printing. If we want to explicitly choose which time unit we want (seconds, minutes, hours, days or weeks), then it’s easier to forgo using the - operator and leverage the difftime() function directly. Specific time unit can then be chosen by setting the units parameter to seconds, minutes etc. Assignment 10.3 - function difftime # How many weeks passed between 1.3.2016. i 1.3.2015.? # use the `difftime` function # NOTE: You do not need to explicitly call the `as.Date` function, the `difftime()` function # will do it by itself if you submit the date in the default format # how many hours have passed between 1.3.2015. and today? # How many weeks passed between 1.3.2016. i 1.3.2015.? # use the `difftime` function # NOTE: You do not need to explicitly call the `as.Date` function, the `difftime()` function # will do it by itself if you submit the date in the default format difftime(&#39;2016-03-01&#39;, &#39;2015-03-01&#39;, units = &quot;weeks&quot;) # how many hours have passed between 1.3.2015. and today? difftime(Sys.Date(), &#39;2015-03-01&#39;, units = &quot;hours&quot;) ## Time difference of 52.28571 weeks ## Time difference of 42865 hours The difftime function actually works with timestamps as well, i.e. we don’t necessarily have to work at the date level, we can get information on finer granularity when it comes to time units. We will show this in more detail after we learn the class POSIXct. Also, if we only require a number (of seconds, hours, days, etc.), we can easily transform the difftime output into an integer using the as.numeric function. The R language also implements a special variant of the seq function for working with dates, which has the following signature: seq(from, to, by, length.out = NULL, along.with = NULL, ...) The parameters of this function are as follows: from - start date (required parameter) to - final date by - step of the sequence in days, or a character string which denotes the time interval such as \"7 days\", \"2 weeks\" etc. (for all possibilities see the documentation!) length.out - length of sequence along.with - vector whose length we take for reference (along.with(x) is the same as length.out = length(x)) Let’s try this function. Assignment 10.4 - function seq and dates # print the date sequence from 1/1/2010. to 1/1/2030. in 6-month increments # make a schedule for one round of cleaning the common areas of an apartment building # common areas must be cleaned every 3 weeks # each apartment must get their own date # apartments are described by the following data frame apartments &lt;- data.frame(appId = 1:10, surname = c(&quot;Ebert&quot;, &quot;Ladovac&quot;, &quot;Cerić&quot;, &quot;Dikla&quot;, &quot;Anic&quot;, &quot;Perić&quot;, &quot;Žužić&quot;, &quot;Babić&quot;, &quot;Ibiza&quot;, &quot;Radler&quot;)) # Add a `cleaningDate` column with one date for each apartment # in order of apartment numbers, starting today # print out the &#39;apartments&#39; data frame # print the date sequence from 1/1/2010. to 1/1/2030. in 6-month increments seq(as.Date(&#39;2010-01-01&#39;), as.Date(&#39;2030-01-01&#39;), by = &quot;6 months&quot;) # Add a `cleaningDate` column with one date for each apartment # in order of apartment numbers, starting today seq(Sys.Date(), by = &quot;3 weeks&quot;, along.with = apartments$appId) -&gt; apartments$cleaningDate # print out the &#39;apartments&#39; data frame apartments ## [1] &quot;2010-01-01&quot; &quot;2010-07-01&quot; &quot;2011-01-01&quot; &quot;2011-07-01&quot; &quot;2012-01-01&quot; ## [6] &quot;2012-07-01&quot; &quot;2013-01-01&quot; &quot;2013-07-01&quot; &quot;2014-01-01&quot; &quot;2014-07-01&quot; ## [11] &quot;2015-01-01&quot; &quot;2015-07-01&quot; &quot;2016-01-01&quot; &quot;2016-07-01&quot; &quot;2017-01-01&quot; ## [16] &quot;2017-07-01&quot; &quot;2018-01-01&quot; &quot;2018-07-01&quot; &quot;2019-01-01&quot; &quot;2019-07-01&quot; ## [21] &quot;2020-01-01&quot; &quot;2020-07-01&quot; &quot;2021-01-01&quot; &quot;2021-07-01&quot; &quot;2022-01-01&quot; ## [26] &quot;2022-07-01&quot; &quot;2023-01-01&quot; &quot;2023-07-01&quot; &quot;2024-01-01&quot; &quot;2024-07-01&quot; ## [31] &quot;2025-01-01&quot; &quot;2025-07-01&quot; &quot;2026-01-01&quot; &quot;2026-07-01&quot; &quot;2027-01-01&quot; ## [36] &quot;2027-07-01&quot; &quot;2028-01-01&quot; &quot;2028-07-01&quot; &quot;2029-01-01&quot; &quot;2029-07-01&quot; ## [41] &quot;2030-01-01&quot; ## appId surname cleaningDate ## 1 1 Ebert 2020-01-20 ## 2 2 Ladovac 2020-02-10 ## 3 3 Ceric 2020-03-02 ## 4 4 Dikla 2020-03-23 ## 5 5 Anic 2020-04-13 ## 6 6 Peric 2020-05-04 ## 7 7 Žužic 2020-05-25 ## 8 8 Babic 2020-06-15 ## 9 9 Ibiza 2020-07-06 ## 10 10 Radler 2020-07-27 10.1.2 ThePOSIXct andPOSIXlt classes The class POSIXct is when in addition to the date we also need to know the exact time for some observation or event. The usual way of creating an object of this class is using the following functions: Sys.time() which returns the current timestamp (using the timezone set by the operating system) as.POSIXct(&lt;timestamp_string&gt;) which uses a character string representing the date and time The function as.POSIXct() expects a timestamp character string that uses the following format: %Y-%m-%d %H:%M:%S. First three format specifications are identical to the date format specification, while %H, %M and %S represent two-digit hours, minutes and seconds (24-hour time format is assumed unless am or pm is noted). The as.POSIXct function can also accept an optional tz parameter which explicitly sets the timezone. To parse other timestamp formats, it is necessary - as with the class Date - to add a format parameter containing a specification on how to interpret the given character string. Again, for the list of all parameters we can refer to the the ?strptime command, although as with the Date class we will later learn it’s often easier to use one of the functions of the lubridate package. Assignment 10.5 - class POSIXct # print the current date and time # convert the following character strings to timestamps and print them on the screen: # &quot;2015-10-28 15:30:42&quot; # &quot;01-12-2001 14:30&quot; &lt;- timestamp read in New York, USA, EST time zone # print the current date and time Sys.time() # convert the following character strings to timestamps and print them on the screen: # &quot;2015-10-28 15:30:42&quot; # &quot;01-12-2001 14:30&quot; &lt;- timestamp read in New York, USA, EST time zone as.POSIXct(&quot;2015-10-28 15:30:42&quot;) as.POSIXct(&quot;01-12-2001 14:30&quot;, tz = &quot;EST&quot;, format = &quot;% d-% m-% Y% H:% M&quot;) ## [1] &quot;2020-01-20 15:33:14 CET&quot; ## [1] &quot;2015-10-28 15:30:42 CET&quot; ## [1] NA Time zone names are standardized (so-called “Olson time zones”) and are retrieved from the underlying operating system. We can explicitly retrieve them using the OlsonNames() function. Also, we can easily print the current platform’s time zone by using the Sys.timezone() function. Assignment 10.6 - time zones # print the current time zone # print out 10 randomly selected time zone names installed on the current platform # print the current time zone Sys.timezone() # print out 10 randomly selected time zone names installed on the current platform sample(OlsonNames(), 10) ## [1] &quot;Europe/Warsaw&quot; ## [1] &quot;Pacific/Kosrae&quot; &quot;America/Havana&quot; &quot;Asia/Aqtau&quot; ## [4] &quot;Africa/Timbuktu&quot; &quot;Israel&quot; &quot;America/Campo_Grande&quot; ## [7] &quot;Asia/Hong_Kong&quot; &quot;Pacific/Johnston&quot; &quot;Asia/Tashkent&quot; ## [10] &quot;Etc/GMT-2&quot; Timestamps can also use + and - operators (with integers as the second operand) to add or substract seconds from the timestamp. Additionally, we can subtract two timestamps to get the difference in seconds, or use the difftime function with the selected time unit value. Assignment 10.7 - timestamp arithmetics # print out what the time will be 1000 seconds from now # print out how many hours have passed since midnight 1/1/2015 # print out what the time will be 1000 seconds from now Sys.time() + 1000 # print out how many hours have passed since midnight 1/1/2015. by far difftime(Sys.time(), &quot;2015-01-01 00:00:00&quot;, units = &quot;hours&quot;) ## [1] &quot;2020-01-20 15:49:55 CET&quot; ## Time difference of 44295.55 hours The class POSIXlt is very similar to its compact relative POSIXct. We use the similarly fashioned as.POSIXlt function to create one, however the difference is that we end up with a list which allows us to easily extract certain parameters from a timestamp, such as number of seconds, minutes, day of the week, etc. We can easily see all the elements of the list if we create a POSIXlt object and then call the unclass function on it, which will convert it to an “ordinary” list. We can even go a step further - if we put this list through the unlist function, we get a simple, easily interpretable character vector as a result. Assignment 10.8 - class POSIXlt # convert the following character string to a timestamp of type `POSIXlt` # store the result in the variable `t_long` # &quot;01/05/2013 13:35&quot; # print just the number of hours of the timestamp `t_long` # then just the number of minutes # you can find this information in subelements called `hour` and `minute` # remove the class and list property of the variable `t_long` # and print it on the screen # convert the following character string to a timestamp of type `POSIXlt` # store the result in the variable `t_long` # &quot;01/05/2013 13:35&quot; t_long &lt;- as.POSIXlt(&quot;5/1/2013 1:35PM&quot;, format = &quot;%d.%m.%Y %H:%M&quot;) # print just the number of hours of the timestamp `t_long` # then just the number of minutes # you can find this information in subelements called `hour` and `minute` t_long$hour t_long$min # remove the class and list property of the variable `t_long` # and print it on the screen t_long %&gt;% unclass() %&gt;% unlist() ## [1] NA ## [1] NA ## sec min hour mday mon year wday yday isdst zone ## NA NA NA NA NA NA NA NA &quot;-1&quot; &quot;&quot; ## gmtoff ## NA 10.1.3 The lubridate package Although the R language has relatively good support for working with dates and timestamps, we can make managing them significantly more efficient by using a package called lubridate package. If we analyze data where the time component is very important, or manage data sets that use exotic ways of noting date and time, then we can greatly simplify and accelerate the analysis process by using the features provided by this package. One of the things that may be most helpful to developers who don’t like to write formatted date parsing specifications is the family of date parsing functions whose names match the overall “structure” of the record we want to parse. For example, the function called ymd can parse character strings in which the date is written in the order year-month-day. The function is “smart” enough to interpret the details of the record itself, such as delimiters, character fields, etc. If the record has a different day, month, and year layout, it is only necessary to arrange the letters appropriately in the function name. Assignment 10.9 - date parsing functions of the lubridate package # library(lubridate) # load if needed! # using the functions in the `lubridate` package # parse the following character strings into dates and print them on the screen # &quot;2016-07-31&quot; # &quot;2/28/1983.&quot; # &quot;07#31#1996&quot; # &quot;20010830&quot; # using the functions in the `lubridate` package # parse the following character strings into dates and print them on the screen # &quot;2016-07-31&quot; # &quot;2/28/1983.&quot; # &quot;07#31#1996&quot; # &quot;20010830&quot; ymd(&quot;7/31/2016&quot;) ## Warning: All formats failed to parse. No formats found. dmy(&quot;02/28/1983.&quot;) ## Warning: All formats failed to parse. No formats found. mdy(&quot;07#31#1996&quot;) ymd(&quot;20010830&quot;) ## [1] NA ## [1] NA ## [1] &quot;1996-07-31&quot; ## [1] &quot;2001-08-30&quot; The aforementioned approach can also be used for timestamps, we just add the underscore and then the “letters” for hours, minutes and/or seconds (e.g. ymd_hms). Assignment 10.10 - timestamp parsing functions of the lubridate package # using the functions in the `lubridate` package # parse the following character strings to timestamps and print them on the screen # &quot;5/17/1977 10:15 pm&quot; # &quot;20160429 05/10/17&quot; # using the functions in the `lubridate` package # parse the following character strings to timestamps and print them on the screen # &quot;5/17/1977 10:15 pm&quot; # &quot;20160429 05/10/17&quot; dmy_hm(&quot;5/17/1977 10:15 pm&quot;) ## Warning: All formats failed to parse. No formats found. ymd_hms(&quot;20160429 05/10/17&quot;) ## [1] NA ## [1] &quot;2016-04-29 05:10:17 UTC&quot; Note that these functions always sets UTC for the time zone. This was intentionally designed to motivate the use of a consistent time zone in the data set we are analyzing. If desired, we can set the time zone with the tz parameter during parsing. Similarly, with timestamps already initialized, we can manage time zones using the following functions force_tz - “enforces” the change to a new time zone while leaving the timestamp values the same with_tz - converts a timestamp into one that matches the requested time zone t &lt;- ymd_hms(&quot;20161129 05/10/17&quot;, tz = &quot;EST&quot;) t force_tz(t, tz = &quot;CET&quot;) with_tz(t, tz = &quot;CET&quot;) ## [1] &quot;2016-11-29 05:10:17 EST&quot; ## [1] &quot;2016-11-29 05:10:17 CET&quot; ## [1] &quot;2016-11-29 11:10:17 CET&quot; The lubridate package also makes it easy to extract date and time segments from timestamps with functions such asyear, week, month, etc. The same functions can also be used to change any of the timestamp components. Assignment 10.11 - extracting timestamp elements x &lt;- dmy_hms(&quot;7/19/1996 4:15:27 PM&quot;) ## Warning: All formats failed to parse. No formats found. # extract and print hours from the timestamp above, followed by minutes # set the year of the above timestamp to 2011, and the month to June # print timestamp `x` on the screen x &lt;- dmy_hms(&quot;7/19/1996 4:15:27 PM&quot;) ## Warning: All formats failed to parse. No formats found. # extract and print hours from the timestamp above, followed by minutes hour(x) minute(x) # set the year of the above timestamp to 2011, and the month to June year(x) &lt;- 2011 month(x) &lt;- 6 # print timestamp `x` on the screen x ## [1] NA ## [1] NA ## [1] NA See the lubridate documentation for a complete list of features. For the current date and time, lubridate offers alternatives to the Sys.Date() and Sys.time() functions, which are simply called today() and now(). Assignment 10.12 - functions today and now # print out tomorrow&#39;s date # print the timestamp which happened exactly an hour ago # print out tomorrow&#39;s date today() + 1 # print the timestamp which happened exactly an hour ago now() - 60 * 60 ## [1] &quot;2020-01-21&quot; ## [1] &quot;2020-01-20 14:33:16 CET&quot; We already mentioned that managing time-related data can become very complex, especially considering that time intervals can be expressed generically (e.g., “2 years”) or specifically (the span of two dates), and that mathematical and calendar ways of expressing time intervals do not necessarily match (e.g., “period of one year” can mathematically mean “the exact number of seconds in 365 days” but also the contextually dependent “period until the same date next year”). The lubridate package defines four options for defining time-related objects: instant - timestamp rounded to the second duration - “generically” defined interval in seconds period - similar to duration, but allows you to define durations that do not always last mathematically (e.g. “3 months”) interval - the time interval between two specific moments We have already met the “istants”, these are the timestamps we have already shown. To create durations and periods, we have intuitively defined functions that are named by English names for time units, with durations having the letter d as a prefix (from duration). Hence, we have the functions called minutes/dminutes, hours/dhours, weeks/dweeks etc. (note that there is no dmonths function, since we cannot unambiguously convert one month into seconds). Assignment 10.13 - durations and periods # print out objects which represent the duration and period of 3 weeks # use the variable `v` to store a period of 5 years, 3 months and 2 days # add the above period to today&#39;s date # print out objects which represent the duration and period of 3 weeks weeks(3) dweeks(3) # use the variable `v` to store a period of 5 years, 3 months and 2 days v &lt;- years(5) + months(3) + days(2) # add the above period to today&#39;s date today() + v ## [1] &quot;21d 0H 0M 0S&quot; ## [1] &quot;1814400s (~3 weeks)&quot; ## [1] &quot;2025-04-22&quot; Notice that the above expression would not be consistent if obtained “mathematically”. Finally, we can create an interval by using the interval function and providing the start and end timestamps, or by using theas.interval function and giving the duration/period and the start timestamp. We can also use the operator %--% with two timestamps as operands. Assignment 10.14 - intervals # create a variable `interval1` to store the interval # between 6 months before today and 6 months after today # create a variable `interval2` and store the interval from today # until the date that will happen in 4 months, 3 weeks and 2 days # create a variable `interval3` that will store the interval # between 1.5.2002. and 1.7.2002. # print out all three intervals # create a variable `interval1` to store the interval # between 6 months before today and 6 months after today interval1 &lt;- interval(today() - months(6), today() + months(6)) # create a variable `interval2` and store the interval from today # until the date that will happen in 4 months, 3 weeks and 2 days interval2 &lt;- as.interval(months(4) + weeks(3) + days(2), today()) # create a variable `interval3` that will store the interval # between 1.5.2002. and 1.7.2002. interval3 &lt;- dmy(&quot;1.5.2002&quot;) %--% dmy(&quot;1.7.2002&quot;) # print out all three intervals interval1 interval2 interval3 ## [1] 2019-07-20 UTC--2020-07-20 UTC ## [1] 2020-01-20 UTC--2020-06-12 UTC ## [1] 2002-05-01 UTC--2002-07-01 UTC When we have intervals defined we can then: check if a timestamp is within an interval with the help of the operator %within% check whether the intervals overlap by using the function int_overlaps() easily retrieve the start and end of intervals using the functions int_start() and int_end() “merge” two intervals with the help of the union function or find the intersection between them with the help of theintersect function other stuff we can learn by looking at the documentation Assignment 10.15 - working with intervals # check whether today is within the interval defined by the variable `interval1` # if `interval1` and` interval2` overlap # print out their intersection # check whether today is within the interval defined by the variable `interval1` today() %within% interval1 # if `interval1` and` interval2` overlap # print out their intersection if(int_overlaps(interval1, interval1)) intersect(interval1, interval2) ## [1] TRUE ## [1] 2020-01-20 UTC--2020-06-12 UTC In this section, we are introduced to some of the functionality offered by the base R’s date and time classes as well as some of the functionalities offered by the lubridate package. For more information, see the official documentation of the R language and the lubridate package, or the article called “Dates and Times Made Easy with lubridate” written by the author of the package Hadley Wickham, available at this link . 10.2 Working with character strings R has very good support for character strings. However, the functions offered by base R are a bit unintuitive and inconsistent when compared to similar functions offered by other programming languages commonly used for text analysis (such as Perl or Python). It is for these specific reasons that the stringr package emerged, offering a very popular alternative to existing character string functions. However, before getting acquainted with the features offered by this package, it is necessary to briefly address the general issues of managing character strings in data analysis as well as introduce a technology without which the implementation of character string analysis is almost unthinkable - “regular expressions”. 10.2.1 Text analysis and regular expressions Text analysis is an inevitable element of data analysis. Whether it’s simple category identification, searching for specific patterns, or something performing more complex tasks commonly known as text mining, it’s hard to imagine any meaningful data analysis that doesn’t at some point require knowledge of at least the basic methods of managing sets of character strings. Regardless of the complexity of character string analysis we want to perform, one technology is ubiquitous and universally applicable - regular expressions. This is a special “language” that we use to define “patterns” which we use to search or process textual information in various ways. A thorough review of regular expression technology is beyond the scope of this coursebook. Below, we will provide only a brief overview. If you have never encountered this technology before, we strongly recommend that you make the effort and master at least the basic concepts, preferably using one of many excellent web resources. One very short yet effective regular expression mini-courses can be found here . A regular expression is simply a string of characters that represents the pattern we are looking for within a text. Eg. the regular expression gram is contained in the character string Programming language R but is not in the character string Text analysis. We say we found a “match” with the first string but not with the second. This kind of regular expression is not too flexible - the true power of regular expressions lies in using special symbols which enable describing patterns in a more “generic” way. Let’s demonsrate this on a simple example. One common scenarios of using regular expressions is checking whether the user has entered an address that corresponds to the “general” form of the email address. One possibility is to simply use the expression @ as a regular expression we want to find a “match” with the email adress we are checking for validity. This can help with filtering out a certain number of invalid email adresses, but will also allow “addresses” such as @@@ and @23456. With a little “work” on the expression, you could come up with a slightly better solution, which may look something like this: \\w+@\\w+\\.\\w+ Although it looks like a series of random characters, with basic knowledge of regular expressions (and, in this case, using the “Perl” standard) we can interpret the above expression relatively easily. The character \\w stands for “letter, number or underscore”, the sign + means “1 or more”, etc. If you wanted to “transcribe” the above regular expression in a spoken language, it would be “one or more letters, digits or underscores’, followed by the sign @, then again one or more letters, digits or underscores, then a dot and then finally one or more letters, digits or underscores.” While this is not an overly sophisticated regular expression, it is still better than the first attempt. Further refinement is certainly possible, and although subsequent additions are increasingly making it harder to be easily interpreted by the human, we are also gaining more and more control over the formal definitions of what an email address should look like (for such specific uses, it is often worthwhile to check publicly available regular expression repositories where we can find complex but of high quality and carefully tested expressions that are easy enough to copy into our program code). We already stated that the above expression is written in so-called “Perl standard”. Unfortunately, today there isn’t a single standard for regular expressions. Most often used are so-called “POSIX standard” in two versions - BRE and ERE (Basic Regular Expressions and Extended Regular Expressions) which are pretty similar, except for the fact that BRE relies a bit on the \\ character and doesn’t recognize some of the “newer” symbols that ERE uses. Another popular standard is the already mentioned “Perl standard” which is a version of regular expressions implemented in the Perl programming language. Because Perl is one of the leading languages for text processing, this standard has become one of the most widely accepted methods of using regular expressions. In general, almost all popular programming languages have support for regular expressions, either embedded in the language or with the help of additional packages. R is one of the languages that already includes support for regular expressions in its base package. What’s more, R has built-in parallel support for the three most widely used standards mentioned above - POSIX ERE, POSIX BRE and Perl. POSIX ERE is the default setting, and with certain parameters we can easily “switch” to BRE (extended = FALSE) or Perl (perl = TRUE). In the following paragraphs, we will stick with the ERE standard, but it is also important to know that the above settings may be used if we already have previously constructed regular expressions that have been developed in another standard (and we do not want to be bothered by switching from one standard to another). The following table gives a brief overview of some of the more commonly used regular expression elements in the language R: Element Meaning abcd literal string “abcd” 1234 literal string “1234” \\\\d or [:digit:] or [0-9] any digit \\\\D or [:alpha:] or [A-Za-z] any letter [:alnum:] any alphanumeric character . any character \\\\. dot (full stop) [abc] only the characters listed [^abc] all characters except those listed * zero or more repetitions + one or more repetitions {n} exactly n repetitions {m, n} at least m, at most n repetitions ? optional character [:space:] or \\\\ s any blank (space, tab, new line) [:punct:] punctuation marks ^ ... $ a start and end markers (ab|cd) string “ab” or string “cd” Note that when using the special character \\ as a part of a regular expression we actually have to use the double character \\\\ (the first time to indicate to R that a special character follows, the second time to literally use it as part of the regular expression). The basic functions of the R language to work with character strings (and thus regular expressions) are, among other things, grep,grepl, regexrp,gregexrp, regmatches, sub, gsub etc. But since the stringr package offers a set of alternative functions with almost the same functionality but with far more intuitive names and more consistent signatures, we will focus on functions from that package, leaving the base functions to readers as an optional exercise to be learned from the official documentation. 10.2.2 The stringr package We have already stated that the stringr package actually reimplements to some extent the already existing functions of the language R, but in a more intuitive and consistent way. To be more precise, the functions of the stringr package have slightly reduced functionality, which is actually by design - one of the imperatives when designing this package was to identify the most commonly used functionalities for text analysis and focus on that primarily. Functionality that is “thrown out” concerns specific cases for which the developer will need to look for alternative solutions (often in the form of basic functions), but the benefit is in simpler, more intuitive features that are easier to learn and use effectively in the long-term effectively. Additional benefits of using the stringr package are: consistent treatment of factors as character strings consistent ordering of parameters, which is especially useful when used in conjunction with the operator %&gt;% We can start with some simpler functions for which we do not need regular expressions (we list simplified function signatures, for more precise definitions consult the package documentation): str_c(string1, string2, ...) - merge character strings, alternative to paste0 str_length(string) - returns the length of the character string str_sub(string, start, end) - returns a substring using start and end as letter indexes (negative index means “counting from the back”) str_sub(string, start, end) &lt;- string2 - modifies string by exchanging the defined substring with string2 (which does need not be the same length as the dropped substring!) str_trim(string) - returns a string with removed blanks from the beginning and end of a string Assignment 10.16 - basic functions for working with character strings string1 &lt;- &quot; This is an example&quot; string2 &lt;- &quot;of string concatenation! &quot; # using one line of instructions concatenate the above strings, # remove the blanks at the beginning and end of the string from the result, # then select the substring from 35th to 55th character # and print the final result on the screen string &lt;- &quot;R is overly complicated and not an easy language!&quot; # in the upper character string, replace all the characters # from 6th place (counted from the start) # to 14th (counting from the end) # with an empty string # print the string string1 &lt;- &quot; This is an example&quot; string2 &lt;- &quot;of string concatenation! &quot; # using one line of instructions concatenate the above strings, # remove the blanks at the beginning and end of the string from the result, # then select the substring from 35th to 55th character # and print the final result on the screen str_c(string1, string2) %&gt;% str_trim() %&gt;% str_sub(35, 55) string &lt;- &quot;R is overly complicated and not an easy language!&quot; # in the upper character string, replace all the characters # from 6th place (counted from the start) # to 14th (counting from the end) # with an empty string str_sub(string, 6, -14) &lt;- &quot;&quot; # print the string string ## [1] &quot;enation!&quot; ## [1] &quot;R is asy language!&quot; The str_c function also has a sep parameter if we want to paste the strings with a specific separator, and the collapse parameter which is NULL by default, but which can be used to merge elements of a character vector into a single string (with the value of the parameter used as a separator). Assignment 10.17 - merging character strings string1 &lt;- &quot;To merge&quot; string2 &lt;- &quot;these strings&quot; string3 &lt;- &quot;you need some space!&quot; # merge the above strings into a single string and print the result pieces &lt;- c(&quot;These&quot;, &quot;vector&quot;, &quot;elements&quot;, &quot;should&quot;, &quot;be&quot;, &quot;joined...&quot;) # merge the elements of the above vector into one string and print the result string1 &lt;- &quot;To merge&quot; string2 &lt;- &quot;these strings&quot; string3 &lt;- &quot;you need some space!&quot; # merge the above strings into a single string and print the result str_c(string1, string2, string3, sep = &quot; &quot;) pieces &lt;- c(&quot;A&quot;, &quot;these&quot;, &quot;arrays&quot;, &quot;are&quot;, &quot;elements&quot;, &quot;vectors ...&quot;) # merge the elements of the above vector into one string and print the result str_c(pieces, collapse = &quot; &quot;) ## [1] &quot;To merge these strings you need some space!&quot; ## [1] &quot;A these arrays are elements vectors ...&quot; Let’s look at some stringr functions that work with regular expressions: str_detect(string, pattern) - returns TRUE if string contains pattern, otherwiseFALSE str_extract(string, pattern) - returns a string of characters corresponding to the first occurrence of a pattern str_extract_all(string, pattern) - returns a list with all occurrences that match the pattern str_replace(string, pattern, replacement) - changes the first occurrence of a pattern with the replacement str_replace_all(string, pattern, replacement) - changes all occurrences of a pattern with the replacement All of these functions are vectorized, which means that they behave logically (ie, “parallelized”) when we use a character vector with multiple elements in place of a specific parameter. For example, if we give a vector of strings and a vector of replacements to the str_replace function, elements of the first vector and replacements will “pair up” in place of a given pattern. We can have other combinations, such as a vector of original elements and a vector of patterns, all three parameters as vectors with multiple elements etc. but every time the behavior of the function is consistent with the already learned principle of vectorization. Assignment 10.18 - functions and regular expressions addresses &lt;- c(&quot;pero.peric@fer.hr&quot;, &quot;iva.ivic@etfos.hr&quot;, &quot;ppetrovic@gmail.com&quot;, &quot;branko1987@yahoo.com&quot;, &quot;jaRULZ4EVR@gmail.nz&quot;, &quot;dperkovic@efzg.hr&quot;, &quot;lalaic1998@gmail.co.uk&quot;, &quot;perica.markic@fer.hr&quot;) # print the total number of mail addresses which belong to the `fer.hr` subdomain # print all addresses that contain at least one digit # list all addresses that have a vowel as the second character # print all unique email address subdomains # (subdomain part of the address is everything after behind the `@` character) # anonymize the addresses above: all character strings in front of &#39;@&#39; # should be replaced with random 6-digit numbers # print the total number of mail addresses which belong to the `fer.hr` subdomain str_detect(addresses, &quot;fer\\\\.hr&quot;)%&gt;% sum # print all addresses that contain at least one digit addresses[str_detect(addresses, &#39;[:digit:]&#39;)] # list all addresses that have a vowel as the second character str_detect(addresses, &quot;^. [aeiouAEIOU]&quot;) %&gt;% addresses[.] # or str_detect(addresses, &quot;^.(a|e|i|o|u|A|E|I|O|U)&quot;) %&gt;% `[`(addresses, .) # print all unique email address subdomains # (subdomain part of the address is everything after behind the `@` character) str_extract(addresses, &#39;@(.*)&#39;) %&gt;% str_sub(2) %&gt;% unique # anonymize the addresses above: all character strings in front of &#39;@&#39; # should be replaced with random 6-digit numbers sample(100000: 999999, length(addresses)) %&gt;% as.character %&gt;% str_replace(addresses, &#39;^[^@]*&#39;, .) ## [1] 2 ## [1] &quot;branko1987@yahoo.com&quot; &quot;jaRULZ4EVR@gmail.nz&quot; ## [3] &quot;lalaic1998@gmail.co.uk&quot; ## character(0) ## [1] &quot;fer.hr&quot; &quot;etfos.hr&quot; &quot;gmail.com&quot; &quot;yahoo.com&quot; &quot;gmail.nz&quot; ## [6] &quot;efzg.hr&quot; &quot;gmail.co.uk&quot; ## [1] &quot;755066@fer.hr&quot; &quot;477550@etfos.hr&quot; &quot;521135@gmail.com&quot; ## [4] &quot;733004@yahoo.com&quot; &quot;812154@gmail.nz&quot; &quot;609008@efzg.hr&quot; ## [7] &quot;371161@gmail.co.uk&quot; &quot;736284@fer.hr&quot; Finally, we learn one relatively useful function called str_split. This function splits the character string into a vector of character strings, depending on the given separator (which may be a space, some other chosen character(s), or even a regular expression), and is often used as a “more primitive” alternative to the read.csv andread.table functions when we want to parse the input “manually”, or - more commonly - to break a column of text into individual words for the purposes of text analysis. This function accepts a character string (or a collection of strings) as input, disassembles it according to the chosen separator and then returns the list of pieces as a result; if we disassemble only one string, we can easily translate the result into a vector using the unlist function. str_split(&quot;Example of str_split function&quot;, pattern = &quot;[:space:]&quot;) %&gt;% unlist ## [1] &quot;Example&quot; &quot;of&quot; &quot;str_split&quot; &quot;function&quot; We will now try to perform a very simple example of text analysis - figuring out which words happen most frequently in a chosen text. To do this we first need to read a text file into an R object. One of the simplest ways to achieve this is using the file function (which opens up the connection towards the textual file) and then the readLines function which will read a chosen number of lines and store them in a character vector. For smaller files we can also immediately read the entire file simply by omitting the number of lines we want to read. An example of using these two functions may look like this: con &lt;- file(&quot;textFile.txt&quot;, &quot;r&quot;) # r = &quot;read&quot; rows &lt;- readLines(con) # or readLines(con, n = 100) close(con) # closing the connection The next assignment will use two files HobbitChapterOne.txt - the text we want to analyse and stopwords.txt - file with frequent words which are “uninteresting” from the analysis point of view Assignment 10.19 - simple text analysis # store the text from `HobbitChapterOne.txt` into a variable called `hobbit` # and text from `stopwords.txt` into a variable called `stopwords` # perform the next steps: # - merge all text segments from `hobbit` into one long character strings # - remove all punctuation marks from the text # - change all text to &quot;lowercase&quot; (use `tolower`) # - split the text using spaces as a separator # - remove &quot;empty words&quot; (words of length 0) if such exist # - remove all words which are present in the `stopwords` variable too # - count the frequencies of the words # - print on screen 20 most frequent words # store the text from `HobbitChapterOne.txt` into a variable called `hobbit` # and text from `stopwords.txt` into a variable called `stopwords` con &lt;- file(&quot;HobbitChapterOne.txt&quot;) hobbit &lt;- readLines(con) close(con) con &lt;- file(&quot;stopwords.txt&quot;) stopwords &lt;- readLines(con) close(con) # perform the next steps: # - merge all text segments from `hobbit` into one long character strings # - remove all punctuation marks from the text # - change all text to &quot;lowercase&quot; (use `tolower`) # - split the text using spaces as a separator # - remove &quot;empty words&quot; (words of length 0) if such exist # - remove all words which are present in the `stopwords` variable too # - count the frequencies of the words # - print on screen 20 most frequent words hobbit %&gt;% str_c(collapse = &quot; &quot;) %&gt;% str_replace_all(&#39;[:punct:]&#39;, &#39;&#39;) %&gt;% tolower %&gt;% str_split(&#39;[:space:]&#39;) %&gt;% unlist -&gt; hobbit2 hobbit3 &lt;- hobbit2[!(hobbit2 %in% stopwords | nchar(hobbit2) == 0)] freq &lt;- table(hobbit3) %&gt;% sort(decreasing = T) freq[1:20] ## hobbit3 ## bilbo gandalf one thorin dwarves door baggins good hobbit ## 39 36 36 34 31 27 25 25 25 ## little long went know time away go old things ## 24 24 23 20 19 17 17 17 17 ## come morning ## 16 16 Exercise Tasks The following tasks relate to the data set stored in the CSV file crimeSample.csv, which is a sample from the record of criminal incidents in the City of Philadelphia (the original data set can be found at this link ). The original set of columns was reduced and 1000 incidents were randomly sampled from the set of all observations. Before solving problems, load the data into the crimes data box and familiarize yourself with the data set (str,head, etc.) Convert the timestamp column from character type to POSIXct type. Add the following columns to the data frame: Year,Month, Hour. Fill in the columns with the appropriate information from the timestamp. Answer the question: in what month does the most crime occur? Which hour of the day is “most dangerous” according to the data? Answer the question: What is the percentage of incidents where the incident description contains the word burglary '' or robbery ’’? (tip: convert the entire crime description column to lowercase using the tolower() function. Print any unique four-digit numbers you can find on street names where a criminal incident is recorded. Let’s Program in Ru by Damir Pintar is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License . Based on work at https://ratnip.github.io/FER_OPJR/ "],
["dplyr.html", "11 Managing data frames with dplyr package - 11.1 Dataset: Titanic 11.2 Creating a subset of observations with filter andslice 11.3 Creating a subset of columns with select 11.4 Creating new columns with mutate 11.5 Sample Dataset: Houston flights 11.6 Grouping and aggregation with group_by andsummarise 11.7 Merging data frames with join functions 11.8 Integrating dplyr with relational databases Exercises", " 11 Managing data frames with dplyr package - The dplyr package is one of the newer R language packages whose main function is to efficiently and easily manage data frames with a set of intuitively designed functions. At the time of writing this lesson, is at the top of the list of most popular packages despite strong “competition” in this area made up of extremely popular packages such as plyr, data.table and base, traditional methods for working with data frames. Either of these options is a good choice, and although the dplyr package is not the most efficient in terms of performance (by far the best performance is currently offered by thedata.table package), the advantage of this package is its intuitive, readable syntax that makes programming and working with data frames fast and easy, representing a great choice for developers who are familiar with R or simply want to focus on working with data through readable, easy-to-use program code. The specific advantages of the dplyr package are: simple syntax (similar to SQL but procedural) that uses the five main “verbs” to manipulate data frames and as such defines a standalone language-within-language of sorts higher efficiency compared to the methods offered by the basic package (initially dplyr was designed for greater programming efficiency, not necessarily for better performance, but in the meantime the implementation of certain routines in C also allowed for improved computational performance) integration with relational databases and big data sources (certain dplyr functions can be used directly on tables in the database by automatically translating those functions into SQL commands) The aforementioned basic “five verbs” offered by the dplyr package are as follows: filter - for filtering the dataset by rows select - for selecting individual columns arrange - for changing the order of the rows mutate - for creating new columns from existing ones summarise - for data aggregation In addition to these five basic verbs, we often use: group_by for grouping data within a dataset the join family of functions for merging data frames SQL language experts will easily notice the parallels between the SQL language and the stated dplyr functionality. The biggest difference is that SQL works “declaratively”, i.e. we have to follow the rules of building an SQL command that “does everything at once”, while in R using the functions of the dplyr package and the already familiar operator%&gt;%actions can be performed on datasets procedurally, with clear data processing flow going from left to right. Before we do a detailed look at the functionality of the dplyr package, let’s get acquainted with one of the two datasets we will be using during this lesson. 11.1 Dataset: Titanic We will choose one commonly used dataset - “Titanic Passenger Survival Dataset”. This dataset contains information about the passengers of the cruise shipTitanic which sank on 14 April 1912, whereupon only 706 out of 2223 passengers survived. This dataset contains, amongst other things, passengers’ names, gender, date of arrival, passenger class, etc. There is a version of this dataset that comes with R distribution itself, but we will use its extended version from a Kaggle competition “Titanic: Machine Learning From Disaster” which can be found more at this link. Let’s load this data set using the read_csv function, areadr package function that “upgrades” the read.csv function. Also, instead of the str function, let’s try to use its equivalent, the glimpse function, provided by the dplyr package. Assignment 11.1 - dataset Titanic # load the data set from the file `Titanic.csv` into the variable` # `titanic` using the `read_csv` function from the `readr` package # please read the documentation before using the function # view the structure of the `titanic` dataframe # using the `glimps` function # and check the first few lines with the `head` function # load the data set from the file `Titanic.csv` into the variable` # `titanic` using the `read_csv` function from the `readr` package # please read the documentation before using the function titanic &lt;- read_csv(&quot;Titanic.csv&quot;) # view the structure of the `titanic` dataframe # using the `glimps` function # and check the first few lines with the `head` function glimpse(titanic) head(titanic) ## Parsed with column specification: ## cols( ## PassengerId = col_integer(), ## Survived = col_integer(), ## Pclass = col_integer(), ## Name = col_character(), ## Sex = col_character(), ## Age = col_double(), ## SibSp = col_integer(), ## Parch = col_integer(), ## Ticket = col_character(), ## Fare = col_double(), ## Cabin = col_character(), ## Embarked = col_character() ## ) ## Observations: 891 ## Variables: 12 ## $ PassengerId &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,... ## $ Survived &lt;int&gt; 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,... ## $ Pclass &lt;int&gt; 3, 1, 3, 1, 3, 3, 1, 3, 3, 2, 3, 1, 3, 3, 3, 2, 3,... ## $ Name &lt;chr&gt; &quot;Braund, Mr. Owen Harris&quot;, &quot;Cumings, Mrs. John Bra... ## $ Sex &lt;chr&gt; &quot;male&quot;, &quot;female&quot;, &quot;female&quot;, &quot;female&quot;, &quot;male&quot;, &quot;mal... ## $ Age &lt;dbl&gt; 22, 38, 26, 35, 35, NA, 54, 2, 27, 14, 4, 58, 20, ... ## $ SibSp &lt;int&gt; 1, 1, 0, 1, 0, 0, 0, 3, 0, 1, 1, 0, 0, 1, 0, 0, 4,... ## $ Parch &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0, 0, 5, 0, 0, 1,... ## $ Ticket &lt;chr&gt; &quot;A/5 21171&quot;, &quot;PC 17599&quot;, &quot;STON/O2. 3101282&quot;, &quot;1138... ## $ Fare &lt;dbl&gt; 7.2500, 71.2833, 7.9250, 53.1000, 8.0500, 8.4583, ... ## $ Cabin &lt;chr&gt; NA, &quot;C85&quot;, NA, &quot;C123&quot;, NA, NA, &quot;E46&quot;, NA, NA, NA, ... ## $ Embarked &lt;chr&gt; &quot;S&quot;, &quot;C&quot;, &quot;S&quot;, &quot;S&quot;, &quot;S&quot;, &quot;Q&quot;, &quot;S&quot;, &quot;S&quot;, &quot;S&quot;, &quot;C&quot;, ... ## # A tibble: 6 x 12 ## PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 0 3 Brau~ male 22 1 0 A/5 2~ 7.25 ## 2 2 1 1 Cumi~ fema~ 38 1 0 PC 17~ 71.3 ## 3 3 1 3 Heik~ fema~ 26 0 0 STON/~ 7.92 ## 4 4 1 1 Futr~ fema~ 35 1 0 113803 53.1 ## 5 5 0 3 Alle~ male 35 0 0 373450 8.05 ## 6 6 0 3 Mora~ male NA 0 0 330877 8.46 ## # ... with 2 more variables: Cabin &lt;chr&gt;, Embarked &lt;chr&gt; Why use the function read_csv instead of its counterpart with a similar name, read.csv? There are several reasons: greater autonomy, i.e. better inference of column types faster processing speed provides status information for easier error detection no automatic categorization the loaded object is automatically converted into a “tibble” All these reasons are clear, except perhaps the last one. What is a “tibble”? This is a colloquial name for an object of class tbl_df, which is an upgrade of sorts of the data.frame class, i.e. the basic data frame. The biggest advantage of this type of object is that when you try to print the whole data frame to the screen (which usually happens by mistake and which can result in a “freezing” of the R console), a special function will be called that will print only part of the frame and be able to resume operation immediately . If we want to “upgrade” an existing dataframe to a tibble, we can do this with the as_tibble command (note that functions in the tidyverse collection often contain an underscore _, to help distinguish them as upgrades of basic R constructs which have similar names but use a dot .). We can fine tune how tibbles get printed by using the options function and the parameters tibble.print_max and `tibble.width, for example: # I want to see a maximum of 10 rows and always print all columns options(tibble.print_max = 10, tibble.width = Inf) An easier way for a one-off printing of more than 10 rows we can simply add a paramter n to the default print function: # `df` is of class `tbl_df`, I want to see 50 lines printed print(df, n = 50) Before proceeding, it would be a good idea to get acquainted with the data set we will be using, either through a more detailed research of the dataset, or by collecting documentation on the dataset. The brief description of the dataset itself, collected from the official Keggle page of the competition, is as follows: VARIABLE DESCRIPTIONS: survival Survival (0 = No; 1 = Yes) pclass Passenger Class (1 = 1st; 2 = 2nd; 3 = 3rd) name Name sex Sex age Age sibsp Number of Siblings/Spouses Aboard parch Number of Parents/Children Aboard ticket Ticket Number fare Passenger Fare cabin Cabin embarked Port of Embarkation (C = Cherbourg; Q = Queenstown; S = Southampton) SPECIAL NOTES: Pclass is a proxy for socio-economic status (SES) 1st ~ Upper; 2nd ~ Middle; 3rd ~ Lower Age is in Years; Fractional if Age less than One (1) If the Age is Estimated, it is in the form xx.5 With respect to the family relation variables (i.e. sibsp and parch) some relations were ignored. The following are the definitions used for sibsp and parch. Sibling: Brother, Sister, Stepbrother, or Stepsister of Passenger Aboard Titanic Spouse: Husband or Wife of Passenger Aboard Titanic (Mistresses and Fiances Ignored) Parent: Mother or Father of Passenger Aboard Titanic Child: Son, Daughter, Stepson, or Stepdaughter of Passenger Aboard Titanic Other family relatives excluded from this study include cousins, nephews/nieces, aunts/uncles, and in-laws. Some children travelled only with a nanny, therefore parch=0 for them. As well, some travelled with very close friends or neighbors in a village, however, the definitions do not support such relations. At this point, it should be considered whether the dataset includes some categorical data. As we have learned, unlike the read.csv function that automatically factorizes all character columns (which is not recommended), the read_csv function from the readr package does not factorize anything, but rather leaves that to the analyst. While this represents additional work for the analyst, the level of control and robustness that is obtained is more than a sufficient compromise. In the titanic dataset we notice the following categorical variables: Survival (survival - 2 categories: 0 and 1) Pclass (passenger class - 3 categories: 1, 2 and 3) Sex (gender - 2 categories: “M” and “F”) Embarked (port of embarkation - 3 categories: “C”, “Q” and “S”) Let’s factorize the columns listed. Assignment 11.2 - factorizing the columns of the Titanic dataset # convert the `Survival`,` Pclass`, `Sex` and` Embarked` columns # of the `titanic` data frame into factors # briefly explore the `titanic` data frame # using the &#39;glimpse&#39; function # convert the `Survival`,` Pclass`, `Sex` and` Embarked` columns # of the `titanic` data frame into factors titanic$Survived &lt;- as.factor(titanic$Survived) titanic$Pclass &lt;- as.factor(titanic$Pclass) titanic$Sex &lt;- as.factor(titanic$Sex) titanic$Embarked &lt;- as.factor(titanic$Embarked) # briefly explore the `titanic` data frame # using the &#39;glimpse&#39; function glimpse(titanic) ## Observations: 891 ## Variables: 12 ## $ PassengerId &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,... ## $ Survived &lt;fct&gt; 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,... ## $ Pclass &lt;fct&gt; 3, 1, 3, 1, 3, 3, 1, 3, 3, 2, 3, 1, 3, 3, 3, 2, 3,... ## $ Name &lt;chr&gt; &quot;Braund, Mr. Owen Harris&quot;, &quot;Cumings, Mrs. John Bra... ## $ Sex &lt;fct&gt; male, female, female, female, male, male, male, ma... ## $ Age &lt;dbl&gt; 22, 38, 26, 35, 35, NA, 54, 2, 27, 14, 4, 58, 20, ... ## $ SibSp &lt;int&gt; 1, 1, 0, 1, 0, 0, 0, 3, 0, 1, 1, 0, 0, 1, 0, 0, 4,... ## $ Parch &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0, 0, 5, 0, 0, 1,... ## $ Ticket &lt;chr&gt; &quot;A/5 21171&quot;, &quot;PC 17599&quot;, &quot;STON/O2. 3101282&quot;, &quot;1138... ## $ Fare &lt;dbl&gt; 7.2500, 71.2833, 7.9250, 53.1000, 8.0500, 8.4583, ... ## $ Cabin &lt;chr&gt; NA, &quot;C85&quot;, NA, &quot;C123&quot;, NA, NA, &quot;E46&quot;, NA, NA, NA, ... ## $ Embarked &lt;fct&gt; S, C, S, S, S, Q, S, S, S, C, S, S, S, S, S, S, Q,... There is also a more concise way to factorizing columns which leverages the lapply function: categories &lt;- c(&quot;Survived&quot;, &quot;Pclass&quot;, &quot;Sex&quot;, &quot;Embarked&quot;) titanic[categories] &lt;- lapply(titanic[categories], as.factor) Now that we have a good understanding of our data set and have made the initial preparations in terms of column categorization, we can begin with the introduction of dplyr package functions. 11.2 Creating a subset of observations with filter andslice In the chapter on data frames, we have already learned that “slicing” data frames can be done similar to slicing matrices - using index vectors to define which rows/columns are retained. We have also learned that index vectors can be numerical (location), logical, and character-typed. When defining a subset of rows, by far the most common type of index vector is logical - with the help of variables, i.e. data frame columns, we define a specific criterion that “filters” the rows. Unfortunately, the basic R syntax of using logical index vectors to determine a subset of rows is somewhat clumsy, as can be seen from this example: df[df$a &gt; 5 &amp; df$b != 3,] The first and obvious issue is the need to repeat the name of the data frame (which we can eliminate with the help of the attach function, which we said was not an ideal solution because it brings a number of new potential problems). The second issue is the readability problem - the above command is not easy to interpret visually, that is, it is not easy to see immediately afte that it is an instruction which aims to reduce the number of rows in a data frame. The alternative is to use the filter function from the package dplyr which uses explicitely indicates that it is filtering rows, and also allows the use of column names without having to reference the name of the data frame: filter(df, a &gt; 5 &amp; b != 3) Also, it is good to note that the first argument of the function is the data frame itself, which allows us to easily chain it. Most of the dplyr package functions are designed on this principle. The above function is the most common way to select a subset of rows (readers who are acquainted with the SQL language will notice a similarity to the WHERE segment of SQL queries). In addition to the filter function, to specify a subset of rows we want to keep we can also use some of the following functions, which also have very intuitive names (for easier interpretation rather than signature functions, we give examples of parameters): distinct(df) - for removing duplicates slice(df, 1:10) - for location indexing sample_frac(df, 0.2) - random selection of a certain percentage of rows sample_n(df, 50) - randomly select of a specific number of rows top_n(df, 10, a) - returns the first 10 rows, ordered by the values of attribute a We can use the following to rearrange the rows in the result: arrange(df, a, desc(b)) - sort by column a in ascending order and then by b in descending order Let’s try this in the following examples: Assignment 11.3 - row selection # print information about all first class passengers over 60 years of age # print information about all surviving male travelers who # have `George` or` Frank` in their name # check whether the dataset contains duplicate observations # randomly select and print information about five passengers who # have not survived the sinking # print order in descending order of ticket price # print information about the five oldest first class passengers # arrange the result in ascending order of age # print information about all first class passengers over 60 years of age filter(titanic, Pclass == 1 &amp; Age&gt; 60) ## # A tibble: 14 x 12 ## PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare ## &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;chr&gt; &lt;fct&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 55 0 1 Ostb~ male 65 0 1 113509 62.0 ## 2 97 0 1 Gold~ male 71 0 0 PC 17~ 34.7 ## 3 171 0 1 Van ~ male 61 0 0 111240 33.5 ## 4 253 0 1 Stea~ male 62 0 0 113514 26.6 ## 5 276 1 1 Andr~ fema~ 63 1 0 13502 78.0 ## 6 439 0 1 Fort~ male 64 1 4 19950 263 ## 7 457 0 1 Mill~ male 65 0 0 13509 26.6 ## 8 494 0 1 Arta~ male 71 0 0 PC 17~ 49.5 ## 9 546 0 1 Nich~ male 64 0 0 693 26 ## 10 556 0 1 Wrig~ male 62 0 0 113807 26.6 ## 11 626 0 1 Sutt~ male 61 0 0 36963 32.3 ## 12 631 1 1 Bark~ male 80 0 0 27042 30 ## 13 746 0 1 Cros~ male 70 1 1 WE/P ~ 71 ## 14 830 1 1 Ston~ fema~ 62 0 0 113572 80 ## # ... with 2 more variables: Cabin &lt;chr&gt;, Embarked &lt;fct&gt; # print information about all surviving male travelers who # have `George` or` Frank` in their name filter(titanic, str_detect(Name, &quot;(George|Frank)&quot;) &amp; Sex == &quot;male&quot; &amp; Survived == 1) ## # A tibble: 6 x 12 ## PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare ## &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;chr&gt; &lt;fct&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 166 1 3 &quot;Gol~ male 9 0 2 363291 20.5 ## 2 371 1 1 Hard~ male 25 1 0 11765 55.4 ## 3 508 1 1 &quot;Bra~ male NA 0 0 111427 26.6 ## 4 571 1 2 Harr~ male 62 0 0 S.W./~ 10.5 ## 5 710 1 3 &quot;Mou~ male NA 1 1 2661 15.2 ## 6 832 1 2 Rich~ male 0.83 1 1 29106 18.8 ## # ... with 2 more variables: Cabin &lt;chr&gt;, Embarked &lt;fct&gt; # check whether the dataset contains duplicate observations nrow(titanic) == nrow(distinct(titanic)) ## [1] TRUE # randomly select and print information about five passengers who # have not survived the sinking # print order in descending order of ticket price filter(titanic, Survived == 0) %&gt;% sample_n(5) %&gt;% arrange(desc(Fare)) ## # A tibble: 5 x 12 ## PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare ## &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;chr&gt; &lt;fct&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 496 0 3 Yous~ male NA 0 0 2627 14.5 ## 2 464 0 2 Mill~ male 48 0 0 234360 13 ## 3 869 0 3 van ~ male NA 0 0 345777 9.5 ## 4 92 0 3 Andr~ male 20 0 0 347466 7.85 ## 5 61 0 3 Sira~ male 22 0 0 2669 7.23 ## # ... with 2 more variables: Cabin &lt;chr&gt;, Embarked &lt;fct&gt; # print information about the five oldest first class passengers # arrange the result in ascending order of age filter(titanic, Pclass == 1 &amp; Sex == &quot;female&quot;) %&gt;% top_n(5, Age) %&gt;% arrange(Age) ## # A tibble: 6 x 12 ## PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare ## &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;chr&gt; &lt;fct&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 12 1 1 Bonn~ fema~ 58 0 0 113783 26.6 ## 2 196 1 1 Lure~ fema~ 58 0 0 PC 17~ 147. ## 3 269 1 1 Grah~ fema~ 58 0 1 PC 17~ 153. ## 4 367 1 1 Warr~ fema~ 60 1 0 110813 75.2 ## 5 830 1 1 Ston~ fema~ 62 0 0 113572 80 ## 6 276 1 1 Andr~ fema~ 63 1 0 13502 78.0 ## # ... with 2 more variables: Cabin &lt;chr&gt;, Embarked &lt;fct&gt; Notice that the top_n function returns the “first n” rows but does not necessarily arrange them in that order. If we want them arranged we also need to use the arrange function. 11.3 Creating a subset of columns with select Another method of slixing a data frame selecting a subset of columns. Unlike selecting a subset of rows, where we most often use logical indexing, columns or variables are most often referenced by their name. The syntax for selecting a subset of columns by name using the basic indexing method in R looks like this: df[, c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;)] Here we also notice a certain clumsiness in the syntax and difficulty in interpretation. Column names must be wrapped in a c function, which reduces readability, and the command doesn’t explicitly state that it is selecting columns of the data frame, we have to conclude it from the position of the index vector. In addition, there is no easy way to select a range of columns by name, the existence of a substring or pattern within the name, etc. The select function allows us to explicitly select columns using syntax: select(df, a, b, c) So we simply list the data frame and the row of columns we want to select. It is also notice the similarity to SQL, specifically the SELECT part of SQL queries. But the above syntax is not all that this feature has to offer - select has a number of helper functions and operators that greatly extend its functionality, such as: select(df, a:c) - select columns from a toc select(df, -a, -b) - select all columns except a andb select(df, starts_with(\"PO\"))) - select columns beginning with the letters \"PO\" select(df, contains(\"col\")) - select columns containing the letters\"col\" select(df, matches(\"[123]{2,3}\")) - select columns that match the given regular expression Additional options are easy to find in the official documentation. Let’s try this command, also on our dataset. Assignment 11.4 - selecting the columns # for randomly selected 10 rows, print the passenger&#39;s name, age and # whether he survived the sinking or not # for the 10 oldest passengers, print all attributes from name to ticket price # print all attributes except for the identifier and cabin number # for a randomly selected 1% of rows # for rows from number 10 to number 20, print out all columns beginning with a vowel # for randomly selected 10 passengers whose age is unknown, print all # attributes from name to ticket price, then the passenger class # and finally whether or not the passenger survived # sort the rows alphabetically by name # for randomly selected 10 rows, print the passenger&#39;s name, age and # whether he survived the sinking or not sample_n(titanic, 10) %&gt;% select(Name, Age, Survived) ## # A tibble: 10 x 3 ## Name Age Survived ## &lt;chr&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 Panula, Mrs. Juha (Maria Emilia Ojala) 41 0 ## 2 Taussig, Miss. Ruth 18 1 ## 3 Sirayanian, Mr. Orsen 22 0 ## 4 Lemore, Mrs. (Amelia Milley) 34 1 ## 5 Hamalainen, Master. Viljo 0.67 1 ## 6 Sandstrom, Miss. Marguerite Rut 4 1 ## 7 Baclini, Miss. Marie Catherine 5 1 ## 8 Stead, Mr. William Thomas 62 0 ## 9 Penasco y Castellana, Mrs. Victor de Satode (Maria Josef~ 17 1 ## 10 Hassan, Mr. Houssein G N 11 0 # for the 10 oldest passengers, print all attributes from name to ticket price top_n(titanic, 10, Age) %&gt;% select(Name:Fare) ## # A tibble: 11 x 7 ## Name Sex Age SibSp Parch Ticket Fare ## &lt;chr&gt; &lt;fct&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Wheadon, Mr. Edward H male 66 0 0 C.A. 245~ 10.5 ## 2 Ostby, Mr. Engelhart Cornelius male 65 0 1 113509 62.0 ## 3 Goldschmidt, Mr. George B male 71 0 0 PC 17754 34.7 ## 4 Connors, Mr. Patrick male 70.5 0 0 370369 7.75 ## 5 Duane, Mr. Frank male 65 0 0 336439 7.75 ## 6 Millet, Mr. Francis Davis male 65 0 0 13509 26.6 ## 7 Artagaveytia, Mr. Ramon male 71 0 0 PC 17609 49.5 ## 8 Barkworth, Mr. Algernon Henry W~ male 80 0 0 27042 30 ## 9 Mitchell, Mr. Henry Michael male 70 0 0 C.A. 245~ 10.5 ## 10 Crosby, Capt. Edward Gifford male 70 1 1 WE/P 5735 71 ## 11 Svensson, Mr. Johan male 74 0 0 347060 7.78 # print all attributes except for the identifier and cabin number # for a randomly selected 1% of rows sample_frac(titanic, 0.01) %&gt;% select(-PassengerId, -Cabin) ## # A tibble: 9 x 10 ## Survived Pclass Name Sex Age SibSp Parch Ticket Fare Embarked ## &lt;fct&gt; &lt;fct&gt; &lt;chr&gt; &lt;fct&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 0 3 Thorneycro~ male NA 1 0 376564 16.1 S ## 2 1 3 Nilsson, M~ fema~ 26 0 0 347470 7.85 S ## 3 1 2 Faunthorpe~ fema~ 29 1 0 2926 26 S ## 4 1 3 Jansson, M~ male 21 0 0 350034 7.80 S ## 5 0 2 Eitemiller~ male 23 0 0 29751 13 S ## 6 0 3 Sdycoff, M~ male NA 0 0 349222 7.90 S ## 7 1 2 Davies, Ma~ male 8 1 1 C.A. ~ 36.8 S ## 8 0 3 Reed, Mr. ~ male NA 0 0 362316 7.25 S ## 9 1 2 Leitch, Mi~ fema~ NA 0 0 248727 33 S # for rows from number 10 to number 20, print out all columns beginning with a vowel slice(titanic, 10:20) %&gt;% select(matches(&quot;^[AEIOUaeiou]&quot;)) ## # A tibble: 11 x 2 ## Age Embarked ## &lt;dbl&gt; &lt;fct&gt; ## 1 14 C ## 2 4 S ## 3 58 S ## 4 20 S ## 5 39 S ## 6 14 S ## 7 55 S ## 8 2 Q ## 9 NA S ## 10 31 S ## 11 NA C # for randomly selected 10 passengers whose age is unknown, print all # attributes from name to ticket price, then the passenger class # and finally whether or not the passenger survived # sort the rows alphabetically by name filter(titanic, is.na(Age)) %&gt;% sample_n(10) %&gt;% select(Name:Fare, Pclass, Survived) %&gt;% arrange(Name) ## # A tibble: 10 x 9 ## Name Sex Age SibSp Parch Ticket Fare Pclass Survived ## &lt;chr&gt; &lt;fct&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; ## 1 Boulos, Mr. Hanna male NA 0 0 2664 7.22 3 0 ## 2 Klaber, Mr. Herman male NA 0 0 113028 26.6 1 0 ## 3 Lennon, Mr. Denis male NA 1 0 370371 15.5 3 0 ## 4 &quot;O&#39;Dwyer, Miss. E~ female NA 0 0 330959 7.88 3 1 ## 5 Peter, Miss. Anna female NA 1 1 2668 22.4 3 1 ## 6 Peters, Miss. Kat~ female NA 0 0 330935 8.14 3 0 ## 7 Plotcharsky, Mr. ~ male NA 0 0 349227 7.90 3 0 ## 8 Razi, Mr. Raihed male NA 0 0 2629 7.23 3 0 ## 9 Sage, Mr. Frederi~ male NA 8 2 CA. 2~ 69.6 3 0 ## 10 Zabour, Miss. Tha~ female NA 1 0 2665 14.5 3 0 11.4 Creating new columns with mutate When working with datasets, there is often a need to create additional variables with the help of information stored in one or more existing variables. Most often, we create a new column using an expression that describes how we transform existing data; motivation can be the normalization of a numerical variable, the creation of an indicator or categorical variable, the summation of multiple variables into one single variable, or any other transformation in order to obtain a new variable that is in some way required for the further steps of the analysis process. Assuming that we want to create a new column that stores the sum of two numerical values of the existing columns, then the base R syntax might look as follows: df$c &lt;- df$a + df$b The dplyr package offers us an alternative in the form ofmutate and transmute functions: mutate(df, c = a + b) transmute(df, c = a + b) Difference between these two function is that mutate returns the entire original data frame with newly created columns, while transmute retains only the columns specified inside the function call. Therefore, we can use transmute as a shortened combination of mutate and select: transmute(df, a, c = a + b) # same as mutate(df, c = a + b) %&gt;% select(a, c) (NOTE: We don’t have to necessarily expect a degradation of peformance here as a result of copying data frames, since R does a “shallow copy” when adding a column, that is, a new data frame only references a new column while retaining references to the remaining columns. If we were creating a “deep copy”, i.e. if we were copying all columns into the new frame, which would make this operation “expensive” when it comes to processing) Note that mutate andtransmute are not alternatives to the UPDATE command from SQL, but rather correspond to the expressions used in the SELECT [segment of the command in scenarios when we do not select individual columns but combine them as part of an expression part of the SQL query. The mutate and transmute functions use common (vectorized) functions and operators, but we also have a number of additional so-called “window” functions that give us additional flexibility when creating new variables, such as: ntile,cut - for transforming a numerical column into a categorical; ntile will make categories of the same size, while cut will cut the range in equally sized intervals dense_rank,min_rank - for ranking of observations (the difference is only in handling observations with the same values) between - for creating an indicator column explaining whether a given variable is in the interval given by two other columns pmin,pmax - “parallel minimum and maximum”, ie the minimum or maximum values of the selected columns viewed by individual rows etc. A list of all available functions can be found in the documentation. Assignment 11.5 - creating new columns # add a `hadRelativesOnBoard` logical column to the` titanic` table which # describes whether the passenger had relatives on board # for randomly selected 10 passengers over the age of 20 who boarded in Southampton # print out passenger&#39;s name, travel class, and transformed ticket price # replace the ticket price with what it would cost today # (assume $1 of 1912 equals $23.85 today, calculating in the inflation) # call the new column `FareToday` # round the amount to two decimal places and add the prefix `$` # sort the result by passenger class in descending order # create a `FareCategory` column which puts the ticket prices # into five equally sized categories # then randomly select 20 passengers and print # passenger name, travel class, ticket price and category # sort the result by price category # add an `EmbarkationPort` column to the `titanic` table, which will contain # the full name of the port of embarkation (Southampton, Queenstown or Cherbourg) # use `mutate` and two `ifelse` functions # print the first 10 rows of the `titanic` table # add a `hadRelativesOnBoard` logical column to the` titanic` table which # describes whether the passenger had relatives on board titanic &lt;- mutate(titanic, hadRelativesOnBoard = SibSp&gt; 0 | Parch&gt; 0) # for randomly selected 10 passengers over the age of 20 who boarded in Southampton # print out passenger&#39;s name, travel class, and transformed ticket price # replace the ticket price with what it would cost today # (assume $1 of 1912 equals $23.85 today, calculating in the inflation) # call the new column `FareToday` # round the amount to two decimal places and add the prefix `$` # sort the result by passenger class in descending order filter(titanic, Sex == &#39;female&#39;, Age&gt; 20, Embarked == &#39;S&#39;) %&gt;% sample_n(10) %&gt;% transmute(Name, Pclass, FareToday = str_c(&quot;$&quot;, round(Fare * 23.85, 2))) %&gt;% arrange(desc(Pclass)) ## # A tibble: 10 x 3 ## Name Pclass FareToday ## &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; ## 1 &quot;Ford, Miss. Doolina Margaret \\&quot;Daisy\\&quot;&quot; 3 $819.84 ## 2 Sandstrom, Mrs. Hjalmar (Agnes Charlotta Bengtsson) 3 $398.3 ## 3 Panula, Mrs. Juha (Maria Emilia Ojala) 3 $946.55 ## 4 Jussila, Miss. Mari Aina 3 $234.33 ## 5 Stanley, Miss. Amy Zillah Elsie 3 $180.07 ## 6 Honkanen, Miss. Eliina 3 $189.01 ## 7 Nilsson, Miss. Helmina Josefina 3 $187.32 ## 8 Davis, Miss. Mary 2 $310.05 ## 9 Leader, Dr. Alice (Farnham) 1 $618.41 ## 10 Carter, Mrs. William Ernest (Lucile Polk) 1 $2862 # create a `FareCategory` column which puts the ticket prices # into five equally sized categories # then randomly select 20 passengers and print # passenger name, travel class, ticket price and category # sort the result by price category mutate(titanic, FareCategory = ntile(Fare, 5)) %&gt;% sample_n(20) %&gt;% select(Name, Pclass, Fare, FareCategory) %&gt;% arrange(FareCategory) ## # A tibble: 20 x 4 ## Name Pclass Fare FareCategory ## &lt;chr&gt; &lt;fct&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Charters, Mr. David 3 7.73 1 ## 2 Connolly, Miss. Kate 3 7.75 1 ## 3 Connaghton, Mr. Michael 3 7.75 1 ## 4 Youseff, Mr. Gerious 3 7.22 1 ## 5 Lemore, Mrs. (Amelia Milley) 2 10.5 2 ## 6 Lievens, Mr. Rene Aime 3 9.5 2 ## 7 Larsson, Mr. August Viktor 3 9.48 2 ## 8 Meyer, Mr. August 2 13 3 ## 9 Lindell, Mr. Edvard Bengtsson 3 15.6 3 ## 10 Jerwan, Mrs. Amin S (Marie Marthe Thuillard) 2 13.8 3 ## 11 Chapman, Mr. Charles Henry 2 13.5 3 ## 12 Blackwell, Mr. Stephen Weart 1 35.5 4 ## 13 Moran, Miss. Bertha 3 24.2 4 ## 14 Collyer, Mrs. Harvey (Charlotte Annie Tate) 2 26.2 4 ## 15 Chip, Mr. Chang 3 56.5 5 ## 16 Chaffee, Mr. Herbert Fuller 1 61.2 5 ## 17 Bissette, Miss. Amelia 1 136. 5 ## 18 Endres, Miss. Caroline Louise 1 228. 5 ## 19 Allison, Mrs. Hudson J C (Bessie Waldo Danie~ 1 152. 5 ## 20 Carter, Master. William Thornton II 1 120 5 # add an `EmbarkationPort` column to the `titanic` table, which will contain # the full name of the port of embarkation (Southampton, Queenstown or Cherbourg) # use `mutate` and two `ifelse` functions mutate(titanic, EmbarkationPort = ifelse(Embarked == &#39;S&#39;, &#39;Southampton&#39;, ifelse(Embarked == &#39;C&#39;, &#39;Cherbourg&#39;, &#39;Queenstown&#39;))) -&gt; titanic # print the first 10 rows of the `titanic` table head(titanic, 10) ## # A tibble: 10 x 14 ## PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare ## &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;chr&gt; &lt;fct&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 0 3 Brau~ male 22 1 0 A/5 2~ 7.25 ## 2 2 1 1 Cumi~ fema~ 38 1 0 PC 17~ 71.3 ## 3 3 1 3 Heik~ fema~ 26 0 0 STON/~ 7.92 ## 4 4 1 1 Futr~ fema~ 35 1 0 113803 53.1 ## 5 5 0 3 Alle~ male 35 0 0 373450 8.05 ## 6 6 0 3 Mora~ male NA 0 0 330877 8.46 ## 7 7 0 1 McCa~ male 54 0 0 17463 51.9 ## 8 8 0 3 Pals~ male 2 3 1 349909 21.1 ## 9 9 1 3 John~ fema~ 27 0 2 347742 11.1 ## 10 10 1 2 Nass~ fema~ 14 1 0 237736 30.1 ## # ... with 4 more variables: Cabin &lt;chr&gt;, Embarked &lt;fct&gt;, ## # hadRelativesOnBoard &lt;lgl&gt;, EmbarkationPort &lt;chr&gt; 11.5 Sample Dataset: Houston flights Let’s now load the hflights data frame, which is in a package of the same name and can be retrieved from the CRAN repository. After loading the package, we can move the data frame to the global environment with the help of the data function. Assignment 11.6 - dataset hflights # load the `hflights` package # if necessary install it from the CRAN repository # put the `hflights` data frame in the global environment using the `data` function # convert the frame to `hflights` into a &#39;tibble&#39; # briefly explore the `hflights` dataset # you can also check the documentation with the help of command `?hflights` # install.packages(&quot;hflights&quot;) #if necessary #library(dplyr) # if not already loaded #library(hflights) data(hflights) # convert the frame to `hflights` in &#39;tibble&#39; hflights &lt;- as_tibble(hflights) glimpse(hflights) head(hflights) ## Observations: 227,496 ## Variables: 21 ## $ Year &lt;int&gt; 2011, 2011, 2011, 2011, 2011, 2011, 2011, 20... ## $ Month &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,... ## $ DayofMonth &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1... ## $ DayOfWeek &lt;int&gt; 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6,... ## $ DepTime &lt;int&gt; 1400, 1401, 1352, 1403, 1405, 1359, 1359, 13... ## $ ArrTime &lt;int&gt; 1500, 1501, 1502, 1513, 1507, 1503, 1509, 14... ## $ UniqueCarrier &lt;chr&gt; &quot;AA&quot;, &quot;AA&quot;, &quot;AA&quot;, &quot;AA&quot;, &quot;AA&quot;, &quot;AA&quot;, &quot;AA&quot;, &quot;A... ## $ FlightNum &lt;int&gt; 428, 428, 428, 428, 428, 428, 428, 428, 428,... ## $ TailNum &lt;chr&gt; &quot;N576AA&quot;, &quot;N557AA&quot;, &quot;N541AA&quot;, &quot;N403AA&quot;, &quot;N49... ## $ ActualElapsedTime &lt;int&gt; 60, 60, 70, 70, 62, 64, 70, 59, 71, 70, 70, ... ## $ AirTime &lt;int&gt; 40, 45, 48, 39, 44, 45, 43, 40, 41, 45, 42, ... ## $ ArrDelay &lt;int&gt; -10, -9, -8, 3, -3, -7, -1, -16, 44, 43, 29,... ## $ DepDelay &lt;int&gt; 0, 1, -8, 3, 5, -1, -1, -5, 43, 43, 29, 19, ... ## $ Origin &lt;chr&gt; &quot;IAH&quot;, &quot;IAH&quot;, &quot;IAH&quot;, &quot;IAH&quot;, &quot;IAH&quot;, &quot;IAH&quot;, &quot;I... ## $ Dest &lt;chr&gt; &quot;DFW&quot;, &quot;DFW&quot;, &quot;DFW&quot;, &quot;DFW&quot;, &quot;DFW&quot;, &quot;DFW&quot;, &quot;D... ## $ Distance &lt;int&gt; 224, 224, 224, 224, 224, 224, 224, 224, 224,... ## $ TaxiIn &lt;int&gt; 7, 6, 5, 9, 9, 6, 12, 7, 8, 6, 8, 4, 6, 5, 6... ## $ TaxiOut &lt;int&gt; 13, 9, 17, 22, 9, 13, 15, 12, 22, 19, 20, 11... ## $ Cancelled &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,... ## $ CancellationCode &lt;chr&gt; &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, ... ## $ Diverted &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,... ## # A tibble: 6 x 21 ## Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier FlightNum ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; ## 1 2011 1 1 6 1400 1500 AA 428 ## 2 2011 1 2 7 1401 1501 AA 428 ## 3 2011 1 3 1 1352 1502 AA 428 ## 4 2011 1 4 2 1403 1513 AA 428 ## 5 2011 1 5 3 1405 1507 AA 428 ## 6 2011 1 6 4 1359 1503 AA 428 ## # ... with 13 more variables: TailNum &lt;chr&gt;, ActualElapsedTime &lt;int&gt;, ## # AirTime &lt;int&gt;, ArrDelay &lt;int&gt;, DepDelay &lt;int&gt;, Origin &lt;chr&gt;, ## # Dest &lt;chr&gt;, Distance &lt;int&gt;, TaxiIn &lt;int&gt;, TaxiOut &lt;int&gt;, ## # Cancelled &lt;int&gt;, CancellationCode &lt;chr&gt;, Diverted &lt;int&gt; 11.6 Grouping and aggregation with group_by andsummarise In the data analysis literature, we will often come across a so-called SAC paradigm (Split-Apply-Combine). It’s a strategy that comes down to breaking down a big task into smaller parts, doing some work on each part, and finally combining all the results into a single whole. The need for this paradigm is found in different analysis scenarios - in exploratory data analysis we will want to calculate different statistics or create new variables separately for different subsets of data (eg, depending on a category variable); When processing extremely large amounts of data, we often want to speed up the processing process by breaking the data into smaller sets that will each be processed separately (the well-known Map-Reduce principle). Users of SQL will easily recognize this principle as grouping and aggregation, which are carried out through the GROUP BY segment of an SQL query with the accompanying elements in the SELECT part. The dplyr package offers very similar functionality (albeit in a procedural way) - we first perform “grouping”, i.e. create subsets of rows of a frame, and then carry out further processing in parallel over each subset, to collect all the results in a single dataframe. For grouping, dplyr offers thegroup_by function, which converts a table (data frame) into a grouped table (grouped_tbl`): group_by(df, a, b, c) Let’s try this feature on our hflights framework. Assignment 11.7 - grouped table # create a `flight815` variable that will contain rows from the `hflights` data frame # related to flight number 815 # create a `grouped815` variable that will contain rows from `flight815` # grouped by month # check the class of variables `flight815` and `grouped815` # briefly explore the `flight815` and `grouped815` variable structures # with the help of the `glimpse` function # create a `flight815` variable that will contain rows from the `hflights` data frame # related to flight number 815 flight815 &lt;- filter(hflights, FlightNum == 815) # create a `grouped815` variable that will contain rows from `flight815` # grouped by month grouped815 &lt;- group_by(flight815, Month) # check the class of variables `flight815` and `grouped815` class(flight815) class(grouped815) ## [1] &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; ## [1] &quot;grouped_df&quot; &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; # briefly explore the `flight815` and `grouped815` variable structures # with the help of the `glimpse` function glimpse(flight815) print(&quot;------------------------&quot;) # for more transparent printing on the console glimpse(grouped815) ## Observations: 95 ## Variables: 21 ## $ Year &lt;int&gt; 2011, 2011, 2011, 2011, 2011, 2011, 2011, 20... ## $ Month &lt;int&gt; 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,... ## $ DayofMonth &lt;int&gt; 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, ... ## $ DayOfWeek &lt;int&gt; 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1, 7, 6,... ## $ DepTime &lt;int&gt; 2116, 2109, 2111, 2112, 2124, 2145, 2114, 21... ## $ ArrTime &lt;int&gt; 2255, 2252, 2255, 2326, 2321, 2327, 2324, 23... ## $ UniqueCarrier &lt;chr&gt; &quot;CO&quot;, &quot;CO&quot;, &quot;CO&quot;, &quot;CO&quot;, &quot;CO&quot;, &quot;CO&quot;, &quot;CO&quot;, &quot;C... ## $ FlightNum &lt;int&gt; 815, 815, 815, 815, 815, 815, 815, 815, 815,... ## $ TailNum &lt;chr&gt; &quot;N74856&quot;, &quot;N57852&quot;, &quot;N78866&quot;, &quot;N57863&quot;, &quot;N57... ## $ ActualElapsedTime &lt;int&gt; 219, 223, 224, 254, 237, 222, 250, 231, 229,... ## $ AirTime &lt;int&gt; 194, 191, 190, 197, 198, 200, 192, 187, 186,... ## $ ArrDelay &lt;int&gt; 12, 6, 9, 40, 35, 41, 38, 18, 19, 10, 29, 33... ## $ DepDelay &lt;int&gt; 6, -1, 1, 2, 14, 35, 4, 0, 6, 9, 6, 17, 38, ... ## $ Origin &lt;chr&gt; &quot;IAH&quot;, &quot;IAH&quot;, &quot;IAH&quot;, &quot;IAH&quot;, &quot;IAH&quot;, &quot;IAH&quot;, &quot;I... ## $ Dest &lt;chr&gt; &quot;LAX&quot;, &quot;LAX&quot;, &quot;LAX&quot;, &quot;LAX&quot;, &quot;LAX&quot;, &quot;LAX&quot;, &quot;L... ## $ Distance &lt;int&gt; 1379, 1379, 1379, 1379, 1379, 1379, 1379, 13... ## $ TaxiIn &lt;int&gt; 10, 13, 15, 18, 12, 10, 14, 19, 12, 12, 20, ... ## $ TaxiOut &lt;int&gt; 15, 19, 19, 39, 27, 12, 44, 25, 31, 13, 39, ... ## $ Cancelled &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,... ## $ CancellationCode &lt;chr&gt; &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, ... ## $ Diverted &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,... ## [1] &quot;------------------------&quot; ## Observations: 95 ## Variables: 21 ## Groups: Month [6] ## $ Year &lt;int&gt; 2011, 2011, 2011, 2011, 2011, 2011, 2011, 20... ## $ Month &lt;int&gt; 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,... ## $ DayofMonth &lt;int&gt; 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, ... ## $ DayOfWeek &lt;int&gt; 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1, 7, 6,... ## $ DepTime &lt;int&gt; 2116, 2109, 2111, 2112, 2124, 2145, 2114, 21... ## $ ArrTime &lt;int&gt; 2255, 2252, 2255, 2326, 2321, 2327, 2324, 23... ## $ UniqueCarrier &lt;chr&gt; &quot;CO&quot;, &quot;CO&quot;, &quot;CO&quot;, &quot;CO&quot;, &quot;CO&quot;, &quot;CO&quot;, &quot;CO&quot;, &quot;C... ## $ FlightNum &lt;int&gt; 815, 815, 815, 815, 815, 815, 815, 815, 815,... ## $ TailNum &lt;chr&gt; &quot;N74856&quot;, &quot;N57852&quot;, &quot;N78866&quot;, &quot;N57863&quot;, &quot;N57... ## $ ActualElapsedTime &lt;int&gt; 219, 223, 224, 254, 237, 222, 250, 231, 229,... ## $ AirTime &lt;int&gt; 194, 191, 190, 197, 198, 200, 192, 187, 186,... ## $ ArrDelay &lt;int&gt; 12, 6, 9, 40, 35, 41, 38, 18, 19, 10, 29, 33... ## $ DepDelay &lt;int&gt; 6, -1, 1, 2, 14, 35, 4, 0, 6, 9, 6, 17, 38, ... ## $ Origin &lt;chr&gt; &quot;IAH&quot;, &quot;IAH&quot;, &quot;IAH&quot;, &quot;IAH&quot;, &quot;IAH&quot;, &quot;IAH&quot;, &quot;I... ## $ Dest &lt;chr&gt; &quot;LAX&quot;, &quot;LAX&quot;, &quot;LAX&quot;, &quot;LAX&quot;, &quot;LAX&quot;, &quot;LAX&quot;, &quot;L... ## $ Distance &lt;int&gt; 1379, 1379, 1379, 1379, 1379, 1379, 1379, 13... ## $ TaxiIn &lt;int&gt; 10, 13, 15, 18, 12, 10, 14, 19, 12, 12, 20, ... ## $ TaxiOut &lt;int&gt; 15, 19, 19, 39, 27, 12, 44, 25, 31, 13, 39, ... ## $ Cancelled &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,... ## $ CancellationCode &lt;chr&gt; &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, ... ## $ Diverted &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,... # print the first five rows of the `flight815` variable head(flight815) print( &quot;------------------------&quot;) # print the first five rows of the `grouped815` variable head(grouped815) ## # A tibble: 6 x 21 ## Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier FlightNum ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; ## 1 2011 4 30 6 2116 2255 CO 815 ## 2 2011 4 29 5 2109 2252 CO 815 ## 3 2011 4 28 4 2111 2255 CO 815 ## 4 2011 4 27 3 2112 2326 CO 815 ## 5 2011 4 26 2 2124 2321 CO 815 ## 6 2011 4 25 1 2145 2327 CO 815 ## # ... with 13 more variables: TailNum &lt;chr&gt;, ActualElapsedTime &lt;int&gt;, ## # AirTime &lt;int&gt;, ArrDelay &lt;int&gt;, DepDelay &lt;int&gt;, Origin &lt;chr&gt;, ## # Dest &lt;chr&gt;, Distance &lt;int&gt;, TaxiIn &lt;int&gt;, TaxiOut &lt;int&gt;, ## # Cancelled &lt;int&gt;, CancellationCode &lt;chr&gt;, Diverted &lt;int&gt; ## [1] &quot;------------------------&quot; ## # A tibble: 6 x 21 ## # Groups: Month [1] ## Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier FlightNum ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; ## 1 2011 4 30 6 2116 2255 CO 815 ## 2 2011 4 29 5 2109 2252 CO 815 ## 3 2011 4 28 4 2111 2255 CO 815 ## 4 2011 4 27 3 2112 2326 CO 815 ## 5 2011 4 26 2 2124 2321 CO 815 ## 6 2011 4 25 1 2145 2327 CO 815 ## # ... with 13 more variables: TailNum &lt;chr&gt;, ActualElapsedTime &lt;int&gt;, ## # AirTime &lt;int&gt;, ArrDelay &lt;int&gt;, DepDelay &lt;int&gt;, Origin &lt;chr&gt;, ## # Dest &lt;chr&gt;, Distance &lt;int&gt;, TaxiIn &lt;int&gt;, TaxiOut &lt;int&gt;, ## # Cancelled &lt;int&gt;, CancellationCode &lt;chr&gt;, Diverted &lt;int&gt; We see that by grouping we have not “lost” any information - the grouped data frame still looks identical to the original, “non-grouped” frame. In fact, the only indication that something is different is the new, inherited class and the Groups: .. line in the structure print. This means that grouping a frame is merely an indication that some further (most often aggregation) operations are not necessarily performed over the entire data frame, but over individual groups. Also, if we want, we can always easily “ungroup” the frame with the help of the ungroup function. For aggregation, we use the summarise function that receives data (a grouped table) and then combinations of aggregation functions and columns over which they are executed, for example: summarise(df, meanA = mean(a), sdA = sd(a)) As a rule, for the aggregation function we can use any function that reduces a vector to a single value (eg mean,max, sd etc.). Also, dplyr offers some useful functions like: first,last, nth - retreieves the first, last, or n-th element of the group n,n_distinct - counts the number of (distinct) values Assignment 11.8 - function summarise # calculate the average flight arrival delay for the data frames `flight815` and` grouped815` # use the `summarise` function summarise(flight815, meanDelay = mean(ArrDelay)) print( &quot;------------------------&quot;) summarise(grouped815, meanDelay = mean(ArrDelay)) ## # A tibble: 1 x 1 ## meanDelay ## &lt;dbl&gt; ## 1 21.6 ## [1] &quot;------------------------&quot; ## # A tibble: 6 x 2 ## Month meanDelay ## &lt;int&gt; &lt;dbl&gt; ## 1 4 21.1 ## 2 5 20.9 ## 3 6 28.0 ## 4 8 -27.5 ## 5 9 14 ## 6 10 -13 Note that it is advisable to name the aggregated columns. In practice, we usually do not create separate “grouped” data frames but rather we carry out the entire process of selecting rows and columns, grouping, and performing aggregation in a single command. If we use the operator %&gt;%, then it a possible example might look like this: filter(df, a&gt; 5) %&gt;% group_by(a, b) %&gt;% summarise(meanC = mean(c)) %&gt;% arrange(desc(b)) Finally, let us reiterate the readily apparent similarity of the above expression with SQL queries in relational databases, with the essential difference that we perform operations here procedurally, which greatly increases readability and allows us to easily store and check the intermediate result at any time. Now let’s try to take advantage of all the current knowledge of the dplyr package and solve the following examples. All tasks are related to the entire hflights dataset. Assignment 11.9 - advanced queries # print out how many flights were canceled because of # bad weather in each month # result table must have contain only columns called # `Month` and` BadWeatherCancellations` # print out the average time of arrival delay # for departing flights to airports # LAX, JFK and LGA on different days of the week # name the new column &#39;MeanArrDelay&#39; # ignore rows with NA values # sort the result by descending average delay time # print out how many flights were canceled because of # bad weather in each month # result table must have contain only columns called # `Month` and` BadWeatherCancellations` filter(hflights, CancellationCode == &#39;B&#39;) %&gt;% group_by(Month) %&gt;% summarise(BadWeatherCancellations = n()) ## # A tibble: 12 x 2 ## Month BadWeatherCancellations ## &lt;int&gt; &lt;int&gt; ## 1 1 149 ## 2 2 929 ## 3 3 50 ## 4 4 40 ## 5 5 106 ## 6 6 49 ## 7 7 29 ## 8 8 108 ## 9 9 38 ## 10 10 46 ## 11 11 13 ## 12 12 95 # print out the average time of arrival delay # for departing flights to airports # LAX, JFK and LGA on different days of the week # name the new column &#39;MeanArrDelay&#39; # ignore rows with NA values # sort the result by descending average delay time filter(hflights, Dest %in% c(&#39;LAX&#39;, &#39;JFK&#39;, &#39;LGA&#39;)) %&gt;% group_by(DayOfWeek, Dest) %&gt;% summarise(MeanArrDelay = round(mean(ArrDelay, na.rm = T), 2)) %&gt;% ungroup() %&gt;% arrange(desc(MeanArrDelay)) ## # A tibble: 21 x 3 ## DayOfWeek Dest MeanArrDelay ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 4 LGA 21.3 ## 2 4 JFK 17.2 ## 3 5 LGA 13.6 ## 4 1 LGA 13.5 ## 5 7 JFK 10.8 ## 6 1 LAX 10.7 ## 7 3 LGA 10.5 ## 8 4 LAX 10.3 ## 9 2 LGA 9.85 ## 10 6 JFK 9.76 ## # ... with 11 more rows Assignment 11.10 - advanced queries (2) # in the tasks that follow, imagine you wrote your own # function that finds the most common category: mostFreqValue &lt;- function(x) table(x) %&gt;% sort(decreasing = T) %&gt;% names%&gt;% `[`(1) # in the `hflights` table, divide the delay time # in 10 categories with the help of the `ntile` function # call the new column `ArrDelayCatId` # then print all the category ids, # total number of flights within a category, # minimum and maximum time of arrival delay # and which airport most often # appears in this category # repeat the previous example but instead of the function # `ntile` try the` cut` function Assignment 11.11 - advanced queries (2) # in the `hflights` table, divide the delay time # in 10 categories with the help of the `ntile` function # call the new column `ArrDelayCatId` # print an identifier for each category # categories, total number of flights within a category, # minimum and maximum time of arrival delay # and which airport is most often # appears in this category mutate(hflights, ArrDelayCatId = ntile(ArrDelay, 10)) %&gt;% group_by(ArrDelayCatId) %&gt;% summarise(totalFlights = n(), minArrDelay = min(ArrDelay), maxArrDelay = max(ArrDelay), mostFreqDest = mostFreqValue(Dest)) ## # A tibble: 11 x 5 ## ArrDelayCatId totalFlights minArrDelay maxArrDelay mostFreqDest ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 1 22388 -70 -14 ATL ## 2 2 22387 -14 -9 ATL ## 3 3 22388 -9 -6 DAL ## 4 4 22387 -6 -3 DAL ## 5 5 22387 -3 0 DAL ## 6 6 22388 0 3 DAL ## 7 7 22387 3 8 DAL ## 8 8 22388 8 15 DAL ## 9 9 22387 15 32 DAL ## 10 10 22387 32 978 DAL ## 11 NA 3622 NA NA DAL # repeat the previous example but instead of the function # `ntile` try the` cut` function mutate(hflights, ArrDelayCat = cut(ArrDelay, 10)) %&gt;% group_by(ArrDelayCat) %&gt;% summarise(totalFlights = n(), minArrDelay = min(ArrDelay), maxArrDelay = max(ArrDelay), mostFreqDest = mostFreqValue(Dest)) ## Warning: Factor `ArrDelayCat` contains implicit NA, consider using ## `forcats::fct_explicit_na` ## # A tibble: 11 x 5 ## ArrDelayCat totalFlights minArrDelay maxArrDelay mostFreqDest ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 (-71,34.8] 203101 -70 34 DAL ## 2 (34.8,140] 18551 35 139 DAL ## 3 (140,244] 1849 140 244 ATL ## 4 (244,349] 294 245 346 ATL ## 5 (349,454] 48 350 450 ATL ## 6 (454,559] 16 458 556 ATL ## 7 (559,664] 4 579 663 DFW ## 8 (664,768] 4 685 766 DFW ## 9 (768,873] 4 775 861 DEN ## 10 (873,979] 3 918 978 DFW ## 11 &lt;NA&gt; 3622 NA NA DAL 11.7 Merging data frames with join functions Merging data frames is an operation known to all developers who have experience in working with relational databases and SQL. When stored in a relational database, tables are often decomposed into multiple tables by a process called “normalization”. The purpose of normalization is mainly to eliminate unnecessary redundancy - each table retains enough data to reconstruct the original data, i.e. sets of observations, which is done through the “join” operation. Although there are different forms of joining tables, by far the most common is the so-called a “natural” join where we have unique identifiers in one table related to the data in the other table. For example - suppose we have a student and city tables, where a student table may have a column that stores the zip code of the user’s place of residence, while other location-related information is in the city table. Storing this information in the students table would be redundant, since the zip code as such uniquely identifies a city, and if we want to see the names of the cities when printing users, we perform a natural join of two tables by the zip code of the city. This zip code is the so-called “foreign key” in the student table and at the same time “primary key” in table city. We will create two simple data frames to demonstrate this. # initializing the `user` and` place` data frames student &lt;- data.frame(id = c(1: 3), lastName = c(&quot;Ivic&quot;, &quot;Peric&quot;, &quot;Anic&quot;), zipResidence = c(10000, 31000, 10000)) city &lt;- data.frame(zip = c(10000, 21000, 31000), nameCity = c(&quot;Zagreb&quot;, &quot;Split&quot;, &quot;Osijek&quot;)) If we want to have a data frame with columns (id, lastName, zipResidence, nameCity), then we need to naturally merge the two data frames. For this, the dplyr package offers theinner_join function. For example, if we want to join data frames df1 and df2, then we can perform the natural join like this: inner_join(df1, df2, by = c(&quot;s1&quot; = &quot;s2&quot;)) where the character strings \"s1\" and \"s2\" denote the column names of these “left” and “right” frames that we are joining (note only one equality sign! ). If the column we use has the same name in both tables, we can only specify the name of that column (or the character vector of multiple columns if we connect via a so-called “composite” foreign key), or omit this parameter completely (if the columns to which we perform merging are the only columns whose names match). Assignment 11.12 - natural join # print the result of natural joining of data frames `student` and `city` # print the result of natural joining of data frames `student` and `city` inner_join(student, city, by = c(&quot;zipResidence&quot; = &quot;zip&quot;)) ## id lastName zipResidence nameCity ## 1 1 Ivic 10000 Zagreb ## 2 2 Peric 31000 Osijek ## 3 3 Anic 10000 Zagreb Keep in mind that joining is a very “expensive” operation; if you join frames with a very large number of rows, the operation could take a very long time and take up a lot of memory. In the event that such “large” joins are often required, then it is strongly recommended to use the data.table package, which implements algorithms with significantly better performance in join operations (using indexing), or - if we work with relational database as a data source - perform the joins on the database side and then pull the result in R. In the next section, we will show how this can potentially be done without having to explicitly write separate SQL commands. If we look at the result of the example above, we can see that we “lost” one row from the city table. Specifically, line (21000, ‘Split’) had no corresponding student and “disappeared” from the result. This is a completely expected result of a natural join, but there are times when we want to keep all the rows from one of the tables we merge. In this case, we have the option of using the so-called “outer natural join” which works identically to the “inner” natural join that we have already seen, but retains all rows from one or both of the tables so that the rows that fail to join remain in the result but with NA values on the opposite side. We distinguish between “left”, “right”, and “full” outer join, depending on whether we want to keep all rows from the left, right or both tables. The dplyr package offers functions that perform all these types of joins, they have a signature identical to the function seen above and are calledleft_join, right_join andfull_join. Assignment 11.13 - outer join # print the result of natural joining of data frames `student` and `city` # which retains all rows from the `city` table # print the result of natural joining of data frames `student` and `city` # which retains all rows from the `city` table right_join(student, city, by = c(&quot;zipResidence&quot; = &quot;zip&quot;)) ## id lastName zipResidence nameCity ## 1 1 Ivic 10000 Zagreb ## 2 3 Anic 10000 Zagreb ## 3 NA &lt;NA&gt; 21000 Split ## 4 2 Peric 31000 Osijek 11.8 Integrating dplyr with relational databases Relational databases are a common source of data for the analysis done in R. One of the standard procedures for preparing data for the analytical process is the exporting from the database to a CSV file, which is then loaded into R. Often, before creating a CSV file, the data from the database must first be adequately collected and prepared, which means that the overall data preparation process requires initial work done in the SQL language directly on the database. This is especially important if we are working with larger datasets that would unnecessarily burden the machine that runs R, and which could be processed much more efficiently in the database itself. The above does not necessarily mean that we need a separate tool to run SQL queries for collecting data from the database. R contains a number of packages for directly communicating with more popular databases, such as MySQL or PostgreSQL. With these packages, we can connect to the base and execute SQL queries directly within the R script, all with the help of R commands, with the result being a data frame ready for further analysis in R. For example, communicating with the MySQL database from R might look like this: install.packages(&quot;RMySQL&quot;) library(RMySQL) # enter real parameters in a realistic scenario! conn &lt;- dbConnect(MySQL(), user = &#39;user&#39;, password = &#39;password&#39;, dbname = &#39;database_name&#39;, host = &#39;host&#39;) df &lt;- dbSendQuery(conn, &quot;SELECT * FROM MY_TABLE&quot;) We see that we first need to load the package with support for connecting to the selected database, then establish a connection with that database (parameters are usually provided to us by the database administrator), followed by providing SQL queries wrapped in appropriate function calls. Although this approach is not complicated and is relatively common, it unmistakably requires mixing R and SQL and programming code. The dplyr package can offer assistance here by relieving the developer of the need to write SQL queries inside the R script. Specifically, dplyr contains integrated support for the automatic conversion of expressions that use dplyr functions directly into an SQL query, which is transparent to the user. In other words, to some extent, we can work on a relational database in the same way as if it we are working on an R data frame. We should be careful here because not all dplyr functionality is translatable into SQL, but most standard preparatory operations such as joining and filtering can be done directly in the database, all the while using just R programming code. In order to put this into practice, we need a working relational database. As it is somewhat impractical to install or prepare an existing relational database just for the needs of a quick example, we can use an efficient alternative offered by the RSQLite package. It allows us to create an SQLite database, a simple relational database that does not require any special installation, does not use a server and holds all its data in one file. In the next program section, we load the RSQLite package, create a new database by creating a new file and saving the mtcars dataframe into a relational table called CARS. # installing and loading the `RSQLite` library - perform if necessary # install.packages ( &quot;RSQLite&quot;) # library(RSQLite) # opening connection with the &quot;SQLite&quot; database # (we&#39;re actually creating a file called simply &#39;mybase&#39;) conn &lt;- dbConnect(drv = SQLite(), dbname = &quot;mybase&quot;) # creating the `CARS` relational table # by using R&#39;s `mtcars` sample dataset dbWriteTable(conn, &quot;CARS&quot;, mtcars, overwrite = T) # checking the list of all existing tables dbListTables(conn) ## [1] &quot;CARS&quot; The usual way to collect data from a database is to embed SQL queries directly into the R program code and store the results in a variable. # get information on the maximum speed and weight of all 6-cylinder cars dbGetQuery(conn, &quot;SELECT mpg, wt FROM CARS WHERE cyl = 6&quot;) ## mpg wt ## 1 21.0 2.620 ## 2 21.0 2.875 ## 3 21.4 3.215 ## 4 18.1 3.460 ## 5 19.2 3.440 ## 6 17.8 3.440 ## 7 19.7 2.770 If we know SQL well, then this method of data collection probably does not seem overly complex and in fact, many developers and analysts are happy to combine SQL and R, optimizing the process of data collection and preparation. However, if we are not too knowledgable in SQL (or simply want a “cleaner” code), we can use the tools provided by the dplyr package. In the following example, we will repeat the process of connecting to the database, but we will execute everything with dplyr functions. # connecting to an existing SQLite database! conn2 &lt;- src_sqlite(&quot;mybase&quot;, create = FALSE) # checking the list of the existing tables src_tbls(conn2) # getting a reference to the existing table # no data gets loaded yet! cars &lt;- tbl(conn2, &quot;CARS&quot;) # get information on the maximum speed and weight of all 6-cylinder cars cars %&gt;% filter(cyl == 6) %&gt;% select(mpg, wt) %&gt;% collect ## [1] &quot;CARS&quot; ## # A tibble: 7 x 2 ## mpg wt ## &lt;dbl&gt; &lt;dbl&gt; ## 1 21 2.62 ## 2 21 2.88 ## 3 21.4 3.22 ## 4 18.1 3.46 ## 5 19.2 3.44 ## 6 17.8 3.44 ## 7 19.7 2.77 One thing to note is that dplyr will not perform anything until we explicitly request the collection of the information, and even then it will retrieve it “in portions” to save memory. This is why we explicitly used the collect function in the example above, essentially forcing R to put the entire result into a data frame (our in our case, simply printting it on the screen). In this way, the R programmer can significantly relieve his R environment by shifting the responsibility of “heavy” operations to the database, while keeping a consistent syntax regardless of whether the operations are done on the database or in memory as R objects. Exercises IThe following exercises use the mammals sleep dataset available in the visualization package ggplot2. Load the ggplot2 package and then put the msleep data frame in the global environment using the data function. Before solving the exercises be sure to explore the dataset and learn its features. Then do the following For 10 herbivores who sleep the longest, print their name, how many hours per day they sleep and their average body weight in kg. Sort the entries by sleep length in a descending order. Print the average, longest and shortest sleep time of the animals depending on their type of diet. Categorize the total sleeping time in 5 bins of equal length. For each category, print the total number of animals belonging to the category, and then the total number of class members who are not herbivores. Results should be sorted by sleep length in a descending order. Columns should have meaningful names and the final table should not contain NA values. The following data frame contains the status codes for animal conservation status and their descriptions: conservationStatus &lt;- data.frame( code = c(&quot;ex&quot;, &quot;ew&quot;, &quot;cr&quot;, &quot;en&quot;, &quot;vu&quot;, &quot;nt&quot;, &quot;cd&quot;, &quot;lc&quot;), description = c(&quot;extinct&quot;, &quot;extinct in the wild&quot;, &quot;critically endangered&quot;, &quot;endangered&quot;, &quot;vulnerable&quot;, &quot;near threatened&quot;, &quot;conservation dependent&quot;, &quot;least concern&quot;)) Add a conservationDesc column to ¸the msleep dataframe which will contain the correspoding conservation status descriptions. Be sure not to lose any rows from the msleep data frame. Programirajmo u R-u by Damir Pintar is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License.Based on a work at https://ratnip.github.io/FER_OPJR/ "],
["ggplot2.html", "12 Visualising data with ggplot2 package 12.1 Exploratory data analysis 12.2 Data visualization using the R language 12.3 Grammar of graphics and the ggplot2 package 12.4 Graphs in Exploratory Analysis and Reporting Exercises", " 12 Visualising data with ggplot2 package 12.1 Exploratory data analysis EDA - exploratory data analysis is a process of dataset analysis which aims to allow for familiarization with the data and reaching certain conclusions (or at least hypotheses) about the behavior of variables and observations contained within. The book R for Data Science (co-authored by Hadley Wickham, author of megapopular R packages such as stringr,lubridate, plyr,dplyr, etc.) states that exploratory analysis in principle consists of three main parts: data wrangling data visualization model making whereby this is not a sequential process but rather a circular one, with the phases often being intertwined. It is important to emphasize that exploratory analysis is not a process that can be automated or passively conducted; the analyst is not only a data observer but plays an active role in the process. What initiates the process are precisely the questions about the data which the analyst needs to provide themselves, and which are then answered by exploratory analysis; questions can be diverse, concise or complex, general or specific, and very often require a high level of creativity and curiosity on the part of the analyst. If we try to give a general template for these questions it might be these: how does a variable behave? How is it changing? What are the relationships between two or more variables? Can changing one variable explain the change of another variable? We have already been familiar with data wrangling by learning the basic R as well as functions offered by the tidyr anddplyr packages. In this lesson we will learn what many consider to be the backbone of exploratory analysis - data visualization. The third mentioned phase of the process - model creation - concerns the creation of concise representations of data in the form of mathematical (or other) models, which describe the relationships and behavior of variables in a useful and/or easily interpretable way (e.g. recognizing that a linear relationship between two variables can be described with a simple mathematical equation). We will deal with modeling in one of the upcoming chapters. 12.2 Data visualization using the R language One of the frequently mentioned features of the R language is its superior data visualization functionality. There are a number of analysts and developers who use R almost exclusively as a visualization tool because they can produce professional, attractive and easily interpretable graphs in a very fast and easy way. The basic R language itself has very good support for graphing (the so-called base plot system), but the true power of visualization lies in the many additional packages available through the CRAN repository. Basic support for graphing is achieved with the help of the generic plot function which has a great advantage in its exceptional simplicity when used in the straightforward fashion. Almost every popular class has its own implementation of this function, which means that for “fast” visualization it is often enough to just pass the desired object (or objects) to the specified function. For example, if we pass two numerical vectors of equal size to the function, the plot function will automatically create a graph with the first vector mapped to the x-axis and the second to the y-axis (represented by a linear continuous scale). The function will also automatically add appropriate annotations such as axis lines, tick marks and associated values, axis labels, etc. Assignment 12.1 - plot function x &lt;- 1:50 # pass arguments `x` and `x * x` to the `plot` function x &lt;- 1:50 # pass `x` and `x * x` to the `plot` function plot (x, x * x) Basic support is functional and simple, but limited. New things can be added to the created graph, but not modified. Likewise, fine-tuning certain aspects of the graph often makes the calls cumbersome and unreadable, and the aspect of simplicity (which is the main reason for using the plot function) is lost. Some of the R extensions which offer additional ways to draw visualizations are popular visualization packages such as grid andlattice. The grid package offers a richer set of functions for creating visualizations than those available within basic support, but there is no ability to calculate statistics related to the visualization itself, which often then needs to be performed “manually” before visualization functions are called. The lattice package is especially popular for creating so-called “conditional” or “faceted” graphs, which means more graphs of the same type are quickly vreated where each graph corresponds to an individual value of a feature (e.g. multiple graphs created for each gender or age group). The lattice package also has support for the automatic creation of graph legends, etc., which often has to be done manually with other packages. The biggest issue this package is probably the fact that it is not based on any formal model, which amongst other things makes it difficult to extend it with additional functionality. There are other popular packages, either for general visualization needs or for specific applications, but we will ultimately focus on the most popular visualization packages of the R language today - the ggplot2 package. The author of this package is already mentioned prof. Hadley Wickham, and the package itself is based on the so-called “grammar of graphics” (which is why it’s called ggplot2, with 2 denoting that the package is primarily used for drawing two-dimensional visualizations). The popularity of this package lies in the fact that it tries to combine the benefits of basic graphing support and the lattice package, while relying of a formal, clearly defined model. The advantage of this approach is that it enables the creation of a wide range of visualizations based on concise and cleary syntax as well as allowing for easy extension with additional functionality. A potential problem is a slightly steeper initial learning curve since it is necessary to first adopt the “logic” of graph creation, or to be more precise to learn the basic principles of the “grammar of graphics”. But once this initial obstacle is overcome, creating high quality visualizations is quick, easy, and effective, as evidenced by the fact that ggplot2 is now one of the most popular data visualization packages that has gone beyond the boundaries of R and is being reimplemented in other programming languages for data analysis (e.g. package ggplot in Python, package gramm in Matlab etc.). 12.3 Grammar of graphics and the ggplot2 package Grammar of graphics gives us the following: principles that enable the creation and interpretation of complex visualizations guidelines to understand what represents a “well-crafted” visualization Just as grammar in linguistics enables the formation of “quality” sentences, so does the grammar of graphics allow to view graphs as a kind of “sentence” whose understanding depends on how to fit the individual components into a clear, understandable whole. But, again just like in “regular” grammar, a sentence can be grammatically correct but still meaningless - in other words, grammar is the basis for quality, but not a guarantee of the same; the meaningfulness and purposefulness of the final result still depends on the creativity and ability of the creator of the “sentence”, i.e. visualization. In order to facilitate the learning of grammar of graphics, which realistically represents the biggest obstacle to mastering the ggplot2 package, it is important to adhere to its basic principle that can be paraphrased in this way - quality visualization is actually a composition of a number of components, each of which has a clearly defined role. Consequently, we should not view the graph as one compact unit, but should try to identify the individual parts and learn how they contribute to the final visualization. These parts are not necessarily the visual components of the graph, that is, the parts that make up the graphics we are looking at, but the building blocks that the visualization system uses to produce the final result. 12.3.1 Aspects of data, aesthetics and geometries To begin with, let’s introduce a simplified grammar model which uses three basic components: data (which we want to visualize) aesthetics (which describe how to map data to graph elements) geometries (which describe how graph elements appear visually) Data is, of course, a key component of the graph. It represents the central thing we want the showcase with the visualization itself. It is also relatively independent of the other components of visualization - we can develop the same principles of visualization and then apply them over different datasets. Nevertheless, the creation of a new graph usually begins with the inspecting the features of the dataset and how they may dictate the direction of the further steps in the visualization process. Aesthetics does not really have to do with its literal interpretation of “set of principles concerned with the nature and appreciation of beauty” but is actually about choosing how to display particular segments of the dataset on a graph. Namely, in order for the visualization of the data to make sense, we must present that information in a visually interpretable way - or in another words, “map the dataset variables to graph elements”. A common principle is to display a variable value as a position on a two-dimensional plane with the help of a Cartesian coordinate system, which segments the plane orthogonally using two axes, called x and y, which in turn represent two “basic aesthetics”. These are not the only aesthetics available to us - there are also color, shape, pattern, etc. One way explain aesthetics is “that what is being explained by a legend beside a graph”; if aesthetics is actually a mapping to a visual component of a graph, the legend of the graph is its inverse - an explanation of what that component actually means. Finally, geometry is actually a description of how to actually draw specific graph elements. For example, if we mapped some dataset variables to the x and y axes, then an individual observation could be represented by a point, which in turn would mean we are using the so-called scatterplot graph which relies on point geometry. We could also decide on the line geometry and present the same data with a line which connects observations (which is a common way to depict time series data). In short, geometry can most easily be tied to that what we colloquially call the “graph type” - we choose to draw a scatterplot, a line graph, a boxplot, a bar chart, a histogram etc. and all this boils down to picking the appropriate “geometry” for our graph. Each geometry has its own parameters that can be either fixed or data dependent. For example, a point has the properties of its position (x and y coordinates in the Cartesian system), color and shape. We can depict all points on a graph by using a circle shape, or a cross, or any other available symbol, but we can also make it so the shape o this point depends on the value of a categorical variable. Furthermore, we can “stack” different geometries so that the same graph ultimately becomes a merged combination of various geometries. Let’s demonstrate all this with an example. For the first batch of examples, we will leverage the mtcars dataset that we received with the basic R language distribution (within the datasets package). Let’s load that dataset into the global environment using the `data¸function. Assignment 12.2 - getting acquainted with mtcars dataset # load the `mtcars` dataframe into the global environment # briefly explore the `mtcars` dataset (glimpse, head ...) # load the `mtcars` dataframe into the global environment data(mtcars) # briefly explore the `mtcars` dataset (glimpse, head ...) glimpse(mtcars) head(mtcars) ## Observations: 32 ## Variables: 11 ## $ mpg &lt;dbl&gt; 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.... ## $ cyl &lt;dbl&gt; 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, ... ## $ disp &lt;dbl&gt; 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146.7, 1... ## $ hp &lt;dbl&gt; 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180, ... ## $ drat &lt;dbl&gt; 3.90, 3.90, 3.85, 3.08, 3.15, 2.76, 3.21, 3.69, 3.92, 3.9... ## $ wt &lt;dbl&gt; 2.620, 2.875, 2.320, 3.215, 3.440, 3.460, 3.570, 3.190, 3... ## $ qsec &lt;dbl&gt; 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.00, 2... ## $ vs &lt;dbl&gt; 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, ... ## $ am &lt;dbl&gt; 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, ... ## $ gear &lt;dbl&gt; 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4, ... ## $ carb &lt;dbl&gt; 4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2, ... ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 We can see that this data describes the characteristics of 32 (old) cars such as: weight, maximum speed, horsepower, number of cylinders and the like. Since some of the variables in this dataset have a numeric datatype even though a categorical type would be more suitable, before we continue we can first factorize these variables. While we are doing this, we can also demonstrate a few as yet unused parameters and functions which help with the categorization - the labels parameter which allows “relabeling” of categories (according to the numerical and alphabetical order of the original categories) and the ordered function which is simply a shorthand way to convert a numerical variable into a categorical with the natural order of numbers being used as a template for ordering category levels. mtcars$vs &lt;- factor(mtcars$vs, labels = c(&quot;V&quot;, &quot;S&quot;)) mtcars$am &lt;- factor(mtcars$am, labels = c(&quot;automatic&quot;, &quot;manual&quot;)) mtcars$cyl &lt;- ordered(mtcars$cyl) mtcars$gear &lt;- ordered(mtcars$gear) mtcars$carb &lt;- ordered(mtcars$carb) When we create a ggplot2 visualization it often helps to think about the “layers” of the graph. Each layer in some way “overlays” a graph like a transparent sheet, which allows us to place multiple different types of data representations on the same graph (e.g. we show points and then also connect them with a line). Let’s say we’re interested in the relationship between car weight and its fuel consumption. An intuitive way of visualizing would be: car weight (wt) on the x axis of the graph fuel consumption (mpg) on the y axis of the graph Let’s see how to do this with the help of ggplot2 visualization. Note that we will intentionally use the “extended” syntax first - this way of creating ggplot2 graphs is almost never used in practice since there is a much more convenient, concise method, but doing it like this first we can more easily spot some of the essential elements of graph construction which correspond to the components outlined by the grammar of graphics. Let’s now create a scatterplot showing the relationship between the weights and fuel consumption values of cars as described by the mtcars dataset. 12.3.1.1 The first ggplot2 graph ggplot() + layer (data = mtcars, # 1. data mapping = aes(x = wt, y = mpg), # 2. mapping / aesthetics geom = &quot;point&quot;, # 3. geometry stat = &quot;identity&quot;, # ignore for now position = &quot;identity&quot;) # ignore for now The basic graph building function is the ggplot function (notice the absence of the number 2 in the function call! - to simplify things, we will call these graphs ggplot graphs from now on, to mimic the name of the function, not the package). This function initializes an object of class ggplot - ggplot visualizations are actually objects, and what we normally consider to be a graph is only their visual representation. This is where the power of this type of representation actually lies - a graph is something we can change, reshape, extend, and store at will, with the visualization being merely the final by-product of managing this object. We then add “layers” to this object using the layer function. We add a layer to the object with the help of the operator +, which is ubiquitous when creating ggplot graphs. This layer as such has those grammatical aspects that we discussed earlier - data, aesthetics, and geometry. In the call, we see two more aspects of graphic grammar - statistics and position - which we will explain later in this chapter; for now it suffices to say that identity corresponds to ‘leave it as is’, that is, we are not using some additional processing within the visualization process which may be provided by these aspects if needed. Although formally each layer has its own grammatical aspects, there are almost always aspects that are common to all layers (e.g. very often one graph shows just one data set and all layers share the x and y axes). If this is the case with our visualization, then we can define them as such immediately when creating the ggplot object, which then become the default parameters of the subsequent layers we add (although the layers always have the option of overriding these “default” parameters). Furthermore, when we add layers we rarely call the layer function itself; a much more convenient choice is calling one of the many helper functions offered by the ggplot2 package that have intuitive names and pre-set many of the parameters for us. For example, the geom_point function adds a layer that will inherit the default parameters set in the ggplot call and automatically sets the geometry aspect to “point”. In the following example we can see this “shorthand” way of creating a ggplot graph: # first `ggplot` graph, abbreviated graph construction method ggplot(mtcars, aes(x = wt, y = mpg)) + geom_point() There is another “simplified” way to create ggplot graphs which leverages the qplot function (from quick plot’*). This function is actually a wrapper that allows us to create ggplot graphs using a syntax which is very similar to the syntax of the default plot function. # first `ggplot2` graph created using the `qplot` function qplot(x = wt, y = mpg, data = mtcars) The main reason for the existence of this function is offering a quick and easy alternative to the programmers used to the default plot function. While this way may seem convenient and straightforward, but is not recommended to be used in the long run, since it shares the same issues as the default plot function - it is good for quick and simple calls, but soon loses the advantage of simplicity when our visualizations become more involved. Now let’s go back to our graph - what if we wanted to add an extra variable on it? For example, we can see that all cars have either 4, 6 or 8 cylinders, and we want to communicate this information of our graph. Would this mean that we need to add a third dimesion to our two-dimensional graph? Not necessarily - we can simply use some of the unused aesthetics so far, such as color, size or shape of the points. Assignment 12.3 - the shape aesthetic # create a `ggplot` graph of the `mtcars` dataset with mappings: x = wt, y = mpg, shape = cyl # use point geometry # what happens if we put `as.numeric(cyl)` instead of the categorical variant? ggplot(mtcars, aes(x = wt, y = mpg, shape = cyl)) + geom_point() ## Warning: Using shapes for an ordinal variable is not advised Assignment 12.4 - the color aesthetic # create the same graph, but instead of the `shape` aesthetic, use the `color` aesthetic ggplot(mtcars, aes(x = wt, y = mpg, color = cyl)) + geom_point() Assignment 12.5 - combining aesthetics # create the same graph, but now for the `cyl` column, combine both the `shape` and `color` aesthetics ggplot(mtcars, aes(x = wt, y = mpg, color = cyl, shape = cyl)) + geom_point () ## Warning: Using shapes for an ordinal variable is not advised By comparing the graphs, we can conclude that the color communicates information much better than the shape, which means that it is often the preferred aesthetic (but not appropriate if our graphs have to be in black and white). Also, notice that we can easily combine two aesthetics over the same variable if we wanted. 12.3.1.2 The labs function We have seen that ggplot automatically creates a legend for its aesthetics and that it names the axes with a variable name ( axes x and y are also “legends” of sorts). If we want to manually name axes and legends, and perhaps add a title to the graph, we can use the labs function, which we also add as a new layer using the following syntax: ggplot(...) + ... + labs (x = &quot;x axis title&quot;, y = &quot;y axis title&quot;, title = &quot;Graph Title&quot;) Let’s try this in the next assignment. Assignment 12.6 - labs function # rename axes and legend of the following graph # and add a suitable title (something that succintly describes what the graph represents) ggplot(mtcars, aes (x = wt, y = mpg, color = cyl, shape = cyl)) + geom_point() ## Warning: Using shapes for an ordinal variable is not advised # rename axes and legend of the following graph # and add a suitable title (something that succintly describes what the graph represents) ggplot(mtcars, aes(x = wt, y = mpg, color = cyl, shape = cyl)) + geom_point () + labs (x = &quot;Weight / 1000 lb&quot;, y = &quot;Consumption / miles per gallon&quot;, color = &quot;Number of cylinders&quot;, shape = &quot;Number of cylinders&quot;, title = &quot;Heavier cars spend more fuel&quot;) ## Warning: Using shapes for an ordinal variable is not advised 12.3.2 Fixed geometry parameters Before proceeding, let us focus on one rather important thing that we have not yet clarified: what if we want to tweak certain parameters of a selected geometry but want to keep them fixed, instead of tying them to a particular aesthetic (i.e. mapping to a particular variable)? Or, specifically - what if we wanted to make a scatterplot which has red points, or “X” shapes - that is, fixed color and shape, rather than dependent on a variable? To achieve this, we simply initialize the parameters inside a function corresponding to the geometry we are adding, and we pay attention NOT to wrap the parameters inside the aes function (which automatically corresponds to “mapping”). The values to which we initialize these parameters have to be meaningful for that aesthetic (i.e. “color” can be set to \"red\" or \"#FF0000\"', \"shape\" to numbers from0to25` which correspond to a specific table of shapes etc.) The appropriate values of each aesthetic can be found in the official documentation. Correct example of changing the “global” color of points: ggplot(mtcars, aes(wt, mpg)) + geom_point(color = &quot;blue&quot;) Example which uses the wrong syntax: #ggplot will do the mapping of the word &quot;blue&quot; to the `color` aesthetic ggplot(mtcars, aes(wt, mpg)) + geom_point(aes(color = &quot;blue&quot;)) Assignment 12.7 - fixed geometry parameters # draw a scatterpolot showing the relationship between `wt` and `mpg` # make the points red, and set their shape to `4` and size to `3` ggplot(mtcars, aes(x = wt, y = mpg)) + geom_point(color = &quot;red&quot;, shape = 4, size = 3) 12.3.3 The aspects of statistics and position Let’s look again at the graph we keep drawing. What if instead of a scatterplot we opted for a different geometry, one that shows the relationship between weight and fuel consumption with connected lines? Try adding a line geometry layer to this graph. You can use the layer function with the geom parameter set to line, but the more popular approach is to use the geom_line helper function, which works similar to the geom_point function. Since we keep recycling the same foundation for our graph, we can first save it to an object before adding layers to it. Assignment 12.8 - adding a line layer # we store the foundation of our graph in a variable called `graph` graph &lt;- ggplot(mtcars, aes (x = wt, y = mpg)) # add the point geometry followed by the line geometry to this object and print it on screen # add the point geometry followed by the line geometry to this object and print it on screen graph + geom_point() + geom_line() We got what we asked for, but the result is not overly successful because this jagged line obscures rather than adds additional information. This type of data representation is popular for time series graph, but does not seem to be the correct choice for this dataset. What would probably be more appropriate for us is a line which showcases the relationship between weight and fuel consumption by simplifying it to a single, “smooth” line, that is, a line that approximates the positions of points that describe weight and consumption, rather than directly describing them. For this we need to get back to the we have met in our very first call to the ggplot function (more precisely, thelayer function). Those are aspects of statistics and position. Statistic is an aspect of the grammar of graphics that performs some additional calculations over a dataset before visualizing it. While we can always manually perform these calculations, usually it’s much more convenient to let them be calculated automatically, especially if said calculations are only for one-time use in our visualization (e.g. a bar chart which shows the frequencies of a categorical variable using the height of columns). Mostly statistics perform some sort of aggregation (although not always). Some of the more commonly used statistics are: count - counting occurrences (for categorical variables) bin - binning occurrences in trays and then counting (for continuous numerical variables) smooth - “smoothing”, i.e. “averaging” using the selected method (usuallylm for linear or loess for curved smoothing) unique - removing duplicates identity - direct mapping, i.e. “leaving it as is” These are just some of the statistics, and more can be found in the documentation. Each statistic has its own helper function that follows the form of stat_&lt;statistic_name&gt; and which creates its own layer related to that statistic. Let us return to our graph of weight vs consumption, but this time instead of adding a layer of linear geometry, we add a layer that will display “smoothing” (i.e. the smooth statistics). This is a good example of using how the statistic aspect works - instead of connecting the points directly with a line, a special function will “average” the values and then connect these averaged values with a (curved or straight) line. To achieve this, we leverage the stat_smooth function, which - if we use default parameters - will create a new layer with a “smoothed” representation of the y variable according to the x variable, using the so-called loess method, while also displaying the confidence intervals. Assignment 12.9 - linear smoothing # add the point geometry to the variable `graph` # then add a linear smoothing layer # use the `stat_smooth` function # with the `method` parameter set to `lm` (linear smoothing) graph + geom_point() + stat_smooth(method = &#39;lm&#39;) Assignment 12.10 - ‘loess’ smoothing # repeat the process but se the smoothing method to `loess` graph + geom_point() + stat_smooth(method = &#39;loess&#39;) Assignment 12.11 - the group aesthetic # create the same graph again # but while creating a smoothing layer # add the `group` aesthetic and set it to `cyl` # What have we achieved by doing this? graph + geom_point () + stat_smooth(aes(group = cyl), method = &#39;loess&#39;) In the last example we witnessed the so-called group aesthetic in action. It works similar to the group_by function in SQL or dplyr, that is, it performs aggregations (or more generally the statistic aspect) over the subgroups defined by the variable instead of the entire dataset. By using this aesthetic, we can show separate calculations in the graph according for each selected group, like in this example where the smoothing is done for each subgroup of cars which share the same number of cylinders. Certain visualizations will automatically set the “group” aesthetic when we put a categorical variable on some other aesthetic (like the “color” or “shape”). This can interfere with our statistic aspects we want to add, especially if we want to perform them over the entire dataset. In that case the easiest solution is to set the group aesthetic to the number 1, which will be interpreted as “there is just one big group” and will encompass the entire dataset when calculating the statistic aspect. 12.3.4 The relationship between geometry and statistics As a rule, certain geometries naturally use “their” statistics (for example, a bar chart naturally uses the statistics which counts the category frequencies). In practice, this means that when we add “statistical” layers, we can choose to use either the stat or geom helper function and the result will be identical, since both functions use the appropriate default geometry/statistics. For example, geom_bar uses the count statistic by default, and will give the same result as using stat_count which has the geometry set to bar. Which helper functions should we lean toward to then, stat orgeom? In practice, `geom ’functions are used somewhat more often, mostly because of the slightly more consistent syntax of graph construction, but ultimately the choice is irrelevant if the result is the same. Let’s now create a bar chart of the number of cylinders in cars described in the mtcars dataset. For this we willl use the geom_bar helper function and its preset count statistic. Assignment 12.12 - bar chart # draw a bar chart of the `cyl` variable from the `mtcars` table # use either the `geom_bar` or `stat_count` function # draw a bar chart of the `cyl` variable from the `mtcars` table # use either the `geom_bar` or `stat_count` function ggplot(mtcars, aes(x = cyl)) + geom_bar() We can use a similar representation for our numerical variable. Unlike a bar chart, which directly counts the occurencies of different categories, with numerical variables we first need to group values into so-called bins. After counting the number of observations in each bin we build a graph called a histogram. Again, this is done automatically using the bin statistic, and all we need to do is call the geom_histogram function and specify the number of bins. Assignment 12.13 - histogram # draw a histogram of the variable `wt` from the` mtcars` table # group the weights into 4 bins # use the `geom_histogram` function and the `bins` parameter ggplot(mtcars, aes(x = wt)) + geom_histogram(bins = 4) How does the statistical aspect work “under the hood”? As a rule, one or more new (most commonly aggregated) variables are calculated based on existing variables and then temporarily added as additional columns in the data frame. In the documentation we can find specific information about the names of these new variables and their meaning. For example, if we look at the documentation for the stat_bin function, we can see that it creates the variables named count, ncount, density and ndensity. Any of these variables can be used as a height attribute for our category columns, i.e. as a value to put on the y aesthetic. The reason why we did not explicitly state this aesthetic in the previous example is the fact that the statistics function automatically selects the aggregate function that is expected to be one most commonly used (in our case it was count). If we want to use a different aggregation variable, we can explicitly set the y aesthetic to one of the other variables, but we need to leverage the ggplot2 convention where we add a prefix and suffix .. to the names of such variables, like in this example: aes (x = hp, y = ..density ..) Let’s try this in the following assignment. Assignment 12.14 - explicitly specifying the aggregation variable # draw a histogram of the variable `wt` from the` mtcars` table # group the weights into 4 bins # use the `geom_histogram` function and the `bins` parameter # set `ncount` as the aggregation variable ggplot(mtcars, aes(x = wt, y = ..ncount..)) + geom_histogram(bins = 4) Let’s go one step further. Draw the same histogram (with default aggregation), but show how many cylinders are represented in each weight category. You can easily do this by adding a fill aesthetic to the geometry which will “fill” the columns with chosen color (as opposed to the color aesthetic that would paint the lines around a rectangles which represent columns in the bar chart). Assignment 12.15 - the fill aesthetic # draw a histogram of the variable `wt` # add the `cyl` variable on the `fill` aesthetic ggplot(mtcars, aes(x = wt, fill = cyl)) + geom_histogram (bins = 4) Here we see an example of a “combined” histogram - the histogram function will calculate the for each number of cylinders in each weight bin. One way this could be shown would be a three-dimensional graph, where base plane there would contain weight bins and cylinders, while the third dimension would be reserved for the height of the columns. However the ggplot2 is used for creating two-dimensional graphs, and projecting the aformentioned three-dimensional graph on the two-dimensional plane, would cause the columns to overlap. In order for the results to be effectively displayed on a two-dimensional graph, these columns are effectively “repositioned” so that the column for the individual cylinders is placed on top of one another within the same weight category, resulting in the histogram we see before us. In other words, the histogram function used the aspect of position to affect the visual representation of these variables. Position is an aspect of the grammar of graphics that allows you to “rearrange” the position of a particular aspect of a graph to achieve better representation clarity. In the previous example, we have already observed the “stratification” of columns based on a chosen categorical variable. Under the hood, the function actually used the stack positional aspect which stacks columns one on top of the other. An alternative to this might be to set the position aspect to dodge which would draw the subgroup columns next to each other. Assignment 12.16 - dodge positional aspect # draw the same histogram, but set the `position` aspect ratio to `dodge` ggplot(mtcars, aes (x = wt, fill = cyl)) + geom_histogram (bins = 4, position = &quot;dodge&quot;) Notice that by using dodge our histogram does not look like our initial histogram (with colored in ratios according to the categorical variable), but has allowed us a clearer view of the relationship between the representation of particular categories within each bin. What happens if we “turn off” the positional aspect by using the identity value? Assignment 12.17 - identity positional aspect # draw the same histogram, but set the `position` aspect ratio to `identity` # also set the `alpha` geometry parameter to `0.2` ggplot(mtcars, aes (x = wt, fill = cyl)) + geom_histogram(bins = 4, position = &quot;identity&quot;, alpha = 0.4) As we can see, the positional aspect of identity means “without repositioning”. The result is the pure projection of the aforementioned three-dimensional graph, which can only be seen by making the columns “transparent” and is not reallzy that useful to us compared to representations which use appropriate repositioning. Let’s show another positional aspect - fill (don’t confuse this with the fill aesthetic!). Assignment 12.18 - fill positional aspect # draw the same histogram, but set the `position` aspect ratio to `fill` # for better visibility, frame the rectangles with a black line # explain the result. What have we achieved with this histogram? ggplot(mtcars, aes(x = wt, fill = cyl)) + geom_histogram(bins = 4, color = &quot;black&quot;, position = &quot;fill&quot;) Another example of positional aspect is the addition of “noise” to the observations on a scatterplot to avoid overlap. By adding the jitter positional aspect, we can better visually communicate that there are more observations than it may appear. Jitterings is effective for smaller datasets; for larger we will often get better results by tweaking the transparency parameter, reducing the size of dots, or sampling the set before visualization. To demonstrate the jittering aspect in action, we will create an “artificial” data frame of 100 partially overlapping observations. Assignment 12.19 - jitter positional aspect df &lt;- data.frame( x = c(rep(1, 90), rep(2, 9), 3), y = c(rep(1, 70), rep(2, 25), rep(3, 5))) # create a scatterplot of the `df` data frame # create a scatterplot of the `df` data frame ggplot(df, aes(x = x, y = y)) + geom_point() Assignment 12.20 - jitter positional aspect (2) # display the same graph, but instead of using `geom_point` use `geom_jitter` # set `width` and` height` parameters to 0.3 (30% added noise) # additionally set the `color` geometry parameter to &#39;blue&#39; # and `alpha` parameter (transparency) to 0.4 ggplot(df, aes(x = x, y = y)) + geom_jitter(width = 0.3, height = 0.3, alpha = 0.4, color = &quot;blue&quot;) 12.3.5 Storing a graph to a file We often need to save our graph to a file so that it can be easily embedded in another report document, scientific paper, forwarded by email, etc. By default, R uses the screen as a “graphical device”. Optionally, we can “redirect” the graphics to another “device”, usually a file of a certain type (png,tiff, pdf, etc.). The list of all options can be viewed with the command ?Devices. The png andtiff formats are recommended for storing graphs in raster format, while it is common to use pdf for vector format. Saving graphs can be done by calling the function corresponding to the format in which we want to store the image (eg the pdf function will save the next image to a pdf file), but the ggplot2 package offers a more convenient way - the ggsave function will store the last drawn graph into a file of the selected name, whereby the image format will be deduced from the file extension we choose. This method is better because we have the chance to see the graph first and then decide on storage. Assignment 12.21 - saving graph to file # save the following graph in the files `figures1.pdf` and` figures1.png` ggplot(mtcars, aes(x = hp, y = mpg, col = as.factor (cyl))) + geom_point() + geom_smooth (aes(x = hp, y = mpg), method = &#39;loess&#39;, linetype = 4, color = &quot;gray&quot;, se = F, inherit.aes = F) + labs(x = &quot;horsepower&quot;, y = &quot;fuel consumption&quot;, col = &quot;number of cylinders&quot;) ggsave(&quot;figure1.pdf&quot;) ggsave(&quot;figure1.png&quot;) 12.3.6 Aspects of scale, coordinate system and theme We have learned so far that creating ggplot graphs often boils down to mapping the dataset columns to the aesthetics of the graph. Scales are an aspect that controls the specifics how this mapping is performed, that is, the method of mapping the data itself to the visual elements of aesthetics. In the case of coordinate axes, it is a matter of mapping numerical or categorical values to specific distances on the axes themselves, while for for the color aesthetic, the scale decides which color corresponds to which value of the original data. The scale aspect is also the basis for creating a legend for the graph. This aspect has always been implicitly present so far, but we have allowed the ggplot2 package to choose the default values for us. In the general case, ggplot2 controls the aspect of the scale relatively well regardless of the aesthetics we use, but very often we want to be able to, for example, change the range of values contained in the graph, labels on axes, colors or shapes used etc. In this lesson, we will focus only on the scales that are relatively common in practice. Other functions and options related to the aspect of scale can be found in the official documentation. As with the aspects of geometry and statistics, when working with the scale aspect, we most often use the helper functions which look like this (* represents the name of aesthetics, such as x,y, color etc.): scale_*_continuous - for mapping continuous (numeric) values scale_*_discrete - for mapping discrete values Each of these functions has a number of parameters that we can use to influence the mapping process. For example, if we look at the documentation for scale_x_continuous we can see that we can set parameters for, among other things: name - the name of the scale which also becomes the name of the axis / legend breaks - the positions where ticks are placed labels - the labels which will be printed below the ticks limits - the range of values that will be on the axis etc. It is important to note that ggplot2 has a lot of additional helper functions that allow us to adjust some parts of the scale aspect, such as the already seen labs function for renaming the graph title and labels on the x and y axis. Another example are also xlim and ylim which only affects the axis range, etc. It is often worth to check out the documentation, and pick and choose the functions which will help our work the most by shortening the syntax for the visualization tasks we perform most often. Let’s try to leverage the scaling aspect in the following assignments. We will use a new dataset - diamonds from the ggplot2 itself. This dataset describes various features of diamonds along with their estimated value. Assignment 12.22 - getting acquainted with diamonds dataset # briefly explore the `diamonds` data frame # briefly explore the `diamonds` data frame glimpse(diamonds) head(diamonds) ## Observations: 53,940 ## Variables: 10 ## $ carat &lt;dbl&gt; 0.23, 0.21, 0.23, 0.29, 0.31, 0.24, 0.24, 0.26, 0.22, ... ## $ cut &lt;ord&gt; Ideal, Premium, Good, Premium, Good, Very Good, Very G... ## $ color &lt;ord&gt; E, E, E, I, J, J, I, H, E, H, J, J, F, J, E, E, I, J, ... ## $ clarity &lt;ord&gt; SI2, SI1, VS1, VS2, SI2, VVS2, VVS1, SI1, VS2, VS1, SI... ## $ depth &lt;dbl&gt; 61.5, 59.8, 56.9, 62.4, 63.3, 62.8, 62.3, 61.9, 65.1, ... ## $ table &lt;dbl&gt; 55, 61, 65, 58, 58, 57, 57, 55, 61, 61, 55, 56, 61, 54... ## $ price &lt;int&gt; 326, 326, 327, 334, 335, 336, 336, 337, 337, 338, 339,... ## $ x &lt;dbl&gt; 3.95, 3.89, 4.05, 4.20, 4.34, 3.94, 3.95, 4.07, 3.87, ... ## $ y &lt;dbl&gt; 3.98, 3.84, 4.07, 4.23, 4.35, 3.96, 3.98, 4.11, 3.78, ... ## $ z &lt;dbl&gt; 2.43, 2.31, 2.31, 2.63, 2.75, 2.48, 2.47, 2.53, 2.49, ... ## # A tibble: 6 x 10 ## carat cut color clarity depth table price x y z ## &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.23 Ideal E SI2 61.5 55 326 3.95 3.98 2.43 ## 2 0.21 Premium E SI1 59.8 61 326 3.89 3.84 2.31 ## 3 0.23 Good E VS1 56.9 65 327 4.05 4.07 2.31 ## 4 0.290 Premium I VS2 62.4 58 334 4.2 4.23 2.63 ## 5 0.31 Good J SI2 63.3 58 335 4.34 4.35 2.75 ## 6 0.24 Very Good J VVS2 62.8 57 336 3.94 3.96 2.48 This dataset has around 54,000 observations which might slow down the drawing of our graphs somewhat. Let’s reduce it by sampling 5000 random observations. Assignment 12.23 - sampling the diamonds data frame set.seed(1001) # sample 5000 random rows from `diamonds` data frame # and store them in a data frame called `diamondsSample` set.seed (1001) # sample 5000 random rows from `diamonds` data frame # and store them in a data frame called `diamondsSample` diamondsSample &lt;- sample_n(diamonds, 5000) Let’s now take a scatterplot which shows the diamonds` volume, price and color and fine-tune its aesthetics with the help of the scale aspect. Assignment 12.24 - tweaking the scaling aspect of aesthetics # &quot;fix&quot; the axes and the legend of the graph # - call the x and y axes &quot;volume in mm3&quot; and &quot;price in $&quot; # - call the color legend &quot;color quality&quot; # - limit x axis from 0 to 500 # - set the y axis breaks on 1000, 5000, 10000 and 20000 # - instead of D-J label the color &quot;quality&quot; as numbers from 1 to 7 ggplot(diamondsSample, aes (x * y * z, price, color = color)) + geom_point (alpha = 0.4) ggplot(diamondsSample, aes(x * y * z, price, color = color)) + geom_point(alpha = 0.6) + scale_x_continuous(name = &quot;volume in mm3&quot;, limits = c (0, 450)) + scale_y_continuous(name = &quot;price in $&quot;, breaks = c (1000, 5000, 10000, 15000)) + scale_color_discrete(name = &quot;color quality&quot;, labels = 1:7) ## Warning: Removed 7 rows containing missing values (geom_point). Notice that ggplot will report if some observations are not shown due to unavailable data (NA values in variables we want to depict). To prevent this warning, it is sufficient to add the argument na.rm = T to the geometry layer. It is very common to notice “an exponential trend” in our data - in other words, that the relationship between two variables reminds us of an exponential function. The previous graph is also a lesser example of such a scenario - it can be observed that the price of diamond initially only “slightly” increases with the diamond volume, and later it starts to rise more steeply. In such cases data analysts often make a decision to transform the data - for example, in our case we could use the natural logarithm of the price instead of its absolute value, to try and turn the exponential trend into a linear one, which we generally prefer. However, the transformation of price into logarithms carries with it a problem of interpretation - a graph is easier to interpret if it contains original data values instead of derived ones (1000 dollars versus 3 “dollar logarithms”). Scales can help us in this case - instead of changing the data, we simply switch to a different scale. Specifically, instead of the scale_*_continuous function we opt for: scale_*_log10 - uses a logarithm scale (base 10) scale_*_reverse - flips the scale from right to left (or down to up) scale_*_sqrt - uses square roots of values Assignment 12.25 - logarithmic scale # change the y axis to a logarithmic one ggplot(diamondsSample, aes (x * y * z, price, color = color)) + geom_point (na.rm = T, alpha = 0.6) + scale_x_continuous (name = &quot;volume in mm3&quot;, limits = c (0, 450)) + scale_y_continuous (name = &quot;price in $&quot;, breaks = c (1000, 5000, 10000, 15000)) + scale_color_discrete (name = &quot;color quality&quot;, labels = 1: 7) # change the y axis to a logarithmic one ggplot(diamondsSample, aes (x * y * z, price, color = color)) + geom_point (na.rm = T, alpha = 0.6) + scale_x_continuous (name = &quot;volume in mm3&quot;, limits = c (0, 450)) + scale_y_log10 (name = &quot;price in $&quot;, breaks = c (1000, 5000, 10000, 15000)) + scale_color_discrete (name = &quot;color quality&quot;, labels = 1: 7) One thing we often like to “fix” on graphs is colors - whether we want to better emphasize the information that the graph conveys, better fit the graph into the environment we would ultimately place it with, or simply want the graph to use colors that we find more aesthetically pleasing. This is why the color andfill aesthetics are very often further adjusted by scales. We have a good set of helper functions for this (we give examples for the fill aesthetics although similar functions also exist for color): scale_fill_brewer - selecting one of the pre-prepared color palettes for displaying discrete values; pallete names can be viewed in the documentation scale_fill_distiller - adjusts palettes for discrete values to continuous variables scale_fill_gradient - select start and end colors that will then “blend”into each other; we useit to display continuous values scale_fill_gradient2,scale_fill_gradientn - same as above but with more colors scale_fill_grey - for black and white visualizations Assignment 12.26 - color adjustment on graph # adjust the `fill` aesthetic of the next graph by using # the `scale_fill_brewer` function # Set the `pallet` parameter to one of the following pallets: # `Blues`, `BuPu`, `Greens`, `Grays`, `Oranges`, `OrRd`, `PuBu`, # `PuRd`, `Purples`, `YlGn`, `YlOrRd` # (more palletes can be found in the documentation) ggplot(diamondsSample, aes(x = x * y * z, fill = color)) + geom_histogram(bins = 30, na.rm = T) + xlim(0, 500) # adjust the `fill` aesthetic of the next graph by using # the `scale_fill_brewer` function # Set the `pallet` parameter to one of the following pallets: # `Blues`, `BuPu`, `Greens`, `Grays`, `Oranges`, `OrRd`, `PuBu`, # `PuRd`, `Purples`, `YlGn`, `YlOrRd` # (more palletes can be found in the documentation) ggplot(diamondsSample, aes(x = x * y * z, fill = color)) + geom_histogram(bins = 30, na.rm = T) + xlim(0, 500) + scale_fill_brewer(palette = &quot;Greens&quot;) # adjust the `fill` aesthetic of the next graph by using # the `scale_fill_brewer` function # Set the `pallet` parameter to one of the following pallets: # `Blues`, `BuPu`, `Greens`, `Grays`, `Oranges`, `OrRd`, `PuBu`, # `PuRd`, `Purples`, `YlGn`, Y`lOrRd` # (more palletes can be found in the documentation) ggplot(diamondsSample, aes(x = x * y * z, fill = color)) + geom_histogram(bins = 30, na.rm = T) + xlim(0, 500) + scale_fill_brewer(palette = &quot;YlOrRd&quot;) The coordinate system aspect is somewhat more rarely used than other grammar of graphic concepts we covered. The reason is that in most cases we want to use Cartesian coordinate system which is used by default. If we feel that our visualization requires something else - whether it is a polar coordinate system, a “sideways” Cartesian system aside, or - which is especially important when analyzing geographic data - a map representation of our data, we can use the following functions: coord_polar - polar coordinate system coord_flip - changes the x and y axes coord_map - uses maps from maps and mapproj packages Unfortunately, there are currently no maps of the Republic of Croatia yet, but more ambitious readers can try to create one (and share with the local R community) by following the instructions at this and this links. Assignment 12.27 - inverted and polar coordinate systems # change the coordinate system to &quot;inverted&quot; and then to &quot;polar&quot; for the following graph ggplot(diamondsSample, aes(x = x * y * z, fill = color)) + geom_histogram(bins = 30, na.rm = T) + xlim(40, 100) # change the coordinate system to &quot;inverted&quot; and then to &quot;polar&quot; for the following graph ggplot(diamondsSample, aes(x = x * y * z, fill = color)) + geom_histogram(bins = 30, na.rm = T) + xlim(40, 100) + coord_polar() # change the coordinate system to &quot;inverted&quot; and then to &quot;polar&quot; for the following graph ggplot(diamondsSample, aes (x = x * y * z, fill = color)) + geom_histogram(bins = 30, na.rm = T) + xlim(40, 100) + coord_flip() One thing to remember is that the xlim andylim parameters work differently depending on whether we use them in terms of scale or the coordinate system; limiting the axis in aspect of the scale will “throw out” observations beyond the specified interval, which can affect the calculation of summary statistics, smoothing curves, etc. On the other hand, the limits given in aspect of the coordinate system will simply “zoom in” the graph to the selected area but will retain all observations when it comes to calculations. Both options are useful, depending on whether we want a graph that displays all the relevant data for a given visualization or a graph that represents an enlarged segment of another graph. Finally, the theme aspect allows us to influence all visual aspects of the graph that are not related to the data. This means we can choose the background color and layout, font type and size, margins, alignments, and many other graph parameters. The theme aspect gives us extremely detailed control over the appearance of the graph, and since the theme itself is actually an object (of class theme), it can be stored in a variable and then easily recycled for all subsequent visualizations. Likewise, ggplot2 offers a number of predefined themes for use and further customization, which we easily retrieve with a set of helper functions, some of which are: theme_gray - default theme theme_bw - black and white axes, projector-friendly theme_classic - “classic” theme similar to the one produced by the default plot function theme_void - “empty” theme Assignment 12.28 - the theme aspect # change the theme of the following graph to `theme_classic` ggplot(diamondsSample, aes(x = x * y * z, fill = color)) + geom_histogram(bins = 30, na.rm = T) + xlim (0, 500) # change the topic to the next graph of your choice ggplot(diamondsSample, aes(x = x * y * z, fill = color)) + geom_histogram(bins = 30, na.rm = T) + xlim(0, 500) + theme_classic() Often we want to change only some of the theme-related aspects (i.e. size or orientation of letters, appearance of ticks on axes, etc.). For these things we use the theme function, which contains a very rich set of parameters easily found by checking out the documentation. Some of these parameters are objects called “theme elements” (e.g. element_line,element_text) that we set by calling the associated function within the theme function call, for example: # changing the visual appearance of the graph title # (recommended for font family to use # `serif`,` sans` or `mono`) ... + theme(title = element_text(family = &#39;serif&#39;, face = &#39;bold.italic&#39;)) Assignment 12.29 - modifying theme elements # change the orientation of the letter on the x axis # so that they are at an angle of 45 degrees # search for help in the official documentation if needed ggplot(diamondsSample, aes(cut)) + geom_bar() ggplot(diamondsSample, aes(cut)) + geom_bar() + theme (axis.text.x = element_text(angle = 45)) 12.3.7 Conditional (faceted) graphs We have previously mentioned the group aesthetic which, prior to visualization within a dataset, groups data into subsets defined by the selected grouping variable and then visualizes them accordingly. We have also learned that we can do implicit groupings using aesthetics of color, shape, size, etc. What is common to these principles is that visual separation, depending on a variable, is performed inside the same graph. In other words, we can use different aesthetics to represent several different visualizations within a single graph. Conditional (faceted) graphs work on the same principle, but separation by a selected variable (or variables) is done by visualizing multiple graphs which are then displayed side by side. The resultant graphs present the same information as the aesthetics of the (implicit or explicit) group, but in a way which makes it somewhat easier to study and compare each subgraph separately. Before demonstrating how to create conditional graphs, we need to perfom a brief detour and explain the term called “statistical formula notation”. This notation is often used in R, especially when training various statistical models, and is actually a way to concisely write a particular, formula-based relationship between variables of the dataset, and then incorporate it into the program code. The formulas will be discussed in more detail later, and for now we will restrict ourselves to a handful of very simple examples. If we want to write a simple formula of “y depending on x,” then the final formula looks like this: y ~ x # means &quot;y depending on x&quot; we can also read this as “y as a function of x” or - in the case of linear models - a more concise notation of y = ax + b. Hence, we read the tilde sign (~) as “depending on”. Here are some more simple formulas: # z depending on x and y (`+` means &quot;and&quot;, not arithmetic sum!) z ~ x + y # y depending on &quot;all other variables&quot; y ~ . # &quot;all other variables&quot; depending on y . ~ y # so-called &quot;one-sided&quot; formula, shorthand for &quot;depending on y&quot; ~ y The last two examples are a little harder to define mathematically, but are often used in function calls for different purposes, mostly because of their simple notation and easy interpretation. Let us now return to the conditional graphs. There are two basic ways to create conditional graphs, by using the functions: facet_grid - for organizing subgraphs into a grid (matrix) facet_wrap - for organizing subgraphs into one or more rows We commonly use the facet_grid function when we do subsets according to one or two categorical variables. Splitting two variables naturally does a “matrix” while splitting them one by one will make either a row or column, the choice we control by choosing a specific formula. On the other hand, we use the facet_wrap function when we want to create subgraphs only according to a single variable, but (due to a potentially larger number of categories) we want the subgraphs to span multimple rows (similar to how “word wrap” breaks sentences in text editors). Assignment 12.30 - facet_grid function # let&#39;s reduce the number of levels first to make the visualization simpler diamondsSample %&gt;% filter(color %in% c(&quot;G&quot;, &quot;H&quot;, &quot;I&quot;, &quot;J&quot;), cut %in% c(&quot;Very Good&quot;, &quot;Premium&quot;, &quot;Ideal&quot;)) -&gt; diamondsSample2 # turn the following graph into a conditional graph # splitting graphs by all combinations of # colors (`color`) and cuts (`cut`) # use the `facet_grid` function and # an appropriate statistical formula as its parameter ggplot(diamondsSample2, aes(depth, fill = clarity)) + geom_histogram(bins = 5, position = &#39;dodge&#39;) ggplot(diamondsSample2, aes(depth, fill = clarity)) + geom_histogram(bins = 5, position = &#39;dodge&#39;) + facet_grid(color ~ cut) Assignment 12.31 - facet_grid function (2) # repeat the same as above, but now group only by color # arrange subgraphs into a column # use the `facet_grid` function and formula notation with a dot ggplot(diamondsSample2, aes(depth, fill = clarity)) + geom_histogram(bins = 5, position = &#39;dodge&#39;) ggplot(diamondsSample2, aes(depth, fill = clarity)) + geom_histogram(bins = 5, position = &#39;dodge&#39;) + facet_grid(color ~ .) Note one important feature of conditional graphs - the coordinate axes (ie scales) are aligned. Specifically, before visualization, ggplot first check the scales of subgraphs in order to find the maximum range, which is then used as a common feature of all graphs. This is usually a preferred, feature, but if we do not want it, we can “free” a particular axis (or both) by setting the scales parameter free (for both axes) or free_x / free_y (if we only want one axis aligned and the other “free”). Let’s now take a look at how the facet_wrap function works: Assignment 12.32 - facet_wrap function # break the following graph into subgraphs based on transparency (`clarity`) # use the `facet_wrap` function and a one-sided formula ggplot(diamondsSample2, aes(x * y * z, price)) + geom_point() # break the following graph into subgraphs based on transparency (`clarity`) # use the `facet_wrap` function and a one-sided formula ggplot(diamondsSample2, aes(x * y * z, price)) + geom_point() + facet_wrap(~clarity) 12.4 Graphs in Exploratory Analysis and Reporting Before we finish the chapter, we can briefly look at the difference between graphs made for exploratory data analysis and those we use in reports, to communicate our conclusions to other people, and to share our findings. Exploratory data analysis often aims for efficiency and quantity. It has already been said that exploratory analysis comes down to answering a series of questions that an analyst asks regarding data. How pretty the graph looks is often irrelevant - the main thing is that graphs have enough information to be able to answer the questions being asked and offer foundations for setting up various hypotheses. The analyst will also often try and experiment with different combinations of aesthetics, geometries and statistics. Sometimes it is convenient to go beyond what ggplot2 package offers and use additional packages that are more suitable for what the analyst wants to achieve. For example, the ggpairs function of the GGally package allows pairing of multiple variables of a data frame and creating a matrix of single and two-variable graphs: #library (GGally) #if necessary # We only take four columns for a clearer view ggpairs(data = diamondsSample[, c (1, 2, 5, 7)]) Exploratory graphs are used by the analyst to visualize the data for themselves, to look for patterns, to recognize distributions or natural groupings of data, or to spot outliers. Likewise, exploratory graphs are often used to quickly gain insight into various aggregate statistics. In the case of predictive data analysis, the analyst will often create some simpler predictive models during the exploratory analysis, which they will then visualize on the graph to be able to more accurately evalute the models’ performance, and based on the identified shortcomings, develop a strategy for further steps of the analysis. For reporting, on the other hand, the quality of the graph is crucial, in terms of presenting information clearly and transparently. The graph must be easy to readily interpret, with carefully chosen explanations and selected use of the added metadata, most commonly in the form of additional text and annotations. To create such graphs, it is recommended to use additional geom_text layers for text tags in the corresponding areas of the graph where they can contribute most, as well as creative use of geom_point, geom_hline,geom_vline, geom_rect and similar geometric layers that will further clarify certain segments of the graph. It is also recommended to prepare a specific graph theme object in advance, which will then be consistently applied to all graphs. In many cases, the graphs used in reporting are a subset of chosen and “beautified” graphs obtained during exploratory analysis. However, the analyst should pay particular attention to the fact that graphs in reports are often made for an audience that is far less familiar with the dataset and the various in-depth knowledge that the analyst has obtained during exploratory analysis. Reporting graphs must therefore be end-user oriented and carefully designed for this purpose. Therefore, it is recommended that all elements of the graph - including the title, legends, etc., be oriented to the communication of information and clarification of what the graph is showing, which is in line with the conclusions drawn by the publication the graph is a part of. Sometimes, due to a lack of space or simply as a presentational choice, we want to put more different graphs inside a single image. We usually do this by storing separate graphs each in its own image file and then collating those images within the interface we use to write a publication containing those graphs. However, we can also combine graphs in advance using the gridExtra package. Among other things, this package offers the grid.arrange function with the help of which graphs are placed into a matrix with the selected number of rows and columns. #library (gridExtra) # if necessary # we first store graphs into separate variables g1 &lt;- ggplot(diamondsSample, aes (x * y * z, price, color = color)) + geom_point(alpha = 0.6) g2 &lt;- ggplot(diamondsSample, aes (x = x * y * z, fill = color)) + geom_histogram(bins = 30, na.rm = T) + scale_fill_brewer (palette = &quot;Greens&quot;) g3 &lt;- ggplot(diamondsSample, aes (x = cut)) + geom_bar(fill = &quot;blue&quot;, alpha = 0.5) g4 &lt;- ggplot(diamondsSample, aes (x = color, fill = clarity)) + geom_bar() + coord_polar() # and then we call the `grid.arrange` function grid.arrange(g1, g2, g3, g4, nrow = 2, ncol = 2) This concludes the story of the grammar of graphics, its aspects and its application with the help of the ggplot2 package. Not all of the features of this package are fully explained here, which is why it is strongly recommended that you read the documentation further and expand your knowledge beyond what was covered in this chapter. Also, one should not forget about a number of additional packages that further extend the capabilities of the ggplot2 package, which we should look for depending on our requirements and desires regarding the visualizations we want to create for our data analysis projects. For inspiration, it’s convenient to look at the gallery of graphs and images created using the R language, available at this link . Exercises Load the diamonds data frame which comes with the ggplot2 package. Show the diamond price distribution with the aid of two graphs - a histogram and the so-called. frequency polygons (function geom_freqpoly)). Divide the prices into 10 bins. add the “clarity” of the diamond to the graphs from a) (the clarity column) which you will set to the fill aesthetics (‘histogram’) or color aesthetic (frequency polygons). Do you see any difference pertaining to the default position aspect? Load the mpg dataset. Then find the graphs called 2a.pdf, 2b.pdf and 2c.pdf in the folder of this homework assignment. Try to reconstruct them. If you do not recognize the geometry used, try to consult the RStudio ggplot2 cheat sheet. Adjust the scale aspect of the following graph in the following fashion: the x axis should be called \"the number of cylinders\" the y axis should be called \"total\" and should have its range expanded to 100 the legend should be named \"year\" the rectangles color should use the Dark2 palette ggplot(mpg, aes(x = as.factor(cyl), fill = as.factor(year))) + geom_bar() Change the theme of the next graph as follows: use the theme which is suitable for use with a projector turn the x axis labels vertically ggplot(mpg, aes(x = as.factor(trans), y = displ)) + geom_boxplot() The following graph shows the highway miles per gallon consumption histogram, with the color of the rectangle reflecting the number of cylinders. Try to improve the graphs’ interpretability by replacing the aesthetics of color and using multiple graphs that are conditioned by the number of cylinders. Organize the graphs in a 2 x 2 matrix. ggplot(mpg, aes(hwy, fill = as.factor(cyl))) + geom_histogram(bins = 10, position = &quot;dodge&quot;) Assume you have the following data frame: sales &lt;- data.frame(month = 1:12, total = c(10000, 5000, 12000, 3000, 5000, 7000, 10000, 2000, 4000, 8000, 11000, 14000)) and you visualize it with a bar graph. However, the geom_bar function by default uses only one variable and automatically calculates the count statistics. How to solve this problem? Suggest a solution and create the appropriate bar graph. Programirajmo u R-u by Damir Pintar is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License.Based on a work at https://ratnip.github.io/FER_OPJR/ #(PART) Machine Learning and Predictive Analytics {-} "],
["regression.html", "13 Selected Machine Learning Methods: Regression Analysis 13.1 Machine learning 13.2 Multiple linear regression", " 13 Selected Machine Learning Methods: Regression Analysis 13.1 Machine learning Machine learning is a field of computer science that deals with a specific type of programming in which we do not give explicit instructions to the computer, but rather expect the computer to come to certain insights on its own based on selected datasets and a particular “learning” method. Machine learning is often broadly divided into so-called supervised learning, where we have clearly defined inputs and outputs(goals), and unsupervised learning, where we do not have predefined outputs, but expect the computer to analyze only they enter to come up with some useful insights into the data itself. There are additional machine learning disciplines such as reinforced learning, anomaly detection, etc. which we will not further explain as they fall outside the scope of this textbook. The term machine learning is often associated with the term “data mining” and “knowledge discovery from datasets”(KDD - knowledge discovery from data). It is difficult to determine the extent to which these terms overlap and what their exact definitions are, since various conventions that are not mutually consistent are often encountered in the literature. For the purposes of this tutorial, machine learning can be considered as a kind of “toolset” that we can use to discover(mine) useful information from datasets. The result of this process gives us some knowledge of the domain we can use to make certain decisions. In this lesson, we will focus on one very commonly used machine learning method - linear regression - with emphasis on its use in the programming language R. Linear regression is an extremely popular method of machine learning that, with the help of mathematical and statistical foundations, describes the potential linear relationship of dataset variables. If sufficient evidence is found that a linear relationship exists, then we have a potentially useful insight into the actual relationship of the two(or more) variables. Likewise, the resulting “formula” can be used to estimate the value of another(target) variable based on the known value of one variable(predictor). Although more advanced predictive methods exist today, linear regression is still often used both for its simplicity and for its fact, understanding linear regression lays a solid foundation for learning and understanding more advanced machine learning methods. Because of this, linear regression is a logical and very important first step when entering the field of predictive analysis. 13.1.1 Simple linear regression Simple linear regression is a method of supervised machine learning for predicting a target numeric variable using a linear function of the input variable. In this way, the creation of a predictive model is reduced to the process of determining the direction coefficient of direction and intercept value, which will form a simple formula for calculating the target variable using the input parameter. Since this method boils down to estimating the above parameters, the linear regression method belongs to so-called “parametric methods” of machine learning, i.e. predictive analysis. Parameters of simple linear regression can often be determined mathematically. The most commonly used method of determining the direction and section coefficient is the least squares method, which draws a line between the points of the graph so that the sum of squares of the distance between estimations and real values is minimal (these distances are called “residuals”). Using a mathematical procedure, we can derive a formula that will calculate these parameters exactly, provided we have sufficient number of observations. The motivation to perform simple linear regression is often found during the process of exploratory data analysis, especially during the visualization of two numerical variables by using scatterplots. If one of these variables is of interest to us as the target variable of the predictive model, and putting another variable on scatterplot results in a shape resembling a line, then this other variable is an obvious candidate for the simple linear regression method. In the following exercise, we will use an “artificial” data frame in which we have an input variable x and four possible target variables y1, y2, y3 and y4. Each of these variables is created by a specific input transformation with the addition of a certain amount of noise. The idea of ​​the assignment is to study the relationship between input and possible output variables and to identify which of these relationships is a good candidate for simple linear regression. Assignment 13.1 - suspecting potential linear relationships # load data from the` data1.csv` file # in a variable called `df` # draw four scatterplots of relationships # between cariax with every single variable y in the data box above # Add to each graph a smoothing geometry, the `lm` method # answer the questions: # in which graphs do you see a possible linear relationship between the variables? # which graph shows nonlinear connection? # for which graph could you say the variables are independent? df &lt;- read.csv(&quot;data1.csv&quot;, stringsAsFactors = F, encoding = &quot;UTF-8&quot;) g1 &lt;- ggplot(df, aes(x, y1)) + geom_point() + geom_smooth(method = &#39;lm&#39;) g2 &lt;- ggplot(df, aes(x, y2)) + geom_point() + geom_smooth(method = &#39;lm&#39;) g3 &lt;- ggplot(df, aes(x, y3)) + geom_point() + geom_smooth(method = &#39;lm&#39;) g4 &lt;- ggplot(df, aes(x, y4)) + geom_point() + geom_smooth(method = &#39;lm&#39;) grid.arrange(g1, g2, g3, g4) Visualizing the relationship between two variables can usually provide us with relatively good intuition when we can expect a linear model to work relatively well. If we want to numerically describe the power of the linear relationship of two variables, we can use a so-called “Pearson correlation coefficient” which will always the value from the interval [-1,1], where -1 means complete “negative” correlation, 1 complete positive, and 0 that there is no correlation. In R, we can get this coefficient easily with the help of the cor function. Let’s calculate the correlation coefficient for all the pairs of input and output variables shown in the graphs from the previous exercise. Assignment 13.2 - calculating the correlation coefficient # calculate and print the Pearson&#39;s correlation coefficient # between all pairs visualized in the previous exercise cor(df$x, df$y1) cor(df$x, df$y2) cor(df$x, df$y3) cor(df$x, df$y4) ## [1] 0.9758326 ## [1] 0.6765991 ## [1] -0.04977038 ## [1] 0.1783745 In addition to this measure, to describe the power of a linear relationship, we often use a variable called the “coefficient of determination”, more commonly known as the “R squared” measure. The name of this variable is derived from the fact that the correlation coefficient for a simple linear model (which we calculated in the previous task) is often called R, and the value of R squared is exactly equal to its square for a simple linear model. This measure can take values between 0 and 1, where a value close to 1 indicates a near-perfect linear relationship while a value close to 0 indicates its absence. The “R squared” measure is actually defined as “the amount of variability explained by the model”. We can simply interpret this as follows - we compare how “scattered” the points are around an imaginary line (of linear regression) compared to their “general scatter” around a horizontal line that passes through their arithmetic mean. It is important to note that the “R squared” is one of the more important criteria for evaluating the quality of a linear model and as such is often contained in the description of model results. Despite this, there is no clear concensus on what constitutes a “good R squared” value - even a model with a small “R squared” can prove useful, depending on other criteria and the specific circumstances where the model is applied. Now let’s show how we create linear models in the R language. 13.1.2 The lm function In R, we create simple linear models using the lm function, which is short for linear model. This function has a number of parameters, and we will use the most important ones - the statistical formula which defines how the model should be trained, and the data set we train on: lm(formula, data) The “formula” is simply a short notation which describes how the left side of the formula “depends” on the right side of the formula. If we want to train a linear regression model where the target variable y depends on the variablex for the data frame df, and save the final model in the variable linMod, then in the code may look like this: linMod &lt;- lm(y ~ x, data = df) Let’s try this on our own in the next task. Assignment 13.3 - creating a simple linear model # Use the `lm` function to create a linear data model for the `df` dataset # where `x` is the input and `y1` is the output variable # Save the result to the `linMod` variable # print the `linMod` variable # Use the `lm` function to create a linear data model for the `df` dataset # where `x` is the input and `y1` is the output variable # Save the result to the `linMod` variable linMod &lt;- lm(y1 ~ x, data = df) # print the `linMod` variable linMod ## ## Call: ## lm(formula = y1 ~ x, data = df) ## ## Coefficients: ## (Intercept) x ## 46.733 3.999 The output of the linMod variable shows us the formula used to create the model and the calculated parameters - direction coefficient and the intercept (which we are generally far less interested in). The direction coefficient is interpreted as follows - if the input variables changes for one measure, the output changes by the amount of the coefficient. The linMod variable from the previous example is an object of class lm. This is a relatively complex object which contains not only the finally calculated coefficients, but also a rich set of information related to the linear model created, which includes even the data set by which the model was created. To get all this information, we can use the following set of helper functions: coef - returns coefficients in the form of vectors fitted.values - returns the prediction vector obtained by applying the model to the set by which the model was created residuals - returns the error vector obtained by applying the model to the set by which the model was created summary - provides a summary of the most important model information predict - applies the model to the new dataset (shown later) Let’s try the summary function onr our linear model. Assignment 13.4 - linear model summary # execute the `summary` function on the trained linear model summary(linMod) ## ## Call: ## lm(formula = y1 ~ x, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -74.010 -15.492 -1.021 15.613 77.371 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 46.73328 5.26089 8.883 3.2e-14 *** ## x 3.99851 0.09045 44.208 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 26.35 on 98 degrees of freedom ## Multiple R-squared: 0.9522, Adjusted R-squared: 0.9518 ## F-statistic: 1954 on 1 and 98 DF, p-value: &lt; 2.2e-16 We see that we have received a wealth of information related to our trained model. Below, we will offer a simple, layman’s explanation on how to interpret this information. To begin with, we note that when creating a linear model from some data, we have several different “uncertainties” in the result: Is there a linear trend at all, or could the observed collinearity occur at random? If there is a trend, how confident are we that the calculated direction coefficient corresponds to the “real” coefficient? Finally, if a trend exists and we have managed to estimate the “true” coefficient well enough, how much does the additional “noise” affect the accuracy of the predictions? The summary below provides an answer to some of these questions. Let’s first look at the columns t value andPr(&gt;|t|), that is, their values concerning our input variable x. They answer the question - do we even have enough evidence to suggest that there is a linear relationship between inputs and outputs? The answer to this question is given by the result of so-called t - test, which a measure of the strength of evidence against the null hypothesis that states that there is no linear relationship at all and that the direction coefficient is actually zero. The value under Pr(&gt;|t|) is a so called “p-value”, i.e. the estimate of the probability that collinearity was observed by chance, and here we see that it is extremely small. The stars next to this value help us to quickly interpret the coefficients for which we are relatively certain to have a linear relationship with the target variable. If we are satisfied with the proof that the direction coefficient is non-zero, then we can also look at the column Std. Error that describes how “confident” we are in the resulting coefficient of direction. Statistically, with the help of the so-called “central theorem”, we believe with 95% certainty that we can say the “real” coefficient is in the interval between two standard errors from the coefficient obtained. The standard error depends on the number of observations in our set - simply stated, the more data we have, the more “confident” we will be in our estimated coefficient. The most interesting information from the summary is the Residual standard error and Adjusted R-squared entries. The “residual” is actually the difference between the prediction given by the model and the actual value observed. The residual standard error is an estimate of how much (on average!) the model “misses” with its predictions of the target variable. Because the residual standard error is expressed in units of measurement of the target variable, it is often very informative to check it out when trying to evaluate the quality of a model. Of course, estimating what we truly consider to be a “acceptable” error depends on the particular use case, i.e. the nature of the target variable, the units of measurement we use, and the desires, that is, the requirements of whoever will actually use the predictive model. For example, if we were trying to estimate the value of a property in millions of euros, a mistake of several thousand euros might not be such a huge deal; on the other hand, if we are trying to predict how much we can get by selling an old car, a mistake in the same absolute amount would probably be unacceptable. The reason why our model “misses” is the so-called “unexplained variability” - everything that we did not measure and which in some way potentially affected the final amount of the target variable. This “noise”, i.e. the impact of unexplained variability, is described by the previously explained R-squared measure (for now, we will not pay too much attention to theAdjusted prefix, which will be discussed further in the lesson). Ideally, our model will have a small standard residual error and a high “R squared” measure- that is, the model will both guess well and contain a very small amount of unexplained variability. In practice, of course, we will not often come across such an ideal scenario, so we will have to evaluate, often individually or in cooperation with domain experts, what we consider to be “good enough”, since statistics here is not able to provide an exact, generally applicable answer. Finally, some other details that are summarized are: “five number summary” (minimum, maximum and averages) of the residuals that helps us to estimate the distribution of errors obtained “F-statistics” that tell us the chance that none of the input variables are affecting the output (this statistic will make more sense when we include more predictor variables) We said that by linear regression we get a “predictive model”, which we can use on new data to create new predictions. How can we use this model to create new predictions? R offers us the generic predict method, to which we commonly give the generated predictive model and a data frame with new data as parameters, taking care that the data frame has columns that match the expected model inputs. Because predictive models often include the set used to create the model, we can actually drop the dataset parameter, in which case the model will simply return the predictions obtained by applying the model on the dataset it was trained on (i.e., the same result that we get by using the fitted.values function). Assignment 13.5 - creating new predictions # the following vector has the &quot;new&quot; values of the input variable `x` new_x &lt;- c(-5, 10, 50, 102) # apply our predictive model on these new variables by using # the `predict` function and`and linear model `linMod` linear model # be sure to wrap new data in the form of a data frame first # calculate predictions &quot;manually&quot; by reading the obtained # coefficients from the linear model # apply our predictive model on these new variables by using # the `predict` function and`and linear model `linMod` linear model # be sure to wrap new data in the form of a data frame first predict(linMod, data.frame(x = new_x)) # calculate predictions &quot;manually&quot; by reading the obtained # coefficients from the linear model coef(linMod)[1] + new_x * coef(linMod)[2] ## 1 2 3 4 ## 26.74074 86.71835 246.65864 454.58101 ## [1] 26.74074 86.71835 246.65864 454.58101 Often, analysts do not want to rely on simple numerical values which describe the resulting model, but want to further evaluate the quality of the model through various visualizations. Analysts are often interested in the behavior of errors, i.e. residuals. Specifically, if the linear model describes the data well, we would expect the points to scatter uniformly around the regression line, without some obvious patterns. Eg. if we look how variable y3 dependes on the variable x in the graph at the beginning of this lesson, we see a clear pattern pattern where the direction first passes above the set of observations, then below and finally again above - which clearly tells us that the linear model poorly explains the relationship between these two variables. So the two questions that an analyst can potentially ask about residuals are: are there obvious patterns in residual behavior with respect to the sequence of original observations and whether the residuals have a normal distribution The first question can be easily answered by creating visualizations which use the following geometries of the ggplot2 package: geom_point - simple scatterplot with predictors on x-axis and residuals on y-axis we expect a “cloud” of residuals with no obvious patterns geom_density - draws the estimated density function of a variable normal distribution of residuals will be bell-shaped, without noticeable “tails” geom_qq - draws the so-called QQ (quantile-quantile graph) the normal distribution in this graph takes the form of a line Before creating these visualizations, we encounter a tricky problem - the ggplot2 package implies the existence of a data frame with the variables we want to display, but we do not have the residuals neatly wrapped in a data frame! Unfortunately, the base R language does not give us an easy way to get that information in the form of a data frame. We can relatively easily assemble it by using functions such as fitted.values to retrieve predictions, residuals to retrieve residuals, and then wrap everything in a new data frame, but these are some boilerplate coding steps that we would like to avoid if possible. The broom package, co-authored by Hadley Wickham, who we already met as developers ofdplyr and ggplot2, comes to our aid. This package offers a number of functions for easily extracting information from our trained models - for example, thetidy function gives us model results packaged in an easily readable data frame, while the glance function does the sam, but for the parameters that describe the quality of the model. Below we will show how a function called augment works. This function is similar to the predict function, but is “augmented” in various useful ways. If we simply give it our predictive model, it will return the original data frame used to create the model, but expanded with a number of useful columns such as: .fitted - Predictions obtained using the model .se.fit - standard prediction error .resid - the amount of residuals, ie errors .std.resid - residuals standardized to intervals [0,1] .hat - a measure of the\" extremity \"of an input variable of observation(* leverage *); .cooksd - a measure of the\" influential point \"of an observation on a model Analysts usually appreciate seeing some of these new values since they provide useful insight in how our model performs and where possible issues arise. For example, observations with a high leverage measure are potential outliers and as such deserve additional attention. Particularly problematic are the observations with high “impact” on the model - these are observations that have both a high leverage and a large residual. Observations like these can strongly can often “draw” the regression line towards themselves, worsening the quality of the model. Let’s apply the augment function over our predictive model. Assignment 13.6 - the augment function # apply the `augment` function on the `linMod` model # store the resulting data frame in the `predictions` variable # look at the first few lines from this variable #library(broom) # if necessary predictions &lt;- augment(linMod) head(predictions) ## # A tibble: 6 x 9 ## y1 x .fitted .se.fit .resid .hat .sigma .cooksd .std.resid ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 391. 83.8 382. 4.01 9.43 0.0232 26.5 0.00156 0.362 ## 2 254. 48.5 241. 2.64 13.1 0.0100 26.5 0.00126 0.499 ## 3 84.1 10.5 88.8 4.46 -4.71 0.0287 26.5 0.000485 -0.181 ## 4 144. 33.4 180. 3.05 -36.4 0.0134 26.2 0.0131 -1.39 ## 5 295. 74.6 345. 3.43 -49.8 0.0170 26.0 0.0313 -1.91 ## 6 148. 36.7 193. 2.91 -45.0 0.0122 26.1 0.0183 -1.72 (NOTE: as stated, we can also use the augment method as an alternative to the generic predict method - we just need to pass it new data using the newdata parameter) Now that we have a data frame that (amongst other things) contains a column which stores the residuals, we can easily create the visualizations we mentioned before. In particular, we will create a scatterplot with predictions on the x axis and (standardized) residuals on the y axis graph which shows the density function of standardized residuals quantile-quantile graph of standardized residuals The reason why we work with standardized instead of “true” residuals is simply easier interpretation, that is, a simpler comparison of results with a “standard” normal distribution having a mean of 0 and a standard deviation of 1. Assignment 13.7 - checking the ‘normality’ of residuals # using the `predictions` data frame # create a scatterplot between fitted values and std. residuals # also draw a horizontal line that goes through zero # create a density graph of the std. residuals # use the `geom_density` geometry function # create a quantile quantile graph ofstd. residuals # use the `geom_qq` geometry function # set residuals to the `sample` aesthetic (not `x`!) #library(gridExtras) # if necessary g1 &lt;- ggplot(predictions, aes(.fitted, .std.resid)) + geom_point() + geom_hline(yintercept = 0, color = &quot;blue&quot;) g2 &lt;- ggplot(predictions, aes(x = .std.resid)) + geom_density() g3 &lt;- ggplot(predictions, aes(sample = .std.resid)) + geom_qq() grid.arrange(g1, g2, g3, ncol = 2) In our (artificially created) example, we have received very convincing arguments that the linear model we have obtained describes the relationship of variables very well and as such is probably a good choice for creating predictions. In actual practice, we often do not get such “clean” results and will have to make a good assessment of whether the model is good enough, whether additional work is needed on the data or the process of building the model, or whether a completely new approach is required. For example, some of the possible conclusions after creating visualizations of residuals might be: if a scatterplot with predictions and residuals shows obvious patterns, it is possible that the linear model is not good for describing the predictor-target relationship and we should try to train model able to describe the more complex nature of the relationship if the residual graph takes the form of a “funnel”, ie if the residuals grow with increasing value of the predictions, it may be necessary to transform the input and/or output variables, for example by using the root or logarithm function if we notice some values that “pop up” strongly in the residual graph, we need to look at them more carefully and potentially remove them from the training set Another thing an analyst can try is to draw a graph of residuals in the order that the observations were obtained in the original set (a scatterplot where the x-axis is the ordinal number of observations and the y is the residual). If a high level of interdependence of residuals close to each other is observed, it is possible that our data actually represents a “time series data”. We should not use linear regression for this kind of data, but rather a more advanced, specialized time series method, such as ARIMA (autoregressive integrated moving average). 13.1.3 Linear regression and categorical variables In the previous section, we presented the process of creating a predictive linear regression model where the input variable was of numerical type. It is justified to ask - can a categorical variable also be an input to a predictive model? The answer is - yes.. but with some adjustment. Let us show how, in a simple example, a two-level categorical variable can be used to train a linear regression model (we will later easily extend this approach to category variables with more than two levels). A categorical variable cannot, by its nature, be a part of a linear equation, since the category itself usually does not have a numerical equivalent that could be meaningfully used to calculate the target variable. However, we can easily turn a categorical variable into a binary (indicator) variable that describes whether a particular observation belongs to the selected category or not (if it does not, then it logically belongs to the second category, which we will call a reference or baseline category). The linear regression will then determine the coefficient that will define the direction by adding the coefficient if the indicator variable is 1, or be ignored if the indicator variable is0. For a two-level categorical variable, one indicator variable is sufficient - the second would simply be an inverse of the first and would not hold any additional information. How many indicator variables do we need for a categorical variable with more than two levels? The answer is logical - one less than the number of categories, since “not belonging” to all but one category necessarily indicates belonging to that one, remaining category. We can mention here that sometimes we can make one indicator variable for each level, if we wish, but then we must make sure that the existence of a single redundant column will not adversely affect the model. Linear regression is one of the methods that is sensitive to such redundant columns, which is why we will avoid them here. Below we present a linear regression with a two-level categorical variable, since this is actually a variant of the simple linear regression. For a larger number of categories, the problem is reduced to the multiple linear regression that we will address later. Let’s now load a new, also artificially created data frame, with one two-level categorical variable x representing input and the target numeric variabley. Assignment 13.8 - sample data frame with a categorical predictor # load data from the `data2.csv` file in the `df2` data frame # examine the loaded data frame # draw a scatterplot of a relationship between `x` and `y` # load data from the `data2.csv` file in the `df2` data frame # examine the loaded data frame df2 &lt;- read_csv(&quot;data2.csv&quot;) ## Parsed with column specification: ## cols( ## x = col_character(), ## y = col_double() ## ) df2$x &lt;- factor(df2$x) glimpse(df2) ## Observations: 100 ## Variables: 2 ## $ x &lt;fct&gt; B, B, A, B, B, B, A, A, A, B, B, A, A, B, B, B, B, B, A, B, ... ## $ y &lt;dbl&gt; 63.61392, 63.91110, 20.24407, 95.70271, 56.93969, 78.36473, ... # draw a scatterplot of a relationship between `x` and `y` ggplot(df2, aes(x, y)) + geom_point() We see that the distribution of the target variable is different for different categories of input variable. We can model this increase by linear regression, although the interpretation of the model will be slightly different compared to the interpretation of the numerical inputs, as we will see after constructing the model itself. One advantage of using factor variables in the R language is that when training linear models, we do not have to “manually” create indicator variables. We just need to put the factors as input variables into the model’s training formula, and R will automatically create indicator variables for us (this also applies to creating predictions once the model is created - provided the factors do not contain “unknown” categories in the new data) . Assignment 13.9 - creating a simple linear model with a categorical input # Use the `lm` function to create a linear data model from the `df2` data frame # where `x` is input and `y` is output variable # save the result to the `linMod2` variable # print out the summary of `linMod2` linMod2 &lt;- lm(y ~ x, data = df2) summary(linMod2) ## ## Call: ## lm(formula = y ~ x, data = df2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -19.206 -6.104 -1.367 5.523 28.665 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 20.493 1.398 14.66 &lt;2e-16 *** ## xB 55.652 1.938 28.71 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 9.684 on 98 degrees of freedom ## Multiple R-squared: 0.8938, Adjusted R-squared: 0.8927 ## F-statistic: 824.4 on 1 and 98 DF, p-value: &lt; 2.2e-16 We see that the summary of the linear model is very similar to the summary already presented where the input variable was of numerical type. The difference in interpretation is as follows - the direction coefficient is related to a specific category (seen in the name of the variable), and refers to the expected difference in the amount of the target variable when the observation has the specified category, relative to the reference category. To conclude this section, let us emphasize only that when using categorical variables as inputs to the linear model, it is important to take into account the representation of categories, that is, we should take care to not have categories that are very poorly represented in the training dataset. The reason is that such observations very often have a great influence on the regression direction, which can have adverse effects on the quality of the linear model. 13.2 Multiple linear regression The principle of simple linear regression is easily extended to a scenario when we have multiple input variables - simply, we are looking for a function that will express the target variable as a linear combination of input variables. The problem of building a model again comes down to finding the “good” direction coefficients that will be given to each input variable (plus the intercept), although formally we cannot talk now about the “regression line”, but rather must a somewhat more complex notion of a “hyper-plane.” Multiple linear regression presents a number of additional challenges that we must face, but when it comes to training the model we still use the already familiar lm function, to which we simply forward the desired formula which expresses the desired relationship between the target and predictors, for example: y ~ x1 + x2 # `y` as a linear combination of `x1` and `x2` y ~ . # `y` as a linear combination of all other variables y ~ . - x1 - x2 # `y` as a linear combination of all other variables EXCEPT `x1` and `x2` log(y) ~ x1 + log(x2) # natural logarithm of `y` as a linear combination of `x1` and natural logarithm of `x2` y ~ x1 + I(x2 ^ 2) # `y` as a linear combination of` x1` and a square of `x2` Note that the formula can also contain transformed inputs (and outputs!). We have to be careful here, because sometimes mathematical notation “clashes” with formula notation (for example, we see that - is not really an arithmetic subtraction but a method of determining an undesirable variable). Therefore, in the last example, we used the I function, which simply indicates that the expression between parentheses should be treated “as is”. The presented transformations of inputs and outputs are moving away from simpler linear regression models with multiple inputs, so we will not use them below. Here, we list them only for reasons of completeness and as a motivation to try more complex regression formulas in situations where the need arises for such an approach. Now let’s try to create a predictive model with multiple input variables. In the assignment, we will use the familiar mtcars dataset (this time we will use a version which comes shipped with our R distribution). data(mtcars) # factorize the `vs` and` am` columns cols &lt;- c(&quot;vs&quot;, &quot;am&quot;) mtcars[, cols] &lt;- lapply(mtcars[, cols], factor) # quicker way to factorize columns by name glimpse(mtcars) ## Observations: 32 ## Variables: 11 ## $ mpg &lt;dbl&gt; 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.... ## $ cyl &lt;dbl&gt; 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, ... ## $ disp &lt;dbl&gt; 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146.7, 1... ## $ hp &lt;dbl&gt; 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180, ... ## $ drat &lt;dbl&gt; 3.90, 3.90, 3.85, 3.08, 3.15, 2.76, 3.21, 3.69, 3.92, 3.9... ## $ wt &lt;dbl&gt; 2.620, 2.875, 2.320, 3.215, 3.440, 3.460, 3.570, 3.190, 3... ## $ qsec &lt;dbl&gt; 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.00, 2... ## $ vs &lt;fct&gt; 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, ... ## $ am &lt;fct&gt; 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, ... ## $ gear &lt;dbl&gt; 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4, ... ## $ carb &lt;dbl&gt; 4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2, ... Assignment 13.10 - creating a linear model with multiple predictors # Use the `lm` function to create a linear data model from the `mtcars` table # use the variables `am`,` cyl` and `wt` as input # and the variable `mpg` as output # name the model `linMod` # # check the model summary linMod &lt;- lm(mpg ~ am + cyl + wt, data = mtcars) summary(linMod) ## ## Call: ## lm(formula = mpg ~ am + cyl + wt, data = mtcars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.1735 -1.5340 -0.5386 1.5864 6.0812 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 39.4179 2.6415 14.923 7.42e-15 *** ## am1 0.1765 1.3045 0.135 0.89334 ## cyl -1.5102 0.4223 -3.576 0.00129 ** ## wt -3.1251 0.9109 -3.431 0.00189 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.612 on 28 degrees of freedom ## Multiple R-squared: 0.8303, Adjusted R-squared: 0.8122 ## F-statistic: 45.68 on 3 and 28 DF, p-value: 6.51e-11 We can see that this summary is not that much different from the summaries we saw when we were creating simple linear models. The biggest difference is that each input numeric variable has its own coefficient, as well as each category for each individual categorical variable (minus one for the reference category). Also, each coefficient has its own “p-value”, i.e. the probability that the linear relationship with the target variable is random - so in this case, because of the high p-value of the input variable am (category 1), we can conclude that the difference between automatic and manual transmissions are very likely irrelevant when looking at the fuel cost, which is not an unexpected conclusion). However, there are several important differences in the interpretation of the results obtained. For example, the coefficient represents how much the target variable changes when the input variable changes by the amount of one, but provided that all other input parameters remain fixed. In other words, the coefficients are treated as being independent of one another. Of course, this assumption is often not the case in the real world, and how this fact affects the results is something that will be discussed later. Now let’s look at the “R-squared” measure. Although this measure has the same interpretation for multiple regression as in the case of simple regression, it can be somewhat problematic due to the fact that the multiple R-squared value, as a rule, always decreases as we add variables to the model. Therefore, in multiple regression, we prefer to look at a adjusted R-squared that compensates for this, because it is designed to “penalize” a larger number of input variables. Finally, the F-statistic makes more sense here than in the simple regression model. It tells us what is the chance that no input variable is associated with the target. This is especially important as we use more input variables - assuming an extremely large number of input variables, the possibility of accidentally detecting a relationship which doesn’t really exist becomes much more probable, and this statistic allows us to estimate how probable it is that such a situation occured. Let us return to the stated assertion that multiple linear regression coefficients treat the input variables as being independent of one another. This is relatively often not true, i.e. the input variables are not only collinear to the target but also to each other. This is a known problem of input collinearity. If it may not be immediately clear why this is potentially problematic, then it is enough to look again at the interpretation of multiple linear regression coefficients. The model assumes that we can select one predictor and change it freely while the values of other predictors remain fixed. Suppose now that the two predictors are extremely collinear, that is, changing one always results in changing the other. This means that the model assumes a much larger number of combinations of the values of these two predictors than is realistically possible, and since a large number of these combinations does not exist in the input set, this results in a greater “uncertainty” of the model in the results obtained. In particular, the calculated p-values of the predictors can be quite large to the extent that they can be treated as irrelevant, although they are actually strongly linearly related to the target. What can we do about this phenomenon? First, we can directly check the collinearity between the input variables with the help of the already used cor function, which will give us a correlation matrix as a result. There are no solid guidelines what represents a “too high” collinearity, but usually values that are in absolute terms above 0.7 probably deserve special attention. Alternatively, we can call the ggpairs function, which will give us not only correlation coefficients, but also visualizations by which we can observe linear trends between the input variables. Assignment 13.11 - collinearity of input variables # put all (truly) numeric columns from the `mtcars` dataset # into a `mtCarsNumPred` data frame # (do not include the `mpg` variable since we will treat it as our goal) # print the correlation matrix using the `cor` function # and the `mtCarsNumPred` as input parameter # pass this data frame to the `ggpairs` function of the `GGally` package # put all (truly) numeric columns from the `mtcars` dataset # into a `mtCarsNumPred` data frame # (do not include the `mpg` variable since we will treat it as our goal) mtcars %&gt;% select(-mpg) %&gt;% select_if(is.numeric) -&gt; mtCarsNumPred # print the correlation matrix using the `cor` function # and the `mtCarsNumPred` as input parameter mtCarsNumPred %&gt;% cor ## cyl disp hp drat wt qsec ## cyl 1.0000000 0.9020329 0.8324475 -0.69993811 0.7824958 -0.59124207 ## disp 0.9020329 1.0000000 0.7909486 -0.71021393 0.8879799 -0.43369788 ## hp 0.8324475 0.7909486 1.0000000 -0.44875912 0.6587479 -0.70822339 ## drat -0.6999381 -0.7102139 -0.4487591 1.00000000 -0.7124406 0.09120476 ## wt 0.7824958 0.8879799 0.6587479 -0.71244065 1.0000000 -0.17471588 ## qsec -0.5912421 -0.4336979 -0.7082234 0.09120476 -0.1747159 1.00000000 ## gear -0.4926866 -0.5555692 -0.1257043 0.69961013 -0.5832870 -0.21268223 ## carb 0.5269883 0.3949769 0.7498125 -0.09078980 0.4276059 -0.65624923 ## gear carb ## cyl -0.4926866 0.5269883 ## disp -0.5555692 0.3949769 ## hp -0.1257043 0.7498125 ## drat 0.6996101 -0.0907898 ## wt -0.5832870 0.4276059 ## qsec -0.2126822 -0.6562492 ## gear 1.0000000 0.2740728 ## carb 0.2740728 1.0000000 # pass this data frame to the `ggpairs` function of the `GGally` package #library (GGally) # if necessary ggpairs(mtCarsNumPred) Another convenient way of visualizing collinearity is the corrplot function of the package of the same name. Assignment 13.12 - function corrplot # load the `corrplot` package (install if necessary) # invoke the `corrplot` function to which you will pass the correlation matrix # made from the `mtCarsNumPred` data frame # load the `corrplot` package (install if necessary) # invoke the `corrplot` function to which you will pass the correlation matrix # made from the `mtCarsNumPred` data frame #library(corrplot) if needed corrplot(cor(mtCarsNumPred)) We see that some variables (eg disp andwt, representing engine volume and vehicle weight) have very high levels of collinearity, which means that they require special attention. The collinearity of the variables we have been looking at is related to pairs of variables, but there is one interesting phenomenon - the so-called multicollinearity. With this phenomenon, it is possible that collinearity is manifested only in the combination of three or more variables, that is, when looking at single pairs we do not see anything unusual, but the negative effect of collinearity persists. In order to facilitate the detection of this phenomenon, a so-called VIF measure (variance inflation factor) can be used. We will not explain in detail the meaning and theory behind this measure, but merely state the fact how to use it in our linear model and how to interpret the values obtained. We calculate the VIF measure using the vif function found in the car package. This function expects the linear model we are analyzing as a parameter. As a result, we obtain a numerical vector with VIF values for each input variable. Again, there is no exact guideline what represents a “high” VIF value - in the literature we come across different references, sometimes it’s a value of 5, a value of 10, etc. As a general rule, we can remember that a two-digit VIF indicates a “problematic” input variable. Assignment 13.13 - multicollinearity and VIF measure # train the linear model `lm_all` which uses the `mtcars` data frame # and has `mpg` as target and all other variables as predictors #pass the above model to the `vif` function of the` cars` package and print the result #library(car) # if necessary lm_all &lt;- lm(mpg ~., data = mtcars) vif(lm_all) ## cyl disp hp drat wt qsec vs ## 15.373833 21.620241 9.832037 3.374620 15.164887 7.527958 4.965873 ## am gear carb ## 4.648487 5.357452 7.908747 Now that we know that the collinearity of input variables is a potential problem, we can ask the question - what to do when we notice this phenomenon occuring? Some possible solutions are: throw out one variable from a pair of “problematic” variables combine collinear variables into a a unique input variable keep all variables in the model (i.e. “do nothing”) Let’s try to train three separate linear models and try to notice the effect of colinear variables on the results. Assignment 13.14 - linear model with collinear inputs # train the following linear models: # `lm1` -` mpg` depending on `disp` # `lm2` -` mpg` depending on `wt` # `lm3` -` mpg` depending on `disp` and` wt` # study the summaries of the linear models obtained, # especially the t-values of the parameters and the R-squared measure lm1 &lt;- lm(mpg ~ disp, data = mtcars) lm2 &lt;- lm(mpg ~ wt, data = mtcars) lm3 &lt;- lm(mpg ~ disp + wt, data = mtcars) summary(lm1) ## ## Call: ## lm(formula = mpg ~ disp, data = mtcars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.8922 -2.2022 -0.9631 1.6272 7.2305 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 29.599855 1.229720 24.070 &lt; 2e-16 *** ## disp -0.041215 0.004712 -8.747 9.38e-10 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.251 on 30 degrees of freedom ## Multiple R-squared: 0.7183, Adjusted R-squared: 0.709 ## F-statistic: 76.51 on 1 and 30 DF, p-value: 9.38e-10 summary(lm2) ## ## Call: ## lm(formula = mpg ~ wt, data = mtcars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.5432 -2.3647 -0.1252 1.4096 6.8727 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 37.2851 1.8776 19.858 &lt; 2e-16 *** ## wt -5.3445 0.5591 -9.559 1.29e-10 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.046 on 30 degrees of freedom ## Multiple R-squared: 0.7528, Adjusted R-squared: 0.7446 ## F-statistic: 91.38 on 1 and 30 DF, p-value: 1.294e-10 summary(lm3) ## ## Call: ## lm(formula = mpg ~ disp + wt, data = mtcars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.4087 -2.3243 -0.7683 1.7721 6.3484 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 34.96055 2.16454 16.151 4.91e-16 *** ## disp -0.01773 0.00919 -1.929 0.06362 . ## wt -3.35082 1.16413 -2.878 0.00743 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.917 on 29 degrees of freedom ## Multiple R-squared: 0.7809, Adjusted R-squared: 0.7658 ## F-statistic: 51.69 on 2 and 29 DF, p-value: 2.744e-10 Comparing the results of the linear models obtained, we can conclude that the linear model lm3 has the smallest standard residual error and the largest\" R-squared \"measure and is thus the best of the three options. However, a potential problem is evident when we look at p-values, which are both significantly larger than when we train models with each variable separately. Thus, the collinearity of variables does not necessarily affect the predictive power of the model, but it does introduce a potentially large uncertainty in the model sense that we remove all collinear predictors from the model as irrelevant. This could prove to be a big problem when we have more potential predictors and are trying to select a relevant subset, which is a topic we will address below. In the previous exercise, we have seen that even with the relatively simple predictive multiple regression method with two potential input variables, a number of questions arise regarding the choice of formula, i.e. which input variables to choose. This issue gets much more complicated as the number of potential input variables grows. This is why variable selection is one of the key challenges we face in designing predictive models, not only in linear regression but also in general. Obviously, a good model should include input variables that “explain” the target variable while being as independent between each other as possible. A possible criterion for deciding which variable to choose to fit into the model may thus be to increase the common “R-squared” measure, reduce the residual standard error, or p-value of the coefficient for that input variable. In addition to these “standard” criteria, there are various others, such as using the popular AIC (Akaike information criterion), which assesses the model’s informativeness by penalizing having more variables thrown into the model. We can select variables manually, but it’s much easier to leave that job to your computer. Statistical tools, including the R language, often have built-in algorithms that, based on a given criterion, construct a predictive model by iteratively selecting variables. The most common model building strategies are: “backwards” from the complete model, e.g. iteratively throwing out the variables with the highest p-value “forwards” from the blank model, e.g. iteratively adding the variables that reduce the RMSE the most various hybrid methods The language R has a step function to iteratively (stepwise) create predictive models, but in practice, using the much better stepAIC function found in the MASS package is recommended. This function, among other things, expects the following parameters: object - initial (linear) model scope - range of models that we include in the strategy; this is only needed for the “forward” approach and we pass it a list with the “poorest” (lower) and ‘richest’ (upper) model direction - forward (forward), backward (backward) or hybrid (both) trace - A binary variable that describes whether we want to print the entire variable selection process (Note: the MASS package has its own select function, which can “override” the select function from thedplyr package; this does not mean that this function is unavailable, but when these two packages are used together, it is better to rely on full function names, i.e. MASS::select and dplyr::select) Finally, we will iteratively create a predictive model for the mtcars data frame where again the target variable will be fuel cost (the variablempg), while the candidates for the input variable will be all other variables. Assignment 13.15 - stepwise variable selection for linear regression #library (MASS) # if necessary # We create a &quot;complete&quot; and &quot;empty&quot; model lm_all &lt;- lm(mpg ~., data = mtcars) lm_blank &lt;- lm(mpg ~ 1, data = mtcars) # check out summaries on these models # to get the feeling of &quot;baseline&quot; performance # use the `stepAIC` function to create the `lm1` and `lm2` models # `lm1` - created by backward selection from full model # (parameter direction = &quot;backward&quot;) # `lm2` - is created by selecting&quot; advance &quot;from the empty model # (parameters direction = &quot;forward&quot;, # scope = list (upper = lm_all, lower = lm_ blank)) # # check out summaries of these models summary(lm_all) ## ## Call: ## lm(formula = mpg ~ ., data = mtcars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.4506 -1.6044 -0.1196 1.2193 4.6271 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 12.30337 18.71788 0.657 0.5181 ## cyl -0.11144 1.04502 -0.107 0.9161 ## disp 0.01334 0.01786 0.747 0.4635 ## hp -0.02148 0.02177 -0.987 0.3350 ## drat 0.78711 1.63537 0.481 0.6353 ## wt -3.71530 1.89441 -1.961 0.0633 . ## qsec 0.82104 0.73084 1.123 0.2739 ## vs1 0.31776 2.10451 0.151 0.8814 ## am1 2.52023 2.05665 1.225 0.2340 ## gear 0.65541 1.49326 0.439 0.6652 ## carb -0.19942 0.82875 -0.241 0.8122 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.65 on 21 degrees of freedom ## Multiple R-squared: 0.869, Adjusted R-squared: 0.8066 ## F-statistic: 13.93 on 10 and 21 DF, p-value: 3.793e-07 summary(lm_blank) ## ## Call: ## lm(formula = mpg ~ 1, data = mtcars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -9.6906 -4.6656 -0.8906 2.7094 13.8094 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 20.091 1.065 18.86 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 6.027 on 31 degrees of freedom lm1 &lt;- stepAIC(lm_all, direction = &quot;backward&quot;, trace = 0) summary(lm1) ## ## Call: ## lm(formula = mpg ~ wt + qsec + am, data = mtcars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.4811 -1.5555 -0.7257 1.4110 4.6610 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 9.6178 6.9596 1.382 0.177915 ## wt -3.9165 0.7112 -5.507 6.95e-06 *** ## qsec 1.2259 0.2887 4.247 0.000216 *** ## am1 2.9358 1.4109 2.081 0.046716 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.459 on 28 degrees of freedom ## Multiple R-squared: 0.8497, Adjusted R-squared: 0.8336 ## F-statistic: 52.75 on 3 and 28 DF, p-value: 1.21e-11 lm2 &lt;- stepAIC (lm_blank, scope = list (upper = lm_all, lower = lm_blank), direction = &quot;forward&quot;, trace = 0) summary(lm2) ## ## Call: ## lm(formula = mpg ~ wt + cyl + hp, data = mtcars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.9290 -1.5598 -0.5311 1.1850 5.8986 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 38.75179 1.78686 21.687 &lt; 2e-16 *** ## wt -3.16697 0.74058 -4.276 0.000199 *** ## cyl -0.94162 0.55092 -1.709 0.098480 . ## hp -0.01804 0.01188 -1.519 0.140015 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.512 on 28 degrees of freedom ## Multiple R-squared: 0.8431, Adjusted R-squared: 0.8263 ## F-statistic: 50.17 on 3 and 28 DF, p-value: 2.184e-11 We can see that the two strategies resulted in two completely different models of similar performance. This means that we cannot commony expect to find the optimal model, but just perform an automated attempt to find the “best” with the criteria and conditions we initially set. If we are not satisfied with the result, we can always try alternative function parameters, a different strategy, or an initial set of input variables. In any case, finding a satisfactory predictive model is not a simple problem, and it is always advisable to remember a quote often found in statistics and machine learning books allegedly made by statistician George Box - “all models are wrong, but some are useful”. Programirajmo u R-u by Damir Pintar is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License.Based on a work at https://ratnip.github.io/FER_OPJR/ "],
["predictive-Modeling.html", "14 Introduction to Predictive Modeling 14.1 What is predictive modeling? 14.2 Creating training and test datasets 14.3 Classification Predictive Models - kNN Classification 14.4 Package caret and predictive modeling", " 14 Introduction to Predictive Modeling 14.1 What is predictive modeling? In the previous chapter, we have shown that with the help of linear regression we can investigate and model the interplay of two (or more) variables. If there is collinearity between the two observed variables, then their relation can be modeled with the help of a simple straight line equation, which then allows to estimate the value of the second variable from the known value of one of the variables. This is the basic idea of so-calle. “predictive modeling” - a process which relies on familiar inputs and developed predictive models to get information about unknown outputs, i.e. goals. Better models provide better, or more accurate results. Or, as defined by Kuhn and Johnson in his book “Applied Predictive Modeling”: Predictive modeling is a process of developing a mathematical tool or model that is able to create accurate predictions. Predictive modeling has numerous usage options in various domains eg: predicting the quantity of sold products or expected profits identification of users planning to cancel the subscription disease diagnostics real estate evaluation estimation of movies’s profits on opening weekened spam detection etc. When developing predictive models, it is very important to: have adequately prepared, high quality input data choose a suitable method for creating a model properly conduct the evaluation and validation process The role of analysts in this process is to conduct all the steps of analysis thoroughly and to avoid common traps and errors. Namely, predictive models can ultimately have poor performance for a number of reasons - poorly prepared data, poorly conducted validation, creating models which “overfit” the training data, etc. In this chapter, we will get acquainted with some of the basic guidelines we must adhere to during predictive modeling. We will also familiarize ourselves with R language packages that allow us to leave complex steps to the computer, that is, to approach predictive modelling from high-level perspective while the computer independently performs low-level data preparation, modeling, and validation steps. 14.2 Creating training and test datasets When we created linear models in the previous chapter, after the creation of the model, we looked at summarized model information with the help of summary function - estimation of predictor and target relationship, the value of residual standard error, the value of adjusted R-squared measure etc. This information has enabled us to evaluate the quality of developed models. But in the whole process we ignored one key thing - all of this information was based on data used to create the model itself. In other words, we have gained insight into how well a model works over known data, which the model has already seen during the model’s creation. As a rule, it is much more important to evaluate how well a model works over unknown data, i.e. to estimate the quality of predictions once the model gets completely new data. We say that we want models that generalize well, i.e. models that have learned generic features of the domain entities described in theobtained data, and not just the specifics of the dataset used to develop the model itself. This latter phenomenon is called “overfitting”. Very often, at the time of creating a model, we only have one dataset that we should also for training and model evaluation. As we have already said, using the same data for both purposes does not give us enough good information about how the model generalizes. A common procedure in this case is to split the initial set into two parts - a training set and a test set. How do you decide which observations to assign to which set? How many observations should be put into training, and how many in the testing set? As a rule, the training set and test set should have enough observations so that the obtained results could be statistically relevant so assuming a sufficiently large input dataset, splitting in two equal parts could be a satisfactory solution. However, in practice, we usually reserve more observations for training than for testing, so it is customary to use a 70:30 spluit, so 70% observations go into the training set, and the rest go in the test set. As for the procedure of selecting observations (i.e. “sampling”), there are several common procedures: random selection stratified random selection (ensuring equal representation of certain categories in both sets) timestamp-based selection (if the time component is key, i.e. it is important to estimate how well past information predicts future events) Random selection is a most commonly used method while more sophisticated sampling methods can be chosen if there is a clear need for them based on the goals of the analysis. Let’s now apply this knowledge to the development of a linear regression model by comparing its effectiveness on the training set, and then on the test data. For this we will use a new set of data related to the characteristics and quality of wine, wines.csv. This set was created by adapting the “Wine Quality Data Set” from the UCI Machine Learning Repository, available at[this link] (https://archive.ics.uci.edu/ml/datasets/wine+quality). In the next exercise, we’ll load this dataset and make some adjustments - we’ll categorize columns that obviously contain a category variable and remove rows with missing values. Lines with missing values often require a little more attention and a more sophisticated approach than simple removal, especially with larger amounts of missing values or potential additional information that these values denote. But in our case there are very few such rows, and when applying certain predictive modeling methods, they can create problems, so we will simply remove them. One of the quick ways to do this is by using the complete.cases function, which returns the indexes of all rows which do not have any missing value for the given data frame. Assignment 14.1 - wine quality dataset # load data from the` wines.csv` file # in a variable called `wine` # examine the loaded data frame # categorize columns as needed # and remove rows with missing values # load data from the` wines.csv` file # in a variable called `wine` # examine the loaded data frame # categorize columns as needed # and remove rows with missing values wine &lt;- read.csv(&quot;wines.csv&quot;, stringsAsFactors = F, encoding = &quot;UTF-8&quot;) glimpse(wine) wine$type &lt;- factor(wine$type) wine &lt;- wine[complete.cases(wine),] ## Observations: 6,497 ## Variables: 13 ## $ fixed.acidity &lt;dbl&gt; 7.4, 7.8, 7.8, 11.2, 7.4, 7.4, 7.9, 7.3, ... ## $ volatile.acidity &lt;dbl&gt; 0.700, 0.880, 0.760, 0.280, 0.700, 0.660,... ## $ citric.acid &lt;dbl&gt; 0.00, 0.00, 0.04, 0.56, 0.00, 0.00, 0.06,... ## $ residual.sugar &lt;dbl&gt; 1.9, 2.6, 2.3, 1.9, 1.9, 1.8, 1.6, 1.2, 2... ## $ chlorides &lt;dbl&gt; 0.076, 0.098, 0.092, 0.075, 0.076, 0.075,... ## $ free.sulfur.dioxide &lt;dbl&gt; 11, 25, 15, 17, 11, 13, 15, 15, 9, 17, 15... ## $ total.sulfur.dioxide &lt;dbl&gt; 34, 67, 54, 60, 34, 40, 59, 21, 18, 102, ... ## $ density &lt;dbl&gt; 0.9978, 0.9968, 0.9970, 0.9980, 0.9978, 0... ## $ pH &lt;dbl&gt; 3.51, 3.20, 3.26, 3.16, 3.51, 3.51, 3.30,... ## $ sulphates &lt;dbl&gt; 0.56, 0.68, 0.65, 0.58, 0.56, 0.56, 0.46,... ## $ alcohol &lt;dbl&gt; 9.4, 9.8, 9.8, 9.8, 9.4, 9.4, 9.4, 10.0, ... ## $ quality &lt;int&gt; 5, 5, 5, 6, 5, 5, 5, 7, 7, 5, 5, 5, 5, 5,... ## $ type &lt;chr&gt; &quot;red&quot;, &quot;red&quot;, &quot;red&quot;, &quot;red&quot;, &quot;red&quot;, &quot;red&quot;,... Suppose the attribute quality is the target variable and all the other columns are potential predictors. How do you split this set on the training set and test set using random selection? There are several ways to do this, and even specialized functions and packages for this purpose, but we will learn a simple and easy-to-understand method that uses the sample function. If we assume that df is the data frame that we want to split into a training and test set that we will store in df.train and df.test variables, then random selection can be done as follows: train_size &lt;- 0.7 * nrow(df) %&gt;% round # about 70% train_ind &lt;- sample(1:nrow(df), train_size) # indexes of training observations df.train &lt;- df[train_ind, ] df.test &lt;- df[-train_ind, ] Let’s apply this to our wines dataset and then train a linear regression model. Assignment 14.2 - Splitting the input dataset and training a model set.seed(1234) # split the `wine` dataframe into `wine.train` and `wine.test` # using a random selection method and 70:30 ratio # train a linear regression model using the `wine.train` set # store the model in a variable called `linMod` # target variable is `quality` and all other variables are predictors # check the summary for the obtained model set.seed(1234) # split the `wine` dataframe into `wine.train` and `wine.test` # using a random selection method and 70:30 ratio train_ind &lt;- sample(1:nrow(wine), 0.7 * nrow(wine) %&gt;% round) wine.train &lt;- wine[train_ind, ] wine.test &lt;- wine[-train_ind, ] # train a linear regression model using the `wine.train` set # store the model in a variable called `linMod` # target variable is `quality` and all other variables are predictors # check the summary for the obtained model linMod &lt;- lm(quality ~ ., data = wine.train) summary(linMod) ## ## Call: ## lm(formula = quality ~ ., data = wine.train) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.7801 -0.4631 -0.0378 0.4576 2.7565 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 9.024e+01 1.618e+01 5.578 2.57e-08 *** ## fixed.acidity 7.248e-02 1.849e-02 3.919 9.01e-05 *** ## volatile.acidity -1.386e+00 9.720e-02 -14.260 &lt; 2e-16 *** ## citric.acid -1.279e-01 9.676e-02 -1.322 0.186209 ## residual.sugar 5.917e-02 6.935e-03 8.532 &lt; 2e-16 *** ## chlorides -6.720e-01 3.934e-01 -1.708 0.087705 . ## free.sulfur.dioxide 5.231e-03 9.121e-04 5.735 1.04e-08 *** ## total.sulfur.dioxide -1.600e-03 3.865e-04 -4.140 3.54e-05 *** ## density -8.915e+01 1.641e+01 -5.431 5.89e-08 *** ## pH 3.912e-01 1.073e-01 3.647 0.000268 *** ## sulphates 7.426e-01 8.966e-02 8.283 &lt; 2e-16 *** ## alcohol 2.446e-01 2.081e-02 11.751 &lt; 2e-16 *** ## typewhite -3.030e-01 6.684e-02 -4.534 5.93e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.7391 on 4533 degrees of freedom ## Multiple R-squared: 0.286, Adjusted R-squared: 0.2842 ## F-statistic: 151.3 on 12 and 4533 DF, p-value: &lt; 2.2e-16 We see that the adjusted R-squared measure is relatively low and the average error is quite high, so linear regression is perhaps not the best option this scenario (or maybe the obtained characteristics of the wines are simply not good enough to predict its quality). In spite of this, we will now check how well the model works on unseen observations. In order to check the quality of our model, we have to choose the criterion to which we will adhere. For numeric goals we often use the RMSE measure (root mean square error): \\[RMSE = \\sqrt{\\frac{\\sum_{i = 1}^{n}(\\widehat{y}_{i} - y_{i})^2}{n}}\\] where n is the number of observations,\\(\\widehat{y}_{i}\\) prediction of the observation i and \\(y_ {i}\\) actual value of the target variable of that observation. Although there are packages that contain this function, we can easily program it ourselves. Assignment 14.3 - Function for calculating the RMSE measure # create a `rmse` function that will use prediction vector # and a vector of actual target vales as parameters # and calculate the RMSE measure according to the above formula # create a `rmse` function that will use prediction vector # and a vector of actual target vales as parameters # and calculate the RMSE measure according to the above formula rmse &lt;- function(pred, act) sqrt(mean((pred - act) ** 2)) Let us now add prection columns to wine.train andwine.test datasets and then calculate the value of the RMSE measure for both. Assignment 14.4 - Model evaluation for the training and test set # add a `predQualityLR` column to `wine.train` and `wine.test` # using the `predict` function and the `linMod` model # print the value of the RMSE measure for both sets # remove the `predQualityLR` column from the `wine.train` dataset # add a `predQualityLR` column to `wine.train` and `wine.test` # using the `predict` function and the `linMod` model wine.train$predQualityLR &lt;- predict(linMod, wine.train) wine.test$predQualityLR &lt;- predict(linMod, wine.test) # print the value of the RMSE measure for both sets rmse(wine.train$predQualityLR, wine.train$quality) rmse(wine.test$predQualityLR, wine.test$quality) # remove the `predQualityLR` column from the `wine.train` dataset wine.train$predQualityLR &lt;- NULL ## [1] 0.738077 ## [1] 0.7205543 We see that the RMSE for the training set roughly corresponds to the residual standard error obtained in the model summary (a small difference is a result of the fact that we used the total number of observations, while in the calculation of the residual standard error we used the number of degrees of freedom - “dependent” variables, i.e. predictors). What is interesting is the fact that the value of RMSE measure of the test set is not dramatically larger than the RMSE measure, which means that the model works almost equally well (or badly) on new data. 14.3 Classification Predictive Models - kNN Classification The linear regression method allowed us to “guess” the numerical target variable. It is reasonable to ask the question - can we build a predictive model that will try to estimate the value of a categoric variable? This is an extremely important goal of predictive modeling since many domains have problems with determining the value of a particular category (the patient is ill or not, the device is corrupt or correct, the transaction is regular ora result of fraud, the client will default on the loan or not etc.) These are so-called “classification” problems for which - just as for the “regression” problems which use numerical goals - there is an extremely large set of developed methods. Very often, similar methods can be used for both purposes (sometimes with certain adjustments), so for example even if linear regression is not specifically suited for classification problems, its related “logistic regression” method is a very effective and popular approach to such a type of problem. In this section, we will focus on another, very popular classification method, which is very intuitive and easy to understand, but which allows us to inspect a number of interesting predictive modeling elements that we have not mentioned so far. It is a method called “k nearest neighbors”, or “kNN classification”. This method works in a very simple way. If we do not know the category of some observation, we simply find a number of observations which are the “closest” to that new observation. We then look at their categories, and then by majority vote determine which category to assign to the new observation. Let’s try to visualize this. Assignment 14.5 - Visualization of the main idea for the method of kNN classification # create a scatterplot for `wine.train` dataset # put `chlorides` on `x` axis # and `volatile.acidity` on `y` axis # color point based on wine type # set the transparency of the points to 0.5 # create a scatterplot for `wine.train` dataset # put `chlorides` on `x` axis # and `volatile.acidity` on `y` axis # color point based on wine type # set the transparency of the points to 0.5 ggplot(wine.train, aes(chlorides, volatile.acidity, col = type)) + geom_point(alpha = 0.5) On the graph we can clearly see how the points form “clusters” of the same colors in certain areas. If we take a new observation regarding the wine of an unknown type, but the known values of the measures set on the x and y axes, by looking at its immediate neighborhood, we could conclude which type of wine the new wine belongs to. Observations deep within one of the “clouds” are very likely to belong to the type shown (although we see that there are exceptions!). On the other hand, observation within the “boundary” areas is much more likely to be misdiagnosed, and the different choice of neighboring numbers could result in different classification results. kNN classification is based on the concept of distance. Although there are different options for distance selection, we often rely on the so-called “Euclidean distance”, which is easily presented in the two- and three-dimensional Cartesian system by the shortest path between two points, and we can easily calculate its value using their coordinates and Pythagorean theorem. This distance is easily applied to n-dimensional spaces, so although we can not easily visualize points in a space whose dimension corresponds to the number of predictors, we can still easily calculate the value of the Euclidean distance between points. So, the way kNN classification works can be easily described as follows: the training set itself represents “domain knowledge”, that is, the predictive model itself for each new observation, we find k closest observations from the training set and assign the category to it by using the majority vote Although this process is relatively simple, there are some common questions which require answers. Firstly - how do we select the parameter k? Secondly, do we need to make some additional preparatory actions on the data set before we perform the kNN classification? Let’s deal with the second question first. Examine the graph we have drawn, more precisely its coordinate axes. We can see that the lengths of the axes do not necessarily scale to their numerical values equally, i.e. the unit interval on the x axis is not necessarily equal to the unit interval on y axis. This is normal and expected, since the values on axes do not necessarily have to use the same scale nor even the measuring unit. But there is one problem - when we use Euclidean distance, it treats all axes equally, which means that variables with larger ranges will automatically gain greater importance (e.g. the maximum value of the chlorides is around 0.611 while the maximum value of sulfur dioxide variable reaches over 3000!) Here we see the importance of data pre-processing . For kNN classification, normalization of numeric variables is recommended, i.e. the transformation of numeric variables in such a way that we deduct their average and divide them by standard deviation, which brings them all to the same scale. This process is somewhat more complex than it seems, because we have to make sure that new observations are also normalized in the same way. This means that we need to make sure both training and test numerical variables need to come from the same distribution. If we have enough data and both training and test observations are representative, then their means and standard deviations should be close enough so we can easily normalize each set independently. If we have small test sets, then the normalization of the test set should be done by remembering using the mean and standard deviation of the training set. To simplify this process somwhat, we will return to the original wine dataset and simply normalize the numeric columns beforehand, and then store it in the wineNorm variable (we can pretend that we knew means and standard deviations of populations of these variables beforehand so we used them to normalize both training and test set independetly). Then, we will split this set into sets of wineNorm.train andwineNorm.test analogously to the previous procedure. `r zadHead(“Normalization of numeric variables of the input dataset”) # normalize all numeric columns of the `wine` data frame # store the result in the `wineNorm` variable # use the `train_ind` object to split `wineNorm` # into `wineNorm.train` and `wineNorm.test` # normalize all the numeric columns of the `wine` data frame # store the result in the `wineNorm` variable wineNorm &lt;- lapply(wine, function(x) { if(is.numeric(x))(x - mean(x)) / sd(x) else x })%&gt;% as.data.frame # use the `train_ind` object to split `wineNorm` # into `wineNorm.train` and `wineNorm.test` wineNorm.train &lt;- wineNorm[train_ind,] wineNorm.test &lt;- wineNorm[-train_ind,] Let’s go back to the problem of selecting the value of the k parameter. How to pick the right value? Generally speaking, this problem is called “choosing a hyperparameter of a model,” since the model besides input data requires some additional input parameters to perform its function. Usually, when no mathematical method of calculating optimal hyperparameters exist, the only option we have is training models with various combinations of hyperparameters and finally selecting those hyperparameter which result in models showing the best performance. In this case, we often need another (third) part of the original dataset, usually called a “validation” dataset, which represents an additional step before testing the set in which we select the values of the hyperparameters for the “final” model. Specifically, for kNN classification: we train a larger number of models for different parameter values k over the training set with the help of the validation set, we find the model that has the best performance and choose its k we use chosen k and the entire training and validation set to train the model we make a final evaluation on the test set Below we will use a simplified version of this process which uses only the training and test set while setting the k parameter arbitrarily to 5. We will leave the entire process of possibly finding a better hyperparameter as an optional exercise. Likewise, as we will see at the end of this chapter, we often do not manually program instruction for finding best hyperparameters, but rather use high-level functions which allow us to only set things up declaratively, without having to deal with low-grade details about the implementation of the process itself. Let’s try to see if we can properly classify wine as “white” or “red” with the help of variables describing its chemical composition. We will use the kNN method, with the number of neighbors set to 5. Predictors will be all available variables except quality andtype. For kNN classification the base R offers the knn function . For our needs we will use the knn3 function from thecaret package. The function knn3 expands the basic knn function in a way that allows us to call it by following the standard programming conventions we have already learned when training linear regression models: model &lt;- selected_method(formula, training_dataset, additional_parameters) predictions &lt;- predict(model, test_dataset, additional_parameters) In the case of the knn3 function, an additional parameter in training the model is k, set to 5. When creating predictions, we will set the type parameter to class, meaning we want to predict the class itself (alternative is probs, returning predicted probabilities for each class). Let’s try to create a kNN model with the help of wineNorm.train and then find wine type predictions for wineNorm.test. Assignment 14.6 - kNN Classification # create the variable `kNN5Mod` which will be # the result of calling `knn3` over the `wineNorm.train` set # target variable is `type` # predictors are all other variables except `quality` # add a `predictedType` column to `wineNorm.test` # which will store a result of calling the `predict` function # with `kNN5Mod` as a model and `wineNorm.test` as new data # set the `type` parameter to `class` # create the variable `kNN5Mod` which will be # the result of calling `knn3` over the `wineNorm.train` set # target variable is `type` # predictors are all other variables except `quality` #library(caret) # if needed kNN5Mod &lt;- knn3(type ~ . - quality, data = wineNorm.train, k = 5) # add a `predictedType` column to `wineNorm.test` # which will store a result of calling the `predict` function # with `kNN5Mod` as a model and `wineNorm.test` as new data # set the `type` parameter to `class` wineNorm.test$predictedType &lt;- predict(kNN5Mod, wineNorm.test, type = &quot;class&quot;) Note that we did not look for a model summary since we can not get too much information in this case. The kNN classifier can not provide us with some aggregated information about “learned” knowledge, it is just a “map of the domain space” that is then used for each new observation to determine which category it belongs to. How do we check the classifier’s performance? A typical procedure (with binary classifiers) is the creation of a so-calle “confusion matrix”. This simply means that we will create a table that will show how well the predicted values match the actual values. The easiest way to create this table is simply to call the table function with the prediction column and the actual value column as parameters. Assignment 14.7 - Simple Confusion Matrix # print a confusion matrix by calling the `table` function # over the appropriate columns of the `wineNorm.test` dataset # print a confusion matrix by calling the `table` function # over the appropriate columns of the `wineNorm.test` dataset table(wineNorm.test$predictedType, wineNorm.test$type) ## ## red white ## red 480 6 ## white 10 1453 Looking at the results we can intuitively conclude that in this case the classifier works extremely well, that is, white and red wines can be very easily classified by looking at their chemical properties. But we often want to describe the quality of the classifier using an objective, numerical measure. There are a number of such measures, and most of them can be directly calculated using values from the confusion matrix. Specifically, if we call the confusion matrix cells TP, TN, FP, FN (true positive, true negative, false positive, false negative), where we treat one class as “positive” and hence assigning the names of the corresponding cells depending on whether the classifier correctly guessed the class (main diagonal) or not (side diagonal). In this case, from the confusion matrix we can directly calculate the following measures: accuracy: \\(\\frac{TP + TN}{TP + FP + TN + FN}\\) sensitivity (recall): \\(\\frac{TP}{TP + FN}\\) precision: \\(\\frac{TP}{TP + FP}\\) false positive rate/false negative rate: \\(\\frac{FP}{FP + TN}\\) ; \\(\\frac{FN}{TP + FN}\\) etc. These are just some of the possible measures. Although accuracy may be the most logical choice (because we actually get a percentage of correctly guessed observations), we often have to be careful because it can give us a distorted picture of the classifier’s effectiveness, especially in when there is a huge disbalance in categories or when one type of error is far more dangerous than the other. A typical example is the diagnosis of rare diseases - if the disease occurs in only 0.1% of cases, then the trivial classifier, which for all observations diagnoses that the disease is not present, works well in 99.9% cases. Also, if it is a dangerous disease, then FP error (the disease is diagnosed although not present) is far less important than FN errors (the disease is present but is not recognized). In these cases, selecting another measure (eg “sensitivity” or “false negative rate”) is often a much better quality indicator of the classifier. We can very easily manually calculate all these measures using base R. However, the confusionMatrix function from the caret package (which also leverages the e1071 package) gives us the same result as the table function but with the added convenience of computing a large number of measures that can help us evaluate the classifier’s quality. Assignment 14.8 - confusionMatrix function # create a variable called `confMat` variable # which will stort the result of the `confusionMatrix` function # using the appropriate columns of the `wineNorm.test` dataset as parameters # print the `confMat` variable # create a variable called `confMat` variable # which will stort the result of the `confusionMatrix` function # using the appropriate columns of the `wineNorm.test` dataset as parameters #library(e1071) # if needed #library(caret) # if needed confMat &lt;- confusionMatrix(wineNorm.test$predictedType, wineNorm.test$type) # print the `confMat` variable confMat ## Confusion Matrix and Statistics ## ## Reference ## Prediction red white ## red 480 6 ## white 10 1453 ## ## Accuracy : 0.9918 ## 95% CI : (0.9867, 0.9953) ## No Information Rate : 0.7486 ## P-Value [Acc &gt; NIR] : &lt;2e-16 ## ## Kappa : 0.9781 ## Mcnemar&#39;s Test P-Value : 0.4533 ## ## Sensitivity : 0.9796 ## Specificity : 0.9959 ## Pos Pred Value : 0.9877 ## Neg Pred Value : 0.9932 ## Prevalence : 0.2514 ## Detection Rate : 0.2463 ## Detection Prevalence : 0.2494 ## Balanced Accuracy : 0.9877 ## ## &#39;Positive&#39; Class : red ## Since the variable confMat is an S3 object, with the help of the unlist function and the selection of the desired element, we can easily obtain only the numeric value of the measure we are interested in. This is useful if we want to integrate this function in our programming scripts. 14.4 Package caret and predictive modeling In the previous chapter we were already introduced to the caret package, more specifically some of its functions that help us with predictive modeling. This package actually offers a lot more than we have seen. Specifically, the caret package provides a set of tools to effectively perform all elements of the predictive modeling process: splitting data pre-processing of data feature selection adjusting the model with the help of re-sampling variable importance estimation As the dplyr package actually changes the way we use the R language to manage the data frames, so thecaret package enables a thorough modification of the predictive modeling approach used when programming in R. The functions of the caret package not only provide a cleaner syntax for low-level jobs, they also give the possibility of leveraging high-level approach for predictive modeling, where we declare declarative calls for what we want to do, and let R do low-level jobs returning us the corresponding result. Details of this package can be found at [this link] (https://topepo.github.io/caret/index.html), and below we will only give you a brief insight into some of the most useful features of this package. To demonstrate the declarative nature of predictive modeling with this package, we will look at two functions: train andtrainControl. The train function is actually a generic interface to a large number of predictive models (a list of all the models currently supported by the function can be [found here] (http://topepo.github.io/caret/train-models-by-tag.html) . In a large number of cases, the this function call is not different from the call of the predictive modeling method functions we have learned so far, specifically lm andknn3. The biggest difference is that instead of calling a specific function, here we define the method of predictive modeling using the method parameter. Let’s try to train a linear regression model using the train function and the previously prepared wine.train dataset. Assignment 14.9 - train function and linear regression # using the `train` function from the `caret` package # train a linear regression model using the `wine.train` dataset # target variable is `quality` and all other variables are predictors # set the `method` `&quot;lm&quot;` # store the resuling model in a variable called `linMod` # using the `train` function from the `caret` package # train a linear regression model using the `wine.train` dataset # target variable is `quality` and all other variables are predictors # set the `method` `&quot;lm&quot;` # store the resuling model in a variable called `linMod` linMod &lt;- train(quality ~., data = wine.train, method = &quot;lm&quot;) # read the summary of the obtained model summary(linMod) ## ## Call: ## lm(formula = .outcome ~ ., data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.7801 -0.4631 -0.0378 0.4576 2.7565 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 9.024e+01 1.618e+01 5.578 2.57e-08 *** ## fixed.acidity 7.248e-02 1.849e-02 3.919 9.01e-05 *** ## volatile.acidity -1.386e+00 9.720e-02 -14.260 &lt; 2e-16 *** ## citric.acid -1.279e-01 9.676e-02 -1.322 0.186209 ## residual.sugar 5.917e-02 6.935e-03 8.532 &lt; 2e-16 *** ## chlorides -6.720e-01 3.934e-01 -1.708 0.087705 . ## free.sulfur.dioxide 5.231e-03 9.121e-04 5.735 1.04e-08 *** ## total.sulfur.dioxide -1.600e-03 3.865e-04 -4.140 3.54e-05 *** ## density -8.915e+01 1.641e+01 -5.431 5.89e-08 *** ## pH 3.912e-01 1.073e-01 3.647 0.000268 *** ## sulphates 7.426e-01 8.966e-02 8.283 &lt; 2e-16 *** ## alcohol 2.446e-01 2.081e-02 11.751 &lt; 2e-16 *** ## typewhite -3.030e-01 6.684e-02 -4.534 5.93e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.7391 on 4533 degrees of freedom ## Multiple R-squared: 0.286, Adjusted R-squared: 0.2842 ## F-statistic: 151.3 on 12 and 4533 DF, p-value: &lt; 2.2e-16 We see that we get the same result as when we called the lm function directly at the beginning of this chapter. Note that in this case we haven’t used a large number of parameters of the train function that we can see in the documentation. For example, using the preprocess parameter can automatically perform some data preparation procedures such as normalization, BoxCox transformation, imputation of missing values, and so on. Let’s create a little more complex predictive model now. First, let’s examine a “control object” we can create using a function called trainControl. This function provides us with a “control panel” which allows us to fine-tune all parameters related to the training of our predictive model. Some of these parameters relate to so-called “data re-sampling” - this means that we can get a better estimate of the behavior of developed predictive models if we perform a process called “cross-validation”, where the training set is split multiple times and then a model is trained over and over again, always using a separate holdover part as the test set. In this way we actually get a number of models, each with their own results, which give us information not only about the quality but also the stability of the model. Additionally, we have the option of tweaking additional parameters, such as which summary funcation we want to apply on the resulting model, or whether we want the model (in the case of classification) to return the probabilities or just the resulting category. When finally create such a “control object”, we can recycle it as much as we want in the future steps of predictive modeling without the need to enter a large number of training-related parameters every time. A simple control object can look like this: # We use repeated cross-validation with 5 repeats ctrl &lt;- trainControl(method = &quot;repeatcv&quot;, repeats = 5) ## Warning: `repeats` has no meaning for this resampling method. Finally, let’s try to use two slightly more advanced methods of predictive modeling - the random forest method (the ranger method from the package of a same name) and the support vector method (thesvmRadial method from the kernlab) package. We will not go deeper into the details of these methods, just focus on how to call them with the help of the trainCtrl andtrain functions. In the following code we will also leverage a function called expand.grid. This function requires a vector different values of the hyperparameters we have provided, and will result in a dataframe containing all combinations of these hyperparameters. It is most commonly used in conjunction with the tuneGrid parameter to assign the candidate sets for predictive model hyperparameter - in this way, the train function will try out all combinations of the default parameters and (in conjunction with the cross validation method) select the parameters that showcase the best performance for the final model. #library(ranger) # if needed #library(kernlab) # if needed # we will use the same control object for both models # set `verboseIter` to TRUE # for better insight into training speed! ctrl &lt;- trainControl( method = &quot;repeatedcv&quot;, number = 5, repeats = 2, verboseIter = FALSE) # random forest model rfMod &lt;- train(quality ~., data = wine.train, method = &#39;ranger&#39;, tuneLength = 10, trControl = ctrl, num.trees = 10) # support vector model # we use a grid of hyperparameters # and pre-process data with normalization svm.grid &lt;- expand.grid(C = c(2, 4, 8), sigma = c(0.25, 1, 2)) svmMod &lt;- train(quality ~., data = wine.train, method = &quot;svmRadial&quot;, trControl = ctrl, tuneGrid = svm.grid, preProcess = c(&quot;center&quot;, &quot;scale&quot;)) Finally, we make a simple evaluation of the model with the help of RMSE measures. Assignment 14.10 - Simple Model Evaluation # with the help of the `predict` function and` rfMod` and `svmMod` models # add `predQualityRF` and `predQualitySVM` columns to `wine.test` dataset # print the value of the RMSE measure for all obtained models # with the help of the `predict` function and` rfMod` and `svmMod` models # add `predQualityRF` and `predQualitySVM` columns to `wine.test` dataset wine.test$predQualityRF &lt;- predict(rfMod, wine.test) wine.test$predQualitySVM &lt;- predict(svmMod, wine.test) # print the value of the RMSE measure for all obtained models cat(&quot;RMSE Linear regression:&quot;, rmse(wine.test$predQualityLR, wine.test$quality)) cat(&quot;\\nRMSE Random Forest:&quot;, rmse(wine.test$predQualityRF, wine.test$quality)) cat(&quot;\\nRMSE Support Vectors:&quot;, rmse(wine.test$predQualitySVM, wine.test$quality)) ## RMSE Linear regression: 0.7205543 ## RMSE Random Forest: 0.6929578 ## RMSE Support Vectors: 0.8515409 Programirajmo u R-u by Damir Pintar is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License.Based on a work at https://ratnip.github.io/FER_OPJR/ "]
]
